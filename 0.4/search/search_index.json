{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Spyglass", "text": "<p>Spyglass is a data analysis framework that facilitates the storage, analysis, and sharing of neuroscience data to support reproducible research. It is designed to be interoperable with the NWB format and integrates open-source tools into a coherent framework.</p>"}, {"location": "#installation", "title": "Installation", "text": "<p>To install to this project, see Installation.</p>"}, {"location": "#contributing", "title": "Contributing", "text": "<p>For contribution instructions see How to Contribute</p>"}, {"location": "#citing-spyglass", "title": "Citing Spyglass", "text": "<p>Kyu Hyun Lee, Eric Denovellis, Ryan Ly, Jeremy Magland, Jeff Soules, Alison Comrie, Jennifer Guidera, Rhino Nevers, Daniel Gramling, Philip Adenekan, Ji Hyun Bak, Emily Monroe, Andrew Tritt, Oliver R\u00fcbel, Thinh Nguyen, Dimitri Yatsenko, Joshua Chu, Caleb Kemere, Samuel Garcia, Alessio Buccino, Emily Aery Jones, Lisa Giocomo, and Loren Frank. 'Spyglass: A Data Analysis Framework for Reproducible and Shareable Neuroscience Research.' (2022) Society for Neuroscience, San Diego, CA.</p>"}, {"location": "CHANGELOG/", "title": "Change Log", "text": ""}, {"location": "CHANGELOG/#unreleased", "title": "[Unreleased]", "text": "<ul> <li>Migrate <code>config</code> helper scripts to Spyglass codebase. #662</li> <li>Revise contribution guidelines. #655</li> <li>Minor bug fixes. #656, #657, #659, #651</li> </ul>"}, {"location": "CHANGELOG/#042-october-10-2023", "title": "0.4.2 (October 10, 2023)", "text": ""}, {"location": "CHANGELOG/#infrastructure-support", "title": "Infrastructure / Support", "text": "<ul> <li>Bumped Python version to 3.9. #583</li> <li>Updated user management helper scripts for MySQL 8. #650</li> <li>Centralized config/path handling to permit setting via datajoint config. #593</li> <li>Fixed Merge Table deletes: error specificity and transaction context. #617</li> </ul>"}, {"location": "CHANGELOG/#pipelines", "title": "Pipelines", "text": "<ul> <li>Common:</li> <li>Added support multiple cameras per epoch. #557</li> <li>Removed <code>common_backup</code> schema. #631</li> <li>Added support for multiple position objects per NWB in <code>common_behav</code> via     PositionSource.SpatialSeries and RawPosition.PosObject #628, #616.     Note: Existing functions have been made compatible, but column labels for     <code>RawPosition.fetch1_dataframe</code> may change.</li> <li>Spike sorting:</li> <li>Added pipeline populator. #637, #646, #647</li> <li>Fixed curation functionality for <code>nn_isolation</code>. #597, #598</li> <li>Position: Added position interval/epoch mapping via PositionIntervalMap. #620,   #621, #627</li> <li>LFP: Refactored pipeline. #594, #588, #605, #606, #607, #608, #615, #629</li> </ul>"}, {"location": "CHANGELOG/#041-june-30-2023", "title": "0.4.1 (June 30, 2023)", "text": "<ul> <li>Add mkdocs automated deployment. #527, #537, #549, #551</li> <li>Add class for Merge Tables. #556, #564, #565</li> </ul>"}, {"location": "CHANGELOG/#040-may-22-2023", "title": "0.4.0 (May 22, 2023)", "text": "<ul> <li>Updated call to <code>spikeinterface.preprocessing.whiten</code> to use dtype np.float16.   #446,</li> <li>Updated default spike sorting metric parameters. #447</li> <li>Updated whitening to be compatible with recent changes in spikeinterface when   using mountainsort. #449</li> <li>Moved LFP pipeline to <code>src/spyglass/lfp/v1</code> and addressed related usability   issues. #468, #478, #482, #484, #504</li> <li>Removed whiten parameter for clusterless thresholder. #454</li> <li>Added plot to plot all DIO events in a session. #457</li> <li>Added file sharing functionality through kachery_cloud. #458, #460</li> <li>Pinned numpy version to <code>numpy&lt;1.24</code></li> <li>Added scripts to add guests and collaborators as users. #463</li> <li>Cleaned up installation instructions in repo README. #467</li> <li>Added checks in decoding visualization to ensure time dimensions are the   correct length.</li> <li>Fixed artifact removed valid times. #472</li> <li>Added codespell workflow for spell checking and fixed typos. #471</li> <li>Updated LFP code to save LFP as <code>pynwb.ecephys.LFP</code> type. #475</li> <li>Added artifact detection to LFP pipeline. #473</li> <li>Replaced calls to <code>spikeinterface.sorters.get_default_params</code> with   <code>spikeinterface.sorters.get_default_sorter_params</code>. #486</li> <li>Updated position pipeline and added functionality to handle pose estimation   through DeepLabCut. #367, #505</li> <li>Updated <code>environment_position.yml</code>. #502</li> <li>Renamed <code>FirFilter</code> class to <code>FirFilterParameters</code>. #512</li> </ul>"}, {"location": "CHANGELOG/#034-march-30-2023", "title": "0.3.4 (March 30, 2023)", "text": "<ul> <li>Fixed error in spike sorting pipeline referencing the \"probe_type\" column   which is no longer accessible from the <code>Electrode</code> table. #437</li> <li>Fixed error when inserting an NWB file that does not have a probe   manufacturer. #433, #436</li> <li>Fixed error when adding a new <code>DataAcquisitionDevice</code> and a new <code>ProbeType</code>.   #436</li> <li>Fixed inconsistency between capitalized/uncapitalized versions of \"Intan\" for   DataAcquisitionAmplifier and DataAcquisitionDevice.adc_circuit. #430, #438</li> </ul>"}, {"location": "CHANGELOG/#033-march-29-2023", "title": "0.3.3 (March 29, 2023)", "text": "<ul> <li>Fixed errors from referencing the changed primary key for <code>Probe</code>. #429</li> </ul>"}, {"location": "CHANGELOG/#032-march-28-2023", "title": "0.3.2 (March 28, 2023)", "text": "<ul> <li>Fixed import of <code>common_nwbfile</code>. #424</li> </ul>"}, {"location": "CHANGELOG/#031-march-24-2023", "title": "0.3.1 (March 24, 2023)", "text": "<ul> <li>Fixed import error due to <code>sortingview.Workspace</code>. #421</li> </ul>"}, {"location": "CHANGELOG/#030-march-24-2023", "title": "0.3.0 (March 24, 2023)", "text": "<ul> <li>Refactor common for non Frank Lab data, allow file-based mods #420</li> <li>Allow creation and linkage of device metadata from YAML #400</li> <li>Move helper functions to utils directory #386</li> </ul>"}, {"location": "LICENSE/", "title": "Copyright", "text": "<p>Copyright (c) 2020-present Loren Frank</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BEsq LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"}, {"location": "contribute/", "title": "Developer notes", "text": "<p>Notes on how the repo / database is organized, intended for a new developer.</p>"}, {"location": "contribute/#development-workflow", "title": "Development workflow", "text": "<p>New contributors should follow the Fork-and-Branch workflow. See GitHub instructions here.</p> <p>Regular contributors may choose to follow the Feature Branch Workflow for features that will involve multiple contributors.</p>"}, {"location": "contribute/#code-organization", "title": "Code organization", "text": "<ul> <li>Tables are grouped into schemas by topic (e.g., <code>common_metrics</code>)</li> <li>Schemas</li> <li>Are defined in a <code>py</code> pile.</li> <li>Correspond to MySQL 'databases'.</li> <li>Are organized into modules (e.g., <code>common</code>) by folders.</li> <li>The common module</li> <li>In principle, contains schema that are shared across all projects.</li> <li>In practice, contains shared tables (e.g., Session) and the first draft of     schemas that have since been split into their own modality-specific     modules (e.g., <code>lfp</code>)</li> <li>Should not be added to without discussion.</li> <li>A pipeline</li> <li>Refers to a set of tables used for processing data of a particular modality     (e.g., LFP, spike sorting, position tracking).</li> <li>May span multiple schema.</li> <li>For analysis that will be only useful to you, create your own schema.</li> </ul>"}, {"location": "contribute/#types-of-tables", "title": "Types of tables", "text": "<p>Spyglass uses DataJoint's default table tiers.</p> <p>By convention, an individual pipeline has one or more the following table types:</p> <ul> <li>Common/Multi-pipeline table</li> <li>NWB ingestion table</li> <li>Parameters table</li> <li>Selection table</li> <li>Data table</li> <li>Merge Table (see also doc</li> </ul>"}, {"location": "contribute/#commonmulti-pipeline", "title": "Common/Multi-pipeline", "text": "<p>Tables shared across multiple pipelines for shared data types.</p> <ul> <li>Naming convention: None</li> <li>Data tier: <code>dj.Manual</code></li> <li>Examples: <code>IntervalList</code> (time interval for any analysis), <code>AnalysisNwbfile</code>   (analysis NWB files)</li> </ul> <p>Note: Because these are stand-alone tables not part of the dependency structure, developers should include enough information to link entries back to the pipeline where the data is used.</p>"}, {"location": "contribute/#nwb-ingestion", "title": "NWB ingestion", "text": "<p>Automatically populated when an NWB file is ingested (i.e., <code>dj.Imported</code>) to keep track of object hashes (i.e., <code>object_id</code>) in the NWB file. All such tables should be included in the <code>make</code> method of <code>Session</code>.</p> <ul> <li>Naming convention: None</li> <li>Data tier: <code>dj.Imported</code></li> <li>Primary key: foreign key from <code>Session</code></li> <li>Non-primary key: <code>object_id</code>, the unique hash of an object in the NWB file.</li> <li>Examples: <code>Raw</code>, <code>Institution</code>, etc.</li> <li>Required methods:</li> <li><code>make</code>: must read information from an NWB file and insert it to the table.</li> <li><code>fetch_nwb</code>: retrieve the data specified by the object ID.</li> </ul>"}, {"location": "contribute/#parameters", "title": "Parameters", "text": "<p>Stores the set of values that may be used in an analysis.</p> <ul> <li>Naming convention: end with <code>Parameters</code> or <code>Params</code></li> <li>Data tier: <code>dj.Manual</code>, or <code>dj.Lookup</code></li> <li>Primary key: <code>{pipeline}_params_name</code>, <code>varchar</code></li> <li>Non-primary key: <code>{pipeline}_params</code>, <code>blob</code> - dict of parameters</li> <li>Examples: <code>RippleParameters</code>, <code>DLCModelParams</code></li> <li>Possible method: if <code>dj.Manual</code>, include <code>insert_default</code></li> </ul> <p>Notes: Some early instances of Parameter tables (a) used non-primary keys for each individual parameter, and (b) use the Manual rather than Lookup tier, requiring a class method to insert defaults.</p>"}, {"location": "contribute/#selection", "title": "Selection", "text": "<p>A staging area to pair sessions with parameter sets, allowing us to be selective in the analyses we run. It may not make sense to pair every paramset with every session.</p> <ul> <li>Naming convention: end with <code>Selection</code></li> <li>Data tier: <code>dj.Manual</code></li> <li>Primary key(s): Foreign key references to</li> <li>one or more NWB or data tables</li> <li>optionally, one or more parameter tables</li> <li>Non-primary key: None</li> <li>Examples: <code>MetricSelection</code>, <code>LFPSElection</code></li> </ul> <p>It is possible for a Selection table to collect information from more than one Parameter table. For example, the Selection table for spike sorting holds information about both the interval (<code>SortInterval</code>) and the group of electrodes (<code>SortGroup</code>) to be sorted.</p>"}, {"location": "contribute/#data", "title": "Data", "text": "<p>The output of processing steps associated with a selection table. Has a <code>make</code> method that carries out the computation specified in the Selection table when <code>populate</code> is called.</p> <ul> <li>Naming convention: None</li> <li>Data tier: <code>dj.Computed</code></li> <li>Primary key: Foreign key reference to a Selection table.</li> <li>Non-primary key: <code>analysis_file_name</code> inherited from <code>AnalysisNwbfile</code> table   (i.e., name of the analysis NWB file that will hold the output of the   computation).</li> <li>Required methods:</li> <li><code>make</code>: carries out the computation and insert a new entry; must also     create an analysis NWB file and insert it to the <code>AnalysisNwbfile</code> table.     Note that this method is never called directly; it is called via     <code>populate</code>. Multiple entries can be run in parallel when called with     <code>reserve_jobs=True</code>.</li> <li><code>delete</code>: extension of the <code>delete</code> method that checks user privilege     before deleting entries as a way to prevent accidental deletion of     computations that take a long time (see below).</li> <li>Example: <code>QualityMetrics</code>, <code>LFPV1</code></li> </ul>"}, {"location": "contribute/#merge", "title": "Merge", "text": "<p>Following a convention outlined in the dedicated doc, merges the output of different pipelines dedicated to the same modality as part tables (e.g., common LFP, LFP v1, imported LFP) to permit unified downstream processing.</p> <ul> <li>Naming convention: <code>{Pipeline}Output</code></li> <li>Data tier: custom <code>_Merge</code> class</li> <li>Primary key: <code>merge_id</code>, <code>uuid</code></li> <li>Non-primary key: <code>source</code>, <code>varchar</code> table name associated with that entry</li> <li>Required methods: None - see custom class methods with <code>merge_</code> prefix</li> <li>Example: <code>LFPOutput</code>, <code>PositionOutput</code></li> </ul>"}, {"location": "contribute/#integration-with-nwb", "title": "Integration with NWB", "text": ""}, {"location": "contribute/#nwb-files", "title": "NWB files", "text": "<p>NWB files contain everything about the experiment and form the starting point of all analyses.</p> <ul> <li>Naming: <code>{animal name}YYYYMMDD.nwb</code></li> <li>Storage:</li> <li>On disk, directory identified by <code>settings.py</code> as <code>raw_dir</code> (e.g.,     <code>/stelmo/nwb/raw</code>)</li> <li>In database, in the <code>Nwbfile</code> table</li> <li>Copies:</li> <li>made with an underscore <code>{animal name}YYYYMMDD_.nwb</code></li> <li>stored in the same <code>raw_dir</code></li> <li>contain pointers to objects in original file</li> <li>permit adding new parts to the NWB file without risk of corrupting the     original data</li> </ul>"}, {"location": "contribute/#analysis-files", "title": "Analysis files", "text": "<p>Hold the results of intermediate steps in the analysis.</p> <ul> <li>Naming: <code>{animal name}YYYYMMDD_{10-character random string}.nwb</code></li> <li>Storage:</li> <li>On disk, directory identified by <code>settings.py</code> as <code>analysis_dir</code> (e.g.,     <code>/stelmo/nwb/analysis</code>). Items are further sorted into folders matching     original NWB file name</li> <li>In database, in the <code>AnalysisNwbfile</code> table.</li> <li>Examples: filtered recordings, spike times of putative units after sorting,   or waveform snippets.</li> </ul> <p>Note: Because NWB files and analysis files exist both on disk and listed in tables, these can become out of sync. You can 'equalize' the database table lists and the set of files on disk by running <code>cleanup</code> method, which deletes any files not listed in the table from disk.</p>"}, {"location": "contribute/#reading-and-writing-recordings", "title": "Reading and writing recordings", "text": "<p>Recordings start out as an NWB file, which is opened as a <code>NwbRecordingExtractor</code>, a class in <code>spikeinterface</code>. When using <code>sortingview</code> for visualizing the results of spike sorting, this recording is saved again in HDF5 format. This duplication should be resolved in the future.</p>"}, {"location": "contribute/#naming-convention", "title": "Naming convention", "text": "<p>The following objects should be uniquely named.</p> <ul> <li>Recordings: Underscore-separated concatenations of uniquely defining   features,   <code>NWBFileName_IntervalName_ElectrodeGroupName_PreprocessingParamsName</code>.</li> <li>SpikeSorting: Adds <code>SpikeSorter_SorterParamName</code> to the name of the   recording.</li> <li>Waveforms: Adds <code>_WaveformParamName</code> to the name of the sorting.</li> <li>Quality metrics: Adds <code>_MetricParamName</code> to the name of the waveform.</li> <li>Analysis NWB files:   <code>NWBFileName_IntervalName_ElectrodeGroupName_PreprocessingParamsName.nwb</code></li> <li>Each recording and sorting is given truncated UUID strings as part of concatenations.</li> </ul> <p>Following broader Python conventions, methods a method that will not be explicitly called by the user should start with <code>_</code></p>"}, {"location": "contribute/#time", "title": "Time", "text": "<p>The <code>IntervalList</code> table stores all time intervals in the following format: <code>[start_time, stop_time]</code>, which represents a contiguous time of valid data. These are used to exclude any invalid timepoints, such as missing data from a faulty connection.</p> <ul> <li>Intervals can be nested for a set of disjoint intervals.</li> <li>Some recordings have explicit   PTP timestamps   associated with each sample. Some older recordings are missing PTP times, and   times must be inferred from the TTL pulses from the camera.</li> </ul>"}, {"location": "contribute/#misc", "title": "Misc", "text": "<ul> <li>During development, we suggest using a Docker container. See   example.</li> <li>DataJoint is unable to set delete permissions on a per-table basis.   If a user is able to delete entries in a given table, she can delete   entries in any table in the schema. The <code>SpikeSorting</code> table   extends the built-in <code>delete</code> method to check if the username matches a list   of allowed users when <code>delete</code> is called. Issues #226 and #586 track the   progress of generalizing this feature.</li> <li><code>numpy</code> style docstrings will be interpreted by API docs. To check for   compliance, monitor the std out when building docs (see <code>docs/README.md</code>)</li> <li><code>fetch_nwb</code> is currently reperated across many tables. For progress on a   fix, follow issue #530</li> </ul>"}, {"location": "contribute/#making-a-release", "title": "Making a release", "text": "<p>Spyglass follows Semantic Versioning with  versioning of the form <code>X.Y.Z</code> (e.g., <code>0.4.2</code>).</p> <ol> <li>In <code>CITATION.cff</code>, update the <code>version</code> key.</li> <li>Make a pull request with changes. </li> <li>After the pull request is merged, pull this merge commit and tag it with <code>git tag {version}</code> </li> <li>Publish the new release tag. Run <code>git push origin {version}</code>. This will    rebuild docs and push updates to PyPI. </li> <li>Make a new     release on GitHub.</li> </ol>"}, {"location": "installation/", "title": "Installation", "text": "<p>Note: Developers, or those who wish to add features or otherwise work on the codebase should follow the same steps below, but install Spyglass as editable with the <code>-e</code> flag: <code>pip install -e /path/to/spyglass</code></p>"}, {"location": "installation/#basic-installation", "title": "Basic Installation", "text": "<p>For basic installation steps, see the Setup notebook 'local installation' section, including python, mamba (for managing a virtual environment), VSCode, Jupyter, and git. This notebook also covers database access.</p>"}, {"location": "installation/#additional-packages", "title": "Additional Packages", "text": "<p>Some pipelines require installation of additional packages.</p> <p>The spike sorting pipeline relies on <code>spikeinterface</code> and optionally <code>mountainsort4</code>.</p> <pre><code>pip install spikeinterface[full,widgets]\npip install mountainsort4\n</code></pre> <p>WARNING: If you are on an M1 Mac, you need to install <code>pyfftw</code> via <code>conda</code> BEFORE installing <code>ghostipy</code>:</p> <pre><code>conda install -c conda-forge pyfftw\n</code></pre> <p>The LFP pipeline uses <code>ghostipy</code>:</p> <pre><code>pip install ghostipy\n</code></pre>"}, {"location": "installation/#database-access", "title": "Database access", "text": "<p>For basic installation steps, see the Setup notebook 'database connection' section. For additional details, see the DataJoint documentation.</p>"}, {"location": "installation/#config", "title": "Config", "text": ""}, {"location": "installation/#via-file-recommended", "title": "Via File (Recommended)", "text": "<p>A <code>dj_local_conf.json</code> file in your Spyglass directory (or wherever python is launched) can hold all the specifics needed to connect to a database. This can include different directories for different pipelines. If only the <code>base</code> is specified, the subfolder names below are included as defaults.</p> <pre><code>{\n  \"custom\": {\n    \"database.prefix\": \"username_\",\n    \"spyglass_dirs\": {\n      \"base\": \"/your/base/path\",\n      \"raw\": \"/your/base/path/raw\",\n      \"analysis\": \"/your/base/path/analysis\",\n      \"recording\": \"/your/base/path/recording\",\n      \"spike_sorting_storage\": \"/your/base/path/spikesorting\",\n      \"waveforms\": \"/your/base/path/waveforms\",\n      \"temp\": \"/your/base/path/tmp\"\n    }\n  }\n}\n</code></pre> <p><code>dj_local_conf_example.json</code> can be copied and saved as <code>dj_local_conf.json</code> to set the configuration for a given folder. Alternatively, it can be saved as <code>.datajoint_config.json</code> in a user's home directory to be accessed globally. See DataJoint docs for more details.</p>"}, {"location": "installation/#via-environment-variables", "title": "Via Environment Variables", "text": "<p>Older versions of Spyglass relied exclusively on environment for config. If <code>spyglass_dirs</code> is not found in the config file, Spyglass will look for environment variables. These can be set either once in a terminal session, or permanently in a <code>.bashrc</code> file.</p> <pre><code>export SPYGLASS_BASE_DIR=\"/stelmo/nwb\"\nexport SPYGLASS_RECORDING_DIR=\"$SPYGLASS_BASE_DIR/recording\"\nexport SPYGLASS_SORTING_DIR=\"$SPYGLASS_BASE_DIR/sorting\"\nexport SPYGLASS_VIDEO_DIR=\"$SPYGLASS_BASE_DIR/video\"\nexport SPYGLASS_WAVEFORMS_DIR=\"$SPYGLASS_BASE_DIR/waveforms\"\nexport SPYGLASS_TEMP_DIR=\"$SPYGLASS_BASE_DIR/tmp\"\nexport DJ_SUPPORT_FILEPATH_MANAGEMENT=\"TRUE\"\n</code></pre> <p>To load variables from a <code>.bashrc</code> file, run <code>source ~/.bashrc</code> in a terminal.</p>"}, {"location": "installation/#temporary-directory", "title": "Temporary directory", "text": "<p>A temporary directory will speed up spike sorting. If unspecified by either method above, it will be assumed as a <code>tmp</code> subfolder relative to the base path. Be sure it has enough free space (ideally at least 500GB).</p>"}, {"location": "installation/#file-manager", "title": "File manager", "text": "<p><code>kachery-cloud</code> is a file manager for Frank Lab collaborators who do not have access to the lab's production database.</p> <p>To customize <code>kachery</code> file paths, the following can similarly be pasted into your <code>.bashrc</code>. If unspecified, the defaults below are assumed.</p> <pre><code>export KACHERY_CLOUD_DIR=\"$SPYGLASS_BASE_DIR/.kachery-cloud\"\nexport KACHERY_TEMP_DIR=\"$SPYGLASS_BASE_DIR/tmp\"\n</code></pre> <p>Be sure to load these with <code>source ~/.bashrc</code> to persist changes.</p>"}, {"location": "installation/#test-connection", "title": "Test connection", "text": "<p>Finally, open up a python console (e.g., run <code>ipython</code> from terminal) and import <code>spyglass</code> to check that the installation has worked.</p>"}, {"location": "api/src/spyglass/_version/", "title": "_version.py", "text": ""}, {"location": "api/src/spyglass/settings/", "title": "settings.py", "text": ""}, {"location": "api/src/spyglass/settings/#src.spyglass.settings.SpyglassConfig", "title": "<code>SpyglassConfig</code>", "text": "<p>Gets Spyglass dirs from dj.config or environment variables.</p> <p>Uses SpyglassConfig.relative_dirs to (a) gather user settings from dj.config or os environment variables or defaults relative to base, in that order (b) set environment variables, and (c) make dirs that don't exist. NOTE: when passed a base_dir, it will ignore env vars to facilitate testing.</p> Source code in <code>src/spyglass/settings.py</code> <pre><code>class SpyglassConfig:\n    \"\"\"Gets Spyglass dirs from dj.config or environment variables.\n\n    Uses SpyglassConfig.relative_dirs to (a) gather user\n    settings from dj.config or os environment variables or defaults relative to\n    base, in that order (b) set environment variables, and (c) make dirs that\n    don't exist. NOTE: when passed a base_dir, it will ignore env vars to\n    facilitate testing.\n    \"\"\"\n\n    def __init__(self, base_dir=None, **kwargs):\n        \"\"\"\n        Initializes a new instance of the class.\n\n        Parameters\n        ----------\n            base_dir (str): The base directory.\n\n        Returns\n        -------\n            None\n        \"\"\"\n        self.supplied_base_dir = base_dir\n        self._config = dict()\n        self.config_defaults = dict(prepopulate=True)\n\n        self.relative_dirs = {\n            # {PREFIX}_{KEY}_DIR, default dir relative to base_dir\n            \"spyglass\": {\n                \"raw\": \"raw\",\n                \"analysis\": \"analysis\",\n                \"recording\": \"recording\",\n                \"sorting\": \"spikesorting\",\n                \"waveforms\": \"waveforms\",\n                \"temp\": \"tmp\",\n                \"video\": \"video\",\n            },\n            \"kachery\": {\n                \"cloud\": \"kachery_storage\",\n                \"storage\": \"kachery_storage\",\n                \"temp\": \"tmp\",\n            },\n        }\n\n        self.dj_defaults = {\n            \"database.host\": kwargs.get(\"database_host\", \"lmf-db.cin.ucsf.edu\"),\n            \"database.user\": kwargs.get(\"database_user\"),\n            \"database.port\": kwargs.get(\"database_port\", 3306),\n            \"database.use_tls\": kwargs.get(\"database_use_tls\", True),\n            \"filepath_checksum_size_limit\": 1 * 1024**3,\n            \"enable_python_native_blobs\": True,\n        }\n\n        self.env_defaults = {\n            \"FIGURL_CHANNEL\": \"franklab2\",\n            \"DJ_SUPPORT_FILEPATH_MANAGEMENT\": \"TRUE\",\n            \"KACHERY_CLOUD_EPHEMERAL\": \"TRUE\",\n        }\n\n    def load_config(self, force_reload=False):\n        \"\"\"\n        Loads the configuration settings for the object.\n\n        Order of precedence, where X is base, raw, analysis, etc.:\n            1. SpyglassConfig(base_dir=\"string\") for base dir only\n            2. dj.config['custom']['{spyglass/kachery}_dirs']['X']\n            3. os.environ['{SPYGLASS/KACHERY}_{X}_DIR']\n            4. resolved_base_dir/X for non-base dirs\n\n        Parameters\n        ----------\n        force_reload: bool\n            Optional. Default False. Default skip load if already completed.\n\n        Raises\n        ------\n        ValueError\n            If base_dir is not set in either dj.config or os.environ.\n\n        Returns\n        -------\n        dict\n            list of relative_dirs and other settings (e.g., prepopulate).\n        \"\"\"\n        if self._config and not force_reload:\n            return self._config\n\n        dj_custom = dj.config.get(\"custom\", {})\n        dj_spyglass = dj_custom.get(\"spyglass_dirs\", {})\n        dj_kachery = dj_custom.get(\"kachery_dirs\", {})\n\n        resolved_base = (\n            self.supplied_base_dir\n            or dj_spyglass.get(\"base\")\n            or os.environ.get(\"SPYGLASS_BASE_DIR\")\n        )\n\n        if not resolved_base or not Path(resolved_base).exists():\n            raise ValueError(\n                f\"Could not find SPYGLASS_BASE_DIR: {resolved_base}\"\n                + \"\\n\\tCheck dj.config['custom']['spyglass_dirs']['base']\"\n                + \"\\n\\tand os.environ['SPYGLASS_BASE_DIR']\"\n            )\n\n        config_dirs = {\"SPYGLASS_BASE_DIR\": resolved_base}\n        for prefix, dirs in self.relative_dirs.items():\n            for dir, dir_str in dirs.items():\n                dir_env_fmt = self.dir_to_var(dir, prefix)\n\n                env_loc = (  # Ignore env vars if base was passed to func\n                    os.environ.get(dir_env_fmt)\n                    if not self.supplied_base_dir\n                    else None\n                )\n\n                dir_location = (\n                    dj_spyglass.get(dir)\n                    or dj_kachery.get(dir)\n                    or env_loc\n                    or str(Path(resolved_base) / dir_str)\n                ).replace('\"', \"\")\n\n                config_dirs.update({dir_env_fmt: dir_location})\n\n        kachery_zone_dict = {\n            \"KACHERY_ZONE\": (\n                os.environ.get(\"KACHERY_ZONE\")\n                or dj.config.get(\"custom\", {}).get(\"kachery_zone\")\n                or \"franklab.default\"\n            )\n        }\n\n        loaded_env = self._load_env_vars()\n        self._set_env_with_dict(\n            {**config_dirs, **kachery_zone_dict, **loaded_env}\n        )\n        self._mkdirs_from_dict_vals(config_dirs)\n\n        self._config = dict(\n            debug_mode=dj_custom.get(\"debug_mode\", False),\n            **self.config_defaults,\n            **config_dirs,\n            **kachery_zone_dict,\n            **loaded_env,\n        )\n\n        self._set_dj_config_stores(config_dirs)\n\n        return self._config\n\n    def _load_env_vars(self):\n        loaded_dict = {}\n        for var, val in self.env_defaults.items():\n            loaded_dict[var] = os.getenv(var, val)\n        return loaded_dict\n\n    @staticmethod\n    def _set_env_with_dict(env_dict):\n        # NOTE: Kept for backwards compatibility. Should be removed in future.\n        for var, val in env_dict.items():\n            os.environ[var] = str(val)\n\n    @staticmethod\n    def _mkdirs_from_dict_vals(dir_dict):\n        for dir_str in dir_dict.values():\n            Path(dir_str).mkdir(exist_ok=True)\n\n    def _set_dj_config_stores(self, check_match=True, set_stores=True):\n        \"\"\"\n        Checks dj.config['stores'] match resolved dirs. Ensures stores set.\n\n        Parameters\n        ----------\n        dir_dict: dict\n            Dictionary of resolved dirs.\n        check_match: bool\n            Optional. Default True. Check that dj.config['stores'] match resolved dirs.\n        set_stores: bool\n            Optional. Default True. Set dj.config['stores'] to resolved dirs.\n        \"\"\"\n        if check_match:\n            dj_stores = dj.config.get(\"stores\", {})\n            store_raw = dj_stores.get(\"raw\", {}).get(\"location\")\n            store_analysis = dj_stores.get(\"analysis\", {}).get(\"location\")\n\n            err_template = (\n                \"dj.config['stores'] does not match resolved dir.\"\n                + \"\\n\\tdj.config['stores']['{0}']['location']:\\n\\t\\t{1}\"\n                + \"\\n\\tSPYGLASS_{2}_DIR:\\n\\t\\t{3}.\"\n            )\n            if store_raw and Path(store_raw) != Path(self.raw_dir):\n                raise ValueError(\n                    err_template.format(\"raw\", store_raw, \"RAW\", self.raw_dir)\n                )\n            if store_analysis and Path(store_analysis) != Path(\n                self.analysis_dir\n            ):\n                raise ValueError(\n                    err_template.format(\n                        \"analysis\",\n                        store_analysis,\n                        \"ANALYSIS\",\n                        self.analysis_dir,\n                    )\n                )\n\n        if set_stores:\n            dj.config.update(self._dj_stores)\n\n    def dir_to_var(self, dir: str, dir_type: str = \"spyglass\"):\n        \"\"\"Converts a dir string to an env variable name.\"\"\"\n        dir_string = self.relative_dirs.get(dir_type, {}).get(dir, \"base\")\n        return f\"{dir_type.upper()}_{dir_string.upper()}_DIR\"\n\n    def _generate_dj_config(\n        self,\n        base_dir: str = None,\n        database_user: str = None,\n        database_host: str = \"lmf-db.cin.ucsf.edu\",\n        database_port: int = 3306,\n        database_use_tls: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Generate a datajoint configuration file.\n\n        Parameters\n        ----------\n        base_dir : str, optional\n            The base directory. If not provided, will use the env variable or\n            existing config.\n        database_user : str, optional\n            The database user. If not provided, resulting config will not\n            specify.\n        database_host : str, optional\n            Default lmf-db.cin.ucsf.edu. MySQL host name.\n        dapabase_port : int, optional\n            Default 3306. Port number for MySQL server.\n        database_use_tls : bool, optional\n            Default True. Use TLS encryption.\n        **kwargs: dict, optional\n            Any other valid datajoint configuration parameters.\n            Note: python will raise error for params with `.` in name.\n        \"\"\"\n\n        if base_dir:\n            self.supplied_base_dir = base_dir\n            self.load_config(force_reload=True)\n\n        if database_user:\n            kwargs.update({\"database.user\": database_user})\n\n        kwargs.update(\n            {\n                \"database.host\": database_host,\n                \"database.port\": database_port,\n                \"database.use_tls\": database_use_tls,\n            }\n        )\n\n        # `|` merges dictionaries\n        return self.dj_defaults | self._dj_stores | self._dj_custom | kwargs\n\n    def save_dj_config(\n        self,\n        save_method: str = \"global\",\n        filename: str = None,\n        base_dir=None,\n        database_user=None,\n        set_password=True,\n        **kwargs,\n    ):\n        \"\"\"Set the dj.config parameters, set password, and save config to file.\n\n        Parameters\n        ----------\n        save_method : {'local', 'global', 'custom'}, optional\n            The method to use to save the config. If either 'local' or 'global',\n            datajoint builtins will be used to save.\n        filename : str or Path, optional\n            Default to datajoint global config. If save_method = 'custom', name\n            of file to generate. Must end in either be either yaml or json.\n        base_dir : str, optional\n            The base directory. If not provided, will default to the env var\n        database_user : str, optional\n            The database user. If not provided, resulting config will not\n            specify.\n        set_password : bool, optional\n            Default True. Set the database password.\n        \"\"\"\n        if save_method == \"local\":\n            filepath = Path(\".\") / dj.settings.LOCALCONFIG\n        elif not filename or save_method == \"global\":\n            save_method = \"global\"\n            filepath = Path(\"~\").expanduser() / dj.settings.GLOBALCONFIG\n\n        dj.config.update(\n            self._generate_dj_config(\n                base_dir=base_dir, database_user=database_user, **kwargs\n            )\n        )\n\n        if set_password:\n            try:\n                dj.set_password()\n            except OperationalError as e:\n                warnings.warn(f\"Database connection issues. Wrong pass? {e}\")\n                # NOTE: Save anyway? Or raise error?\n\n        user_warn = (\n            f\"Replace existing file? {filepath.resolve()}\\n\\t\"\n            + \"\\n\\t\".join([f\"{k}: {v}\" for k, v in config.items()])\n            + \"\\n\"\n        )\n\n        if filepath.exists() and dj.utils.user_choice(user_warn)[0] != \"y\":\n            return dj.config\n\n        if save_method == \"global\":\n            dj.config.save_global(verbose=True)\n            return\n\n        if save_method == \"local\":\n            dj.config.save_local(verbose=True)\n            return\n\n        with open(filename, \"w\") as outfile:\n            if filename.endswith(\"json\"):\n                json.dump(dj.config, outfile, indent=2)\n            else:\n                yaml.dump(dj.config, outfile, default_flow_style=False)\n\n    @property\n    def _dj_stores(self) -&gt; dict:\n        self.load_config()\n        return {\n            \"stores\": {\n                \"raw\": {\n                    \"protocol\": \"file\",\n                    \"location\": self.raw_dir,\n                    \"stage\": self.raw_dir,\n                },\n                \"analysis\": {\n                    \"protocol\": \"file\",\n                    \"location\": self.analysis_dir,\n                    \"stage\": self.analysis_dir,\n                },\n            }\n        }\n\n    @property\n    def _dj_custom(self) -&gt; dict:\n        self.load_config()\n        return {\n            \"custom\": {\n                \"debug_mode\": str(self.debug_mode).lower(),\n                \"spyglass_dirs\": {\n                    \"base\": self.base_dir,\n                    \"raw\": self.raw_dir,\n                    \"analysis\": self.analysis_dir,\n                    \"recording\": self.recording_dir,\n                    \"sorting\": self.sorting_dir,\n                    \"waveforms\": self.waveforms_dir,\n                    \"temp\": self.temp_dir,\n                    \"video\": self.video_dir,\n                },\n                \"kachery_dirs\": {\n                    \"cloud\": self.config.get(\n                        self.dir_to_var(\"cloud\", \"kachery\")\n                    ),\n                    \"storage\": self.config.get(\n                        self.dir_to_var(\"storage\", \"kachery\")\n                    ),\n                    \"temp\": self.config.get(self.dir_to_var(\"tmp\", \"kachery\")),\n                },\n                \"kachery_zone\": \"franklab.default\",\n            }\n        }\n\n    @property\n    def config(self) -&gt; dict:\n        self.load_config()\n        return self._config\n\n    @property\n    def base_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"base\"))\n\n    @property\n    def raw_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"raw\"))\n\n    @property\n    def analysis_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"analysis\"))\n\n    @property\n    def recording_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"recording\"))\n\n    @property\n    def sorting_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"sorting\"))\n\n    @property\n    def waveforms_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"waveforms\"))\n\n    @property\n    def temp_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"temp\"))\n\n    @property\n    def waveform_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"waveform\"))\n\n    @property\n    def video_dir(self) -&gt; str:\n        return self.config.get(self.dir_to_var(\"video\"))\n\n    @property\n    def debug_mode(self) -&gt; bool:\n        return self.config.get(\"debug_mode\", False)\n</code></pre>"}, {"location": "api/src/spyglass/settings/#src.spyglass.settings.SpyglassConfig.__init__", "title": "<code>__init__(base_dir=None, **kwargs)</code>", "text": "<p>Initializes a new instance of the class.</p> <p>Returns:</p> Type Description <code>    None</code> Source code in <code>src/spyglass/settings.py</code> <pre><code>def __init__(self, base_dir=None, **kwargs):\n    \"\"\"\n    Initializes a new instance of the class.\n\n    Parameters\n    ----------\n        base_dir (str): The base directory.\n\n    Returns\n    -------\n        None\n    \"\"\"\n    self.supplied_base_dir = base_dir\n    self._config = dict()\n    self.config_defaults = dict(prepopulate=True)\n\n    self.relative_dirs = {\n        # {PREFIX}_{KEY}_DIR, default dir relative to base_dir\n        \"spyglass\": {\n            \"raw\": \"raw\",\n            \"analysis\": \"analysis\",\n            \"recording\": \"recording\",\n            \"sorting\": \"spikesorting\",\n            \"waveforms\": \"waveforms\",\n            \"temp\": \"tmp\",\n            \"video\": \"video\",\n        },\n        \"kachery\": {\n            \"cloud\": \"kachery_storage\",\n            \"storage\": \"kachery_storage\",\n            \"temp\": \"tmp\",\n        },\n    }\n\n    self.dj_defaults = {\n        \"database.host\": kwargs.get(\"database_host\", \"lmf-db.cin.ucsf.edu\"),\n        \"database.user\": kwargs.get(\"database_user\"),\n        \"database.port\": kwargs.get(\"database_port\", 3306),\n        \"database.use_tls\": kwargs.get(\"database_use_tls\", True),\n        \"filepath_checksum_size_limit\": 1 * 1024**3,\n        \"enable_python_native_blobs\": True,\n    }\n\n    self.env_defaults = {\n        \"FIGURL_CHANNEL\": \"franklab2\",\n        \"DJ_SUPPORT_FILEPATH_MANAGEMENT\": \"TRUE\",\n        \"KACHERY_CLOUD_EPHEMERAL\": \"TRUE\",\n    }\n</code></pre>"}, {"location": "api/src/spyglass/settings/#src.spyglass.settings.SpyglassConfig.load_config", "title": "<code>load_config(force_reload=False)</code>", "text": "<p>Loads the configuration settings for the object.</p> <p>Order of precedence, where X is base, raw, analysis, etc.:     1. SpyglassConfig(base_dir=\"string\") for base dir only     2. dj.config['custom']['{spyglass/kachery}dirs']['X']     3. os.environ['{SPYGLASS/KACHERY}_DIR']     4. resolved_base_dir/X for non-base dirs</p> <p>Parameters:</p> Name Type Description Default <code>force_reload</code> <p>Optional. Default False. Default skip load if already completed.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If base_dir is not set in either dj.config or os.environ.</p> <p>Returns:</p> Type Description <code>dict</code> <p>list of relative_dirs and other settings (e.g., prepopulate).</p> Source code in <code>src/spyglass/settings.py</code> <pre><code>def load_config(self, force_reload=False):\n    \"\"\"\n    Loads the configuration settings for the object.\n\n    Order of precedence, where X is base, raw, analysis, etc.:\n        1. SpyglassConfig(base_dir=\"string\") for base dir only\n        2. dj.config['custom']['{spyglass/kachery}_dirs']['X']\n        3. os.environ['{SPYGLASS/KACHERY}_{X}_DIR']\n        4. resolved_base_dir/X for non-base dirs\n\n    Parameters\n    ----------\n    force_reload: bool\n        Optional. Default False. Default skip load if already completed.\n\n    Raises\n    ------\n    ValueError\n        If base_dir is not set in either dj.config or os.environ.\n\n    Returns\n    -------\n    dict\n        list of relative_dirs and other settings (e.g., prepopulate).\n    \"\"\"\n    if self._config and not force_reload:\n        return self._config\n\n    dj_custom = dj.config.get(\"custom\", {})\n    dj_spyglass = dj_custom.get(\"spyglass_dirs\", {})\n    dj_kachery = dj_custom.get(\"kachery_dirs\", {})\n\n    resolved_base = (\n        self.supplied_base_dir\n        or dj_spyglass.get(\"base\")\n        or os.environ.get(\"SPYGLASS_BASE_DIR\")\n    )\n\n    if not resolved_base or not Path(resolved_base).exists():\n        raise ValueError(\n            f\"Could not find SPYGLASS_BASE_DIR: {resolved_base}\"\n            + \"\\n\\tCheck dj.config['custom']['spyglass_dirs']['base']\"\n            + \"\\n\\tand os.environ['SPYGLASS_BASE_DIR']\"\n        )\n\n    config_dirs = {\"SPYGLASS_BASE_DIR\": resolved_base}\n    for prefix, dirs in self.relative_dirs.items():\n        for dir, dir_str in dirs.items():\n            dir_env_fmt = self.dir_to_var(dir, prefix)\n\n            env_loc = (  # Ignore env vars if base was passed to func\n                os.environ.get(dir_env_fmt)\n                if not self.supplied_base_dir\n                else None\n            )\n\n            dir_location = (\n                dj_spyglass.get(dir)\n                or dj_kachery.get(dir)\n                or env_loc\n                or str(Path(resolved_base) / dir_str)\n            ).replace('\"', \"\")\n\n            config_dirs.update({dir_env_fmt: dir_location})\n\n    kachery_zone_dict = {\n        \"KACHERY_ZONE\": (\n            os.environ.get(\"KACHERY_ZONE\")\n            or dj.config.get(\"custom\", {}).get(\"kachery_zone\")\n            or \"franklab.default\"\n        )\n    }\n\n    loaded_env = self._load_env_vars()\n    self._set_env_with_dict(\n        {**config_dirs, **kachery_zone_dict, **loaded_env}\n    )\n    self._mkdirs_from_dict_vals(config_dirs)\n\n    self._config = dict(\n        debug_mode=dj_custom.get(\"debug_mode\", False),\n        **self.config_defaults,\n        **config_dirs,\n        **kachery_zone_dict,\n        **loaded_env,\n    )\n\n    self._set_dj_config_stores(config_dirs)\n\n    return self._config\n</code></pre>"}, {"location": "api/src/spyglass/settings/#src.spyglass.settings.SpyglassConfig.dir_to_var", "title": "<code>dir_to_var(dir, dir_type='spyglass')</code>", "text": "<p>Converts a dir string to an env variable name.</p> Source code in <code>src/spyglass/settings.py</code> <pre><code>def dir_to_var(self, dir: str, dir_type: str = \"spyglass\"):\n    \"\"\"Converts a dir string to an env variable name.\"\"\"\n    dir_string = self.relative_dirs.get(dir_type, {}).get(dir, \"base\")\n    return f\"{dir_type.upper()}_{dir_string.upper()}_DIR\"\n</code></pre>"}, {"location": "api/src/spyglass/settings/#src.spyglass.settings.SpyglassConfig.save_dj_config", "title": "<code>save_dj_config(save_method='global', filename=None, base_dir=None, database_user=None, set_password=True, **kwargs)</code>", "text": "<p>Set the dj.config parameters, set password, and save config to file.</p> <p>Parameters:</p> Name Type Description Default <code>save_method</code> <code>(local, 'global', custom)</code> <p>The method to use to save the config. If either 'local' or 'global', datajoint builtins will be used to save.</p> <code>'local'</code> <code>filename</code> <code>str or Path</code> <p>Default to datajoint global config. If save_method = 'custom', name of file to generate. Must end in either be either yaml or json.</p> <code>None</code> <code>base_dir</code> <code>str</code> <p>The base directory. If not provided, will default to the env var</p> <code>None</code> <code>database_user</code> <code>str</code> <p>The database user. If not provided, resulting config will not specify.</p> <code>None</code> <code>set_password</code> <code>bool</code> <p>Default True. Set the database password.</p> <code>True</code> Source code in <code>src/spyglass/settings.py</code> <pre><code>def save_dj_config(\n    self,\n    save_method: str = \"global\",\n    filename: str = None,\n    base_dir=None,\n    database_user=None,\n    set_password=True,\n    **kwargs,\n):\n    \"\"\"Set the dj.config parameters, set password, and save config to file.\n\n    Parameters\n    ----------\n    save_method : {'local', 'global', 'custom'}, optional\n        The method to use to save the config. If either 'local' or 'global',\n        datajoint builtins will be used to save.\n    filename : str or Path, optional\n        Default to datajoint global config. If save_method = 'custom', name\n        of file to generate. Must end in either be either yaml or json.\n    base_dir : str, optional\n        The base directory. If not provided, will default to the env var\n    database_user : str, optional\n        The database user. If not provided, resulting config will not\n        specify.\n    set_password : bool, optional\n        Default True. Set the database password.\n    \"\"\"\n    if save_method == \"local\":\n        filepath = Path(\".\") / dj.settings.LOCALCONFIG\n    elif not filename or save_method == \"global\":\n        save_method = \"global\"\n        filepath = Path(\"~\").expanduser() / dj.settings.GLOBALCONFIG\n\n    dj.config.update(\n        self._generate_dj_config(\n            base_dir=base_dir, database_user=database_user, **kwargs\n        )\n    )\n\n    if set_password:\n        try:\n            dj.set_password()\n        except OperationalError as e:\n            warnings.warn(f\"Database connection issues. Wrong pass? {e}\")\n            # NOTE: Save anyway? Or raise error?\n\n    user_warn = (\n        f\"Replace existing file? {filepath.resolve()}\\n\\t\"\n        + \"\\n\\t\".join([f\"{k}: {v}\" for k, v in config.items()])\n        + \"\\n\"\n    )\n\n    if filepath.exists() and dj.utils.user_choice(user_warn)[0] != \"y\":\n        return dj.config\n\n    if save_method == \"global\":\n        dj.config.save_global(verbose=True)\n        return\n\n    if save_method == \"local\":\n        dj.config.save_local(verbose=True)\n        return\n\n    with open(filename, \"w\") as outfile:\n        if filename.endswith(\"json\"):\n            json.dump(dj.config, outfile, indent=2)\n        else:\n            yaml.dump(dj.config, outfile, default_flow_style=False)\n</code></pre>"}, {"location": "api/src/spyglass/cli/cli/", "title": "cli.py", "text": ""}, {"location": "api/src/spyglass/common/common_behav/", "title": "common_behav.py", "text": ""}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.PositionSource", "title": "<code>PositionSource</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass PositionSource(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    -&gt; IntervalList\n    ---\n    source: varchar(200)             # source of data (e.g., trodes, dlc)\n    import_file_name: varchar(2000)  # path to import file if importing\n    \"\"\"\n\n    class SpatialSeries(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        id = 0 : int unsigned            # index of spatial series\n        ---\n        name=null: varchar(32)       # name of spatial series\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwb_file_name):\n        \"\"\"Add intervals to ItervalList and PositionSource.\n\n        Given an NWB file name, get the spatial series and interval lists from\n        the file, add the interval lists to the IntervalList table, and\n        populate the RawPosition table if possible.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        \"\"\"\n        nwbf = get_nwb_file(nwb_file_name)\n        all_pos = get_all_spatial_series(nwbf, verbose=True)\n        sess_key = Nwbfile.get_file_key(nwb_file_name)\n        src_key = dict(**sess_key, source=\"trodes\", import_file_name=\"\")\n\n        if all_pos is None:\n            return\n\n        sources = []\n        intervals = []\n        spat_series = []\n\n        for epoch, epoch_list in all_pos.items():\n            ind_key = dict(interval_list_name=cls.get_pos_interval_name(epoch))\n\n            sources.append(dict(**src_key, **ind_key))\n            intervals.append(\n                dict(\n                    **sess_key,\n                    **ind_key,\n                    valid_times=epoch_list[0][\"valid_times\"],\n                )\n            )\n\n            for index, pdict in enumerate(epoch_list):\n                spat_series.append(\n                    dict(\n                        **sess_key,\n                        **ind_key,\n                        id=index,\n                        name=pdict.get(\"name\"),\n                    )\n                )\n\n        with cls.connection.transaction:\n            IntervalList.insert(intervals)\n            cls.insert(sources)\n            cls.SpatialSeries.insert(spat_series)\n\n        # make map from epoch intervals to position intervals\n        populate_position_interval_map_session(nwb_file_name)\n\n    @staticmethod\n    def get_pos_interval_name(epoch_num: int) -&gt; str:\n        \"\"\"Return string of the interval name from the epoch number.\n\n        Parameters\n        ----------\n        epoch_num : int\n            Input epoch number\n\n        Returns\n        -------\n        str\n            Position interval name (e.g., pos 2 valid times)\n        \"\"\"\n        try:\n            int(epoch_num)\n        except ValueError:\n            raise ValueError(\n                f\"Epoch number must must be an integer. Received: {epoch_num}\"\n            )\n        return f\"pos {epoch_num} valid times\"\n\n    @staticmethod\n    def _is_valid_name(name) -&gt; bool:\n        return name.startswith(\"pos \") and name.endswith(\" valid times\")\n\n    @staticmethod\n    def get_epoch_num(name: str) -&gt; int:\n        \"\"\"Return the epoch number from the interval name.\n\n        Parameters\n        ----------\n        name : str\n            Name of position interval (e.g., pos epoch 1 index 2 valid times)\n\n        Returns\n        -------\n        int\n            epoch number\n        \"\"\"\n        if not PositionSource._is_valid_name(name):\n            raise ValueError(f\"Invalid interval name: {name}\")\n        return int(name.replace(\"pos \", \"\").replace(\" valid times\", \"\"))\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.PositionSource.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add intervals to ItervalList and PositionSource.</p> <p>Given an NWB file name, get the spatial series and interval lists from the file, add the interval lists to the IntervalList table, and populate the RawPosition table if possible.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwb_file_name):\n    \"\"\"Add intervals to ItervalList and PositionSource.\n\n    Given an NWB file name, get the spatial series and interval lists from\n    the file, add the interval lists to the IntervalList table, and\n    populate the RawPosition table if possible.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    \"\"\"\n    nwbf = get_nwb_file(nwb_file_name)\n    all_pos = get_all_spatial_series(nwbf, verbose=True)\n    sess_key = Nwbfile.get_file_key(nwb_file_name)\n    src_key = dict(**sess_key, source=\"trodes\", import_file_name=\"\")\n\n    if all_pos is None:\n        return\n\n    sources = []\n    intervals = []\n    spat_series = []\n\n    for epoch, epoch_list in all_pos.items():\n        ind_key = dict(interval_list_name=cls.get_pos_interval_name(epoch))\n\n        sources.append(dict(**src_key, **ind_key))\n        intervals.append(\n            dict(\n                **sess_key,\n                **ind_key,\n                valid_times=epoch_list[0][\"valid_times\"],\n            )\n        )\n\n        for index, pdict in enumerate(epoch_list):\n            spat_series.append(\n                dict(\n                    **sess_key,\n                    **ind_key,\n                    id=index,\n                    name=pdict.get(\"name\"),\n                )\n            )\n\n    with cls.connection.transaction:\n        IntervalList.insert(intervals)\n        cls.insert(sources)\n        cls.SpatialSeries.insert(spat_series)\n\n    # make map from epoch intervals to position intervals\n    populate_position_interval_map_session(nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.PositionSource.get_pos_interval_name", "title": "<code>get_pos_interval_name(epoch_num)</code>  <code>staticmethod</code>", "text": "<p>Return string of the interval name from the epoch number.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_num</code> <code>int</code> <p>Input epoch number</p> required <p>Returns:</p> Type Description <code>str</code> <p>Position interval name (e.g., pos 2 valid times)</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@staticmethod\ndef get_pos_interval_name(epoch_num: int) -&gt; str:\n    \"\"\"Return string of the interval name from the epoch number.\n\n    Parameters\n    ----------\n    epoch_num : int\n        Input epoch number\n\n    Returns\n    -------\n    str\n        Position interval name (e.g., pos 2 valid times)\n    \"\"\"\n    try:\n        int(epoch_num)\n    except ValueError:\n        raise ValueError(\n            f\"Epoch number must must be an integer. Received: {epoch_num}\"\n        )\n    return f\"pos {epoch_num} valid times\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.PositionSource.get_epoch_num", "title": "<code>get_epoch_num(name)</code>  <code>staticmethod</code>", "text": "<p>Return the epoch number from the interval name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of position interval (e.g., pos epoch 1 index 2 valid times)</p> required <p>Returns:</p> Type Description <code>int</code> <p>epoch number</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@staticmethod\ndef get_epoch_num(name: str) -&gt; int:\n    \"\"\"Return the epoch number from the interval name.\n\n    Parameters\n    ----------\n    name : str\n        Name of position interval (e.g., pos epoch 1 index 2 valid times)\n\n    Returns\n    -------\n    int\n        epoch number\n    \"\"\"\n    if not PositionSource._is_valid_name(name):\n        raise ValueError(f\"Invalid interval name: {name}\")\n    return int(name.replace(\"pos \", \"\").replace(\" valid times\", \"\"))\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>             Bases: <code>Imported</code></p>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n    \"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    \"\"\"\n\n    class PosObject(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        -&gt; PositionSource.SpatialSeries.proj('id')\n        ---\n        raw_position_object_id: varchar(40) # id of spatial series in NWB file\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs\n            )\n\n        def fetch1_dataframe(self):\n            INDEX_ADJUST = 1  # adjust 0-index to 1-index (e.g., xloc0 -&gt; xloc1)\n\n            id_rp = [(n[\"id\"], n[\"raw_position\"]) for n in self.fetch_nwb()]\n\n            if len(set(rp.interval for _, rp in id_rp)) &gt; 1:\n                print(\"WARNING: loading DataFrame with multiple intervals.\")\n\n            df_list = [\n                pd.DataFrame(\n                    data=rp.data,\n                    index=pd.Index(rp.timestamps, name=\"time\"),\n                    columns=[\n                        col  # use existing columns if already numbered\n                        if \"1\" in rp.description or \"2\" in rp.description\n                        # else number them by id\n                        else col + str(id + INDEX_ADJUST)\n                        for col in rp.description.split(\", \")\n                    ],\n                )\n                for id, rp in id_rp\n            ]\n\n            return reduce(lambda x, y: pd.merge(x, y, on=\"time\"), df_list)\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        interval_list_name = key[\"interval_list_name\"]\n\n        nwbf = get_nwb_file(nwb_file_name)\n        indices = (PositionSource.SpatialSeries &amp; key).fetch(\"id\")\n\n        # incl_times = False -&gt; don't do extra processing for valid_times\n        spat_objs = get_all_spatial_series(nwbf, incl_times=False)[\n            PositionSource.get_epoch_num(interval_list_name)\n        ]\n\n        self.insert1(key)\n        self.PosObject.insert(\n            [\n                dict(\n                    nwb_file_name=nwb_file_name,\n                    interval_list_name=interval_list_name,\n                    id=index,\n                    raw_position_object_id=obj[\"raw_position_object_id\"],\n                )\n                for index, obj in enumerate(spat_objs)\n                if index in indices\n            ]\n        )\n\n    def fetch_nwb(self, *attrs, **kwargs) -&gt; list:\n        \"\"\"\n        Returns a condatenated list of nwb objects from RawPosition.PosObject\n        \"\"\"\n        return (\n            self.PosObject()\n            .restrict(self.restriction)  # Avoids fetch_nwb on whole table\n            .fetch_nwb(*attrs, **kwargs)\n        )\n\n    def fetch1_dataframe(self):\n        \"\"\"Returns a dataframe with all RawPosition.PosObject items.\n\n        Uses interval_list_name as column index.\n        \"\"\"\n        ret = {}\n\n        pos_obj_set = self.PosObject &amp; self.restriction\n        unique_intervals = set(pos_obj_set.fetch(\"interval_list_name\"))\n\n        for interval in unique_intervals:\n            ret[interval] = (\n                pos_obj_set &amp; {\"interval_list_name\": interval}\n            ).fetch1_dataframe()\n\n        if len(unique_intervals) == 1:\n            return next(iter(ret.values()))\n\n        return pd.concat(ret, axis=1)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.RawPosition.fetch_nwb", "title": "<code>fetch_nwb(*attrs, **kwargs)</code>", "text": "<p>Returns a condatenated list of nwb objects from RawPosition.PosObject</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>def fetch_nwb(self, *attrs, **kwargs) -&gt; list:\n    \"\"\"\n    Returns a condatenated list of nwb objects from RawPosition.PosObject\n    \"\"\"\n    return (\n        self.PosObject()\n        .restrict(self.restriction)  # Avoids fetch_nwb on whole table\n        .fetch_nwb(*attrs, **kwargs)\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.RawPosition.fetch1_dataframe", "title": "<code>fetch1_dataframe()</code>", "text": "<p>Returns a dataframe with all RawPosition.PosObject items.</p> <p>Uses interval_list_name as column index.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>def fetch1_dataframe(self):\n    \"\"\"Returns a dataframe with all RawPosition.PosObject items.\n\n    Uses interval_list_name as column index.\n    \"\"\"\n    ret = {}\n\n    pos_obj_set = self.PosObject &amp; self.restriction\n    unique_intervals = set(pos_obj_set.fetch(\"interval_list_name\"))\n\n    for interval in unique_intervals:\n        ret[interval] = (\n            pos_obj_set &amp; {\"interval_list_name\": interval}\n        ).fetch1_dataframe()\n\n    if len(unique_intervals) == 1:\n        return next(iter(ret.values()))\n\n    return pd.concat(ret, axis=1)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.StateScriptFile", "title": "<code>StateScriptFile</code>", "text": "<p>             Bases: <code>Imported</code></p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass StateScriptFile(dj.Imported):\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    ---\n    file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n        \"\"\"Add a new row to the StateScriptFile table.\"\"\"\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        associated_files = nwbf.processing.get(\n            \"associated_files\"\n        ) or nwbf.processing.get(\"associated files\")\n        if associated_files is None:\n            print(\n                \"Unable to import StateScriptFile: no processing module named \"\n                + '\"associated_files\" found in {nwb_file_name}.'\n            )\n            return\n\n        for associated_file_obj in associated_files.data_interfaces.values():\n            if not isinstance(\n                associated_file_obj, ndx_franklab_novela.AssociatedFiles\n            ):\n                print(\n                    f\"Data interface {associated_file_obj.name} within \"\n                    + '\"associated_files\" processing module is not '\n                    + \"of expected type ndx_franklab_novela.AssociatedFiles\\n\"\n                )\n                return\n\n            # parse the task_epochs string\n            # TODO: update associated_file_obj.task_epochs to be an array of\n            # 1-based ints, not a comma-separated string of ints\n\n            epoch_list = associated_file_obj.task_epochs.split(\",\")\n            # only insert if this is the statescript file\n            print(associated_file_obj.description)\n            if (\n                \"statescript\".upper() in associated_file_obj.description.upper()\n                or \"state_script\".upper()\n                in associated_file_obj.description.upper()\n                or \"state script\".upper()\n                in associated_file_obj.description.upper()\n            ):\n                # find the file associated with this epoch\n                if str(key[\"epoch\"]) in epoch_list:\n                    key[\"file_object_id\"] = associated_file_obj.object_id\n                    self.insert1(key)\n            else:\n                print(\"not a statescript file\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.StateScriptFile.make", "title": "<code>make(key)</code>", "text": "<p>Add a new row to the StateScriptFile table.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>def make(self, key):\n    \"\"\"Add a new row to the StateScriptFile table.\"\"\"\n    nwb_file_name = key[\"nwb_file_name\"]\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n\n    associated_files = nwbf.processing.get(\n        \"associated_files\"\n    ) or nwbf.processing.get(\"associated files\")\n    if associated_files is None:\n        print(\n            \"Unable to import StateScriptFile: no processing module named \"\n            + '\"associated_files\" found in {nwb_file_name}.'\n        )\n        return\n\n    for associated_file_obj in associated_files.data_interfaces.values():\n        if not isinstance(\n            associated_file_obj, ndx_franklab_novela.AssociatedFiles\n        ):\n            print(\n                f\"Data interface {associated_file_obj.name} within \"\n                + '\"associated_files\" processing module is not '\n                + \"of expected type ndx_franklab_novela.AssociatedFiles\\n\"\n            )\n            return\n\n        # parse the task_epochs string\n        # TODO: update associated_file_obj.task_epochs to be an array of\n        # 1-based ints, not a comma-separated string of ints\n\n        epoch_list = associated_file_obj.task_epochs.split(\",\")\n        # only insert if this is the statescript file\n        print(associated_file_obj.description)\n        if (\n            \"statescript\".upper() in associated_file_obj.description.upper()\n            or \"state_script\".upper()\n            in associated_file_obj.description.upper()\n            or \"state script\".upper()\n            in associated_file_obj.description.upper()\n        ):\n            # find the file associated with this epoch\n            if str(key[\"epoch\"]) in epoch_list:\n                key[\"file_object_id\"] = associated_file_obj.object_id\n                self.insert1(key)\n        else:\n            print(\"not a statescript file\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.VideoFile", "title": "<code>VideoFile</code>", "text": "<p>             Bases: <code>Imported</code></p>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.VideoFile--notes", "title": "Notes", "text": "<p>The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used. If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass VideoFile(dj.Imported):\n    \"\"\"\n\n    Notes\n    -----\n    The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is\n    used. If PTP is not used, the video timestamps come from\n    videoTimeStamps.cameraHWFrameCount .\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    video_file_num = 0: int\n    ---\n    camera_name: varchar(80)\n    video_file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n        self._no_transaction_make(key)\n\n    def _no_transaction_make(self, key, verbose=True):\n        if not self.connection.in_transaction:\n            self.populate(key)\n            return\n\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        videos = get_data_interface(\n            nwbf, \"video\", pynwb.behavior.BehavioralEvents\n        )\n\n        if videos is None:\n            print(f\"No video data interface found in {nwb_file_name}\\n\")\n            return\n        else:\n            videos = videos.time_series\n\n        # get the interval for the current TaskEpoch\n        interval_list_name = (TaskEpoch() &amp; key).fetch1(\"interval_list_name\")\n        valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        is_found = False\n        for ind, video in enumerate(videos.values()):\n            if isinstance(video, pynwb.image.ImageSeries):\n                video = [video]\n            for video_obj in video:\n                # check to see if the times for this video_object are largely\n                # overlapping with the task epoch times\n\n                if len(\n                    interval_list_contains(valid_times, video_obj.timestamps)\n                    &gt; 0.9 * len(video_obj.timestamps)\n                ):\n                    key[\"video_file_num\"] = ind\n                    camera_name = video_obj.device.camera_name\n                    if CameraDevice &amp; {\"camera_name\": camera_name}:\n                        key[\"camera_name\"] = video_obj.device.camera_name\n                    else:\n                        raise KeyError(\n                            f\"No camera with camera_name: {camera_name} found \"\n                            + \"in CameraDevice table.\"\n                        )\n                    key[\"video_file_object_id\"] = video_obj.object_id\n                    self.insert1(key)\n                    is_found = True\n\n        if not is_found and verbose:\n            print(\n                f\"No video found corresponding to file {nwb_file_name}, \"\n                + f\"epoch {interval_list_name}\"\n            )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_name\"):\n                continue\n            video_nwb = (cls &amp; row).fetch_nwb()[0]\n            if len(video_nwb) != 1:\n                raise ValueError(\n                    f\"Expecting 1 video file per entry. {len(video_nwb)} found\"\n                )\n            row[\"camera_name\"] = video_nwb[0][\"video_file\"].device.camera_name\n            cls.update1(row=row)\n\n    @classmethod\n    def get_abs_path(cls, key: Dict):\n        \"\"\"Return the absolute path for a stored video file given a key.\n\n        Key must include the nwb_file_name and epoch number. The\n        SPYGLASS_VIDEO_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        key : dict\n            dictionary with nwb_file_name and epoch as keys\n\n        Returns\n        -------\n        nwb_video_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n        assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n        if not video_dir.exists():\n            raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n        video_info = (cls &amp; key).fetch1()\n        nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n        nwbf = get_nwb_file(nwb_path)\n        nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n        video_filename = nwb_video.name\n        # see if the file exists and is stored in the base analysis dir\n        nwb_video_file_abspath = pathlib.Path(\n            f\"{video_dir}/{pathlib.Path(video_filename)}\"\n        )\n        if nwb_video_file_abspath.exists():\n            return nwb_video_file_abspath.as_posix()\n        else:\n            raise FileNotFoundError(\n                f\"video file with filename: {video_filename} \"\n                f\"does not exist in {video_dir}/\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.VideoFile.get_abs_path", "title": "<code>get_abs_path(key)</code>  <code>classmethod</code>", "text": "<p>Return the absolute path for a stored video file given a key.</p> <p>Key must include the nwb_file_name and epoch number. The SPYGLASS_VIDEO_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary with nwb_file_name and epoch as keys</p> required <p>Returns:</p> Name Type Description <code>nwb_video_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef get_abs_path(cls, key: Dict):\n    \"\"\"Return the absolute path for a stored video file given a key.\n\n    Key must include the nwb_file_name and epoch number. The\n    SPYGLASS_VIDEO_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    key : dict\n        dictionary with nwb_file_name and epoch as keys\n\n    Returns\n    -------\n    nwb_video_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n    assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n    if not video_dir.exists():\n        raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n    video_info = (cls &amp; key).fetch1()\n    nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n    nwbf = get_nwb_file(nwb_path)\n    nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n    video_filename = nwb_video.name\n    # see if the file exists and is stored in the base analysis dir\n    nwb_video_file_abspath = pathlib.Path(\n        f\"{video_dir}/{pathlib.Path(video_filename)}\"\n    )\n    if nwb_video_file_abspath.exists():\n        return nwb_video_file_abspath.as_posix()\n    else:\n        raise FileNotFoundError(\n            f\"video file with filename: {video_filename} \"\n            f\"does not exist in {video_dir}/\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.convert_epoch_interval_name_to_position_interval_name", "title": "<code>convert_epoch_interval_name_to_position_interval_name(key, populate_missing=True)</code>", "text": "<p>Converts IntervalList key to the corresponding position interval name.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Lookup key</p> required <code>populate_missing</code> <code>bool</code> <p>Whether to populate PositionIntervalMap for the key if missing. Should be False if this function is used inside of another populate call. Defaults to True</p> <code>True</code> <p>Returns:</p> Name Type Description <code>position_interval_name</code> <code>str</code> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>def convert_epoch_interval_name_to_position_interval_name(\n    key: dict, populate_missing: bool = True\n) -&gt; str:\n    \"\"\"Converts IntervalList key to the corresponding position interval name.\n\n    Parameters\n    ----------\n    key : dict\n        Lookup key\n    populate_missing: bool\n        Whether to populate PositionIntervalMap for the key if missing. Should\n        be False if this function is used inside of another populate call.\n        Defaults to True\n\n    Returns\n    -------\n    position_interval_name : str\n    \"\"\"\n    # get the interval list name if given epoch but not interval list name\n    if \"interval_list_name\" not in key and \"epoch\" in key:\n        key[\"interval_list_name\"] = get_interval_list_name_from_epoch(\n            key[\"nwb_file_name\"], key[\"epoch\"]\n        )\n\n    pos_query = PositionIntervalMap &amp; key\n\n    if len(pos_query) == 0:\n        if populate_missing:\n            PositionIntervalMap()._no_transaction_make(key)\n        else:\n            raise KeyError(\n                f\"{key} must be populated in the PositionIntervalMap table \"\n                + \"prior to your current populate call\"\n            )\n\n    if len(pos_query) == 0:\n        print(f\"No position intervals found for {key}\")\n        return []\n\n    if len(pos_query) == 1:\n        return pos_query.fetch1(\"position_interval_name\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.get_interval_list_name_from_epoch", "title": "<code>get_interval_list_name_from_epoch(nwb_file_name, epoch)</code>", "text": "<p>Returns the interval list name for the given epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required <code>epoch</code> <code>int</code> <p>The epoch number.</p> required <p>Returns:</p> Name Type Description <code>interval_list_name</code> <code>str</code> <p>The interval list name.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>def get_interval_list_name_from_epoch(nwb_file_name: str, epoch: int) -&gt; str:\n    \"\"\"Returns the interval list name for the given epoch.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    epoch : int\n        The epoch number.\n\n    Returns\n    -------\n    interval_list_name : str\n        The interval list name.\n    \"\"\"\n    interval_names = [\n        x\n        for x in (IntervalList() &amp; {\"nwb_file_name\": nwb_file_name}).fetch(\n            \"interval_list_name\"\n        )\n        if (x.split(\"_\")[0] == f\"{epoch:02}\")\n    ]\n\n    if len(interval_names) != 1:\n        print(\n            f\"Found {len(interval_name)} interval list names found for \"\n            + f\"{nwb_file_name} epoch {epoch}\"\n        )\n        return None\n\n    return interval_names[0]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/", "title": "common_device.py", "text": ""}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.DataAcquisitionDevice", "title": "<code>DataAcquisitionDevice</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass DataAcquisitionDevice(dj.Manual):\n    definition = \"\"\"\n    data_acquisition_device_name: varchar(80)\n    ---\n    -&gt; DataAcquisitionDeviceSystem\n    -&gt; DataAcquisitionDeviceAmplifier\n    adc_circuit = NULL: varchar(2000)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n        \"\"\"Insert data acquisition devices from an NWB file.\n\n        Note that this does not link the DataAcquisitionDevices with a Session.\n        For that, see DataAcquisitionDeviceList.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to\n            replace in the NWB file.\n        \"\"\"\n        _, ndx_devices, _ = cls.get_all_device_names(nwbf, config)\n\n        for device_name in ndx_devices:\n            new_device_dict = dict()\n\n            # read device properties into new_device_dict from PyNWB extension\n            # device object\n            nwb_device_obj = ndx_devices[device_name]\n\n            name = nwb_device_obj.name\n            adc_circuit = nwb_device_obj.adc_circuit\n\n            # transform system value. check if value is in DB. if not, prompt\n            # user to add an entry or cancel.\n            system = cls._add_system(nwb_device_obj.system)\n\n            # transform amplifier value. check if value is in DB. if not, prompt\n            # user to add an entry or cancel.\n            amplifier = cls._add_amplifier(nwb_device_obj.amplifier)\n\n            # standardize how Intan is represented in the database\n            if adc_circuit.title() == \"Intan\":\n                adc_circuit = \"Intan\"\n\n            new_device_dict[\"data_acquisition_device_name\"] = name\n            new_device_dict[\"data_acquisition_device_system\"] = system\n            new_device_dict[\"data_acquisition_device_amplifier\"] = amplifier\n            new_device_dict[\"adc_circuit\"] = adc_circuit\n\n            cls._add_device(new_device_dict)\n\n        if ndx_devices:\n            print(\n                \"Inserted or referenced data acquisition device(s): \"\n                + f\"{ndx_devices.keys()}\"\n            )\n        else:\n            print(\"No conforming data acquisition device metadata found.\")\n\n    @classmethod\n    def get_all_device_names(cls, nwbf, config) -&gt; tuple:\n        \"\"\"\n        Return device names in the NWB file, after appending and overwriting by\n        the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to\n            replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : tuple\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n        # make a dict mapping device name to PyNWB device object for all devices\n        # in the NWB file that are of type ndx_franklab_novela.DataAcqDevice and\n        # thus have the required metadata\n        ndx_devices = {\n            device_obj.name: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.DataAcqDevice)\n        }\n\n        # make a list of device names that are associated with this NWB file\n        if \"DataAcquisitionDevice\" in config:\n            config_devices = [\n                device_dict[\"data_acquisition_device_name\"]\n                for device_dict in config[\"DataAcquisitionDevice\"]\n            ]\n        else:\n            config_devices = list()\n\n        all_device_names = set(ndx_devices.keys()).union(set(config_devices))\n\n        return all_device_names, ndx_devices, config_devices\n\n    @classmethod\n    def _add_device(cls, new_device_dict):\n        \"\"\"Ensure match between NWB file info &amp; database entry.\n\n        If no DataAcquisitionDevice with the given name exists in the database,\n        check whether the user wants to add a new entry instead of referencing\n        an existing entry. If so, return. If not, raise an exception.\n\n        Parameters\n        ----------\n        new_device_dict : dict\n            Dict of new device properties\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device to the database when prompted or\n            if the device properties from the NWB file do not match the\n            properties of the corresponding database entry.\n        \"\"\"\n        name = new_device_dict[\"data_acquisition_device_name\"]\n        all_values = DataAcquisitionDevice.fetch(\n            \"data_acquisition_device_name\"\n        ).tolist()\n        if name not in all_values:\n            # no entry with the same name exists, prompt user to add a new entry\n            print(\n                f\"\\nData acquisition device '{name}' was not found in the \"\n                f\"database. The current values are: {all_values}. \"\n                \"Please ensure that the device you want to add does not already\"\n                \" exist in the database under a different name or spelling. \"\n                \"If you want to use an existing device in the database, \"\n                \"please change the corresponding Device object in the NWB file.\"\n                \" Entering 'N' will raise an exception.\"\n            )\n            to_db = \" to the database\"\n            val = input(f\"Add data acquisition device '{name}'{to_db}? (y/N)\")\n            if val.lower() in [\"y\", \"yes\"]:\n                cls.insert1(new_device_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add device '{name}'{to_db}.\"\n            )\n\n        # Check if values provided match the values stored in the database\n        db_dict = (\n            DataAcquisitionDevice &amp; {\"data_acquisition_device_name\": name}\n        ).fetch1()\n        if db_dict != new_device_dict:\n            raise PopulateException(\n                \"Data acquisition device properties of PyNWB Device object \"\n                + f\"with name '{name}': {new_device_dict} do not match \"\n                f\"properties of the corresponding database entry: {db_dict}.\"\n            )\n\n    @classmethod\n    def _add_system(cls, system):\n        \"\"\"Check the system value. If not in the db, prompt user to add it.\n\n        This method also renames the system value \"MCU\" to \"SpikeGadgets\".\n\n        Parameters\n        ----------\n        system : str\n            The system value to check.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device system value to the database\n            when prompted.\n\n        Returns\n        -------\n        system : str\n            The system value that was added to the database.\n        \"\"\"\n        if system == \"MCU\":\n            system = \"SpikeGadgets\"\n\n        all_values = DataAcquisitionDeviceSystem.fetch(\n            \"data_acquisition_device_system\"\n        ).tolist()\n        if system not in all_values:\n            print(\n                f\"\\nData acquisition device system '{system}' was not found in\"\n                f\" the database. The current values are: {all_values}. \"\n                \"Please ensure that the system you want to add does not already\"\n                \" exist in the database under a different name or spelling. \"\n                \"If you want to use an existing system in the database, \"\n                \"please change the corresponding Device object in the NWB file.\"\n                \" Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add data acquisition device system '{system}'\"\n                + \" to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                key = {\"data_acquisition_device_system\": system}\n                DataAcquisitionDeviceSystem.insert1(key, skip_duplicates=True)\n            else:\n                raise PopulateException(\n                    \"User chose not to add data acquisition device system \"\n                    + f\"'{system}' to the database.\"\n                )\n        return system\n\n    @classmethod\n    def _add_amplifier(cls, amplifier):\n        \"\"\"Check amplifier value. If not in db, prompt user to add.\n\n        Parameters\n        ----------\n        amplifier : str\n            The amplifier value to check.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device amplifier value to the database\n            when prompted.\n\n        Returns\n        -------\n        amplifier : str\n            The amplifier value that was added to the database.\n        \"\"\"\n        # standardize how Intan is represented in the database\n        if amplifier.title() == \"Intan\":\n            amplifier = \"Intan\"\n\n        all_values = DataAcquisitionDeviceAmplifier.fetch(\n            \"data_acquisition_device_amplifier\"\n        ).tolist()\n        if amplifier not in all_values:\n            print(\n                f\"\\nData acquisition device amplifier '{amplifier}' was not \"\n                f\"found in the database. The current values are: {all_values}. \"\n                \"Please ensure that the amplifier you want to add does not \"\n                \"already exist in the database under a different name or \"\n                \"spelling. If you want to use an existing name in the database,\"\n                \" please change the corresponding Device object in the NWB \"\n                \"file. Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                \"Do you want to add data acquisition device amplifier \"\n                + f\"'{amplifier}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                key = {\"data_acquisition_device_amplifier\": amplifier}\n                DataAcquisitionDeviceAmplifier.insert1(\n                    key, skip_duplicates=True\n                )\n            else:\n                raise PopulateException(\n                    \"User chose not to add data acquisition device amplifier \"\n                    + f\"'{amplifier}' to the database.\"\n                )\n        return amplifier\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.DataAcquisitionDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert data acquisition devices from an NWB file.</p> <p>Note that this does not link the DataAcquisitionDevices with a Session. For that, see DataAcquisitionDeviceList.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n    \"\"\"Insert data acquisition devices from an NWB file.\n\n    Note that this does not link the DataAcquisitionDevices with a Session.\n    For that, see DataAcquisitionDeviceList.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to\n        replace in the NWB file.\n    \"\"\"\n    _, ndx_devices, _ = cls.get_all_device_names(nwbf, config)\n\n    for device_name in ndx_devices:\n        new_device_dict = dict()\n\n        # read device properties into new_device_dict from PyNWB extension\n        # device object\n        nwb_device_obj = ndx_devices[device_name]\n\n        name = nwb_device_obj.name\n        adc_circuit = nwb_device_obj.adc_circuit\n\n        # transform system value. check if value is in DB. if not, prompt\n        # user to add an entry or cancel.\n        system = cls._add_system(nwb_device_obj.system)\n\n        # transform amplifier value. check if value is in DB. if not, prompt\n        # user to add an entry or cancel.\n        amplifier = cls._add_amplifier(nwb_device_obj.amplifier)\n\n        # standardize how Intan is represented in the database\n        if adc_circuit.title() == \"Intan\":\n            adc_circuit = \"Intan\"\n\n        new_device_dict[\"data_acquisition_device_name\"] = name\n        new_device_dict[\"data_acquisition_device_system\"] = system\n        new_device_dict[\"data_acquisition_device_amplifier\"] = amplifier\n        new_device_dict[\"adc_circuit\"] = adc_circuit\n\n        cls._add_device(new_device_dict)\n\n    if ndx_devices:\n        print(\n            \"Inserted or referenced data acquisition device(s): \"\n            + f\"{ndx_devices.keys()}\"\n        )\n    else:\n        print(\"No conforming data acquisition device metadata found.\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.DataAcquisitionDevice.get_all_device_names", "title": "<code>get_all_device_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Return device names in the NWB file, after appending and overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>tuple</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_device_names(cls, nwbf, config) -&gt; tuple:\n    \"\"\"\n    Return device names in the NWB file, after appending and overwriting by\n    the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to\n        replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : tuple\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n    # make a dict mapping device name to PyNWB device object for all devices\n    # in the NWB file that are of type ndx_franklab_novela.DataAcqDevice and\n    # thus have the required metadata\n    ndx_devices = {\n        device_obj.name: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.DataAcqDevice)\n    }\n\n    # make a list of device names that are associated with this NWB file\n    if \"DataAcquisitionDevice\" in config:\n        config_devices = [\n            device_dict[\"data_acquisition_device_name\"]\n            for device_dict in config[\"DataAcquisitionDevice\"]\n        ]\n    else:\n        config_devices = list()\n\n    all_device_names = set(ndx_devices.keys()).union(set(config_devices))\n\n    return all_device_names, ndx_devices, config_devices\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.CameraDevice", "title": "<code>CameraDevice</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass CameraDevice(dj.Manual):\n    definition = \"\"\"\n    camera_name: varchar(80)\n    ---\n    meters_per_pixel = 0: float  # height / width of pixel in meters\n    manufacturer = \"\": varchar(2000)\n    model = \"\": varchar(2000)\n    lens = \"\": varchar(2000)\n    camera_id = -1: int\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n        \"\"\"Insert camera devices from an NWB file\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n\n        Returns\n        -------\n        device_name_list : list\n            List of camera device object names found in the NWB file.\n        \"\"\"\n        device_name_list = list()\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                device_dict = dict()\n                # TODO ideally the ID is not encoded in the name formatted in a\n                # particular way device.name must have the form \"[any string\n                # without a space, usually camera] [int]\"\n                device_dict = {\n                    \"camera_id\": int(device.name.split()[1]),\n                    \"camera_name\": device.camera_name,\n                    \"manufacturer\": device.manufacturer,\n                    \"model\": device.model,\n                    \"lens\": device.lens,\n                    \"meters_per_pixel\": device.meters_per_pixel,\n                }\n                cls.insert1(device_dict, skip_duplicates=True)\n                device_name_list.append(device_dict[\"camera_name\"])\n        if device_name_list:\n            print(f\"Inserted camera devices {device_name_list}\")\n        else:\n            print(\"No conforming camera device metadata found.\")\n        return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.CameraDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert camera devices from an NWB file</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of camera device object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n    \"\"\"Insert camera devices from an NWB file\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n\n    Returns\n    -------\n    device_name_list : list\n        List of camera device object names found in the NWB file.\n    \"\"\"\n    device_name_list = list()\n    for device in nwbf.devices.values():\n        if isinstance(device, ndx_franklab_novela.CameraDevice):\n            device_dict = dict()\n            # TODO ideally the ID is not encoded in the name formatted in a\n            # particular way device.name must have the form \"[any string\n            # without a space, usually camera] [int]\"\n            device_dict = {\n                \"camera_id\": int(device.name.split()[1]),\n                \"camera_name\": device.camera_name,\n                \"manufacturer\": device.manufacturer,\n                \"model\": device.model,\n                \"lens\": device.lens,\n                \"meters_per_pixel\": device.meters_per_pixel,\n            }\n            cls.insert1(device_dict, skip_duplicates=True)\n            device_name_list.append(device_dict[\"camera_name\"])\n    if device_name_list:\n        print(f\"Inserted camera devices {device_name_list}\")\n    else:\n        print(\"No conforming camera device metadata found.\")\n    return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe", "title": "<code>Probe</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass Probe(dj.Manual):\n    definition = \"\"\"\n    # A configuration of a ProbeType. For most probe types, there is only one,\n    # which should always be used. For Neuropixels, the channel map (which\n    # electrodes used, where they are, and in what order) can differ between\n    # users and sessions. Each config should have a different ProbeType.\n    probe_id: varchar(80)     # a unique ID for this probe &amp; dynamic config\n    ---\n    -&gt; ProbeType              # Type of probe, selected from a controlled list\n    -&gt; [nullable] DataAcquisitionDevice  # the data acquisition device used\n    contact_side_numbering: enum(\"True\", \"False\")  # Facing you when numbering\n    \"\"\"\n\n    class Shank(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe\n        probe_shank: int              # unique shank number within probe.\n        \"\"\"\n\n    class Electrode(dj.Part):\n        definition = \"\"\"\n        # Electrode configuration, with ID, contact size, X/Y/Z coordinates\n        -&gt; Probe.Shank\n        probe_electrode: int          # electrode ID, output from acquisition\n                                      # system. Unique within a Probe\n        ---\n        contact_size = NULL: float    # (um) contact size\n        rel_x = NULL: float           # (um) x coordinate of electrode\n        rel_y = NULL: float           # (um) y coordinate of electrode\n        rel_z = NULL: float           # (um) z coordinate of electrode\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n        \"\"\"Insert probe devices from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to\n            replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of probe device types found in the NWB file.\n        \"\"\"\n        all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n        for probe_type in all_probes_types:\n            new_probe_type_dict = dict()\n            new_probe_dict = dict()\n            shank_dict = dict()\n            elect_dict = dict()\n            num_shanks = 0\n\n            if probe_type in ndx_probes:\n                # read probe properties into new_probe_dict from PyNWB extension\n                # probe object\n                nwb_probe_obj = ndx_probes[probe_type]\n                cls.__read_ndx_probe_data(\n                    nwb_probe_obj,\n                    new_probe_type_dict,\n                    new_probe_dict,\n                    shank_dict,\n                    elect_dict,\n                )\n\n            # check that number of shanks is consistent\n            num_shanks = new_probe_type_dict[\"num_shanks\"]\n            assert num_shanks == 0 or num_shanks == len(\n                shank_dict\n            ), \"`num_shanks` is not equal to the number of shanks.\"\n\n            # if probe id already exists, do not overwrite anything or create\n            # new Shanks and Electrodes\n            # TODO: test whether the Shanks and Electrodes in the NWB file match\n            # the ones in the database\n            query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n            if len(query) &gt; 0:\n                print(\n                    f\"Probe ID '{new_probe_dict['probe_id']}' already exists in\"\n                    \" the database. Spyglass will use that and not create a new\"\n                    \" Probe, Shanks, or Electrodes.\"\n                )\n                continue\n\n            cls.insert1(new_probe_dict, skip_duplicates=True)\n\n            for shank in shank_dict.values():\n                cls.Shank.insert1(shank, skip_duplicates=True)\n            for electrode in elect_dict.values():\n                cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n        if all_probes_types:\n            print(f\"Inserted probes {all_probes_types}\")\n        else:\n            print(\"No conforming probe metadata found.\")\n\n        return all_probes_types\n\n    @classmethod\n    def get_all_probe_names(cls, nwbf, config):\n        \"\"\"Get a list of all device names in the NWB.\n\n        Includes all devices, after appending/overwriting by the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to\n            replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n\n        # make a dict mapping probe type to PyNWB object for all devices in the\n        # NWB file that are of type ndx_franklab_novela.Probe and thus have the\n        # required metadata\n        ndx_probes = {\n            device_obj.probe_type: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.Probe)\n        }\n\n        # make a dict mapping probe type to dict of device metadata from the\n        # config YAML if exists\n        config_probes = (\n            [probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]]\n            if \"Probe\" in config\n            else list()\n        )\n\n        # get all the probe types from the NWB file plus the config YAML\n        all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n        return all_probes_types, ndx_probes, config_probes\n\n    @classmethod\n    def __read_ndx_probe_data(\n        cls,\n        nwb_probe_obj: ndx_franklab_novela.Probe,\n        new_probe_type_dict: dict,\n        new_probe_dict: dict,\n        shank_dict: dict,\n        elect_dict: dict,\n    ):\n        # construct dictionary of values to add to ProbeType\n        new_probe_type_dict.update(\n            {\n                \"manufacturer\": getattr(nwb_probe_obj, \"manufacturer\") or \"\",\n                \"probe_type\": nwb_probe_obj.probe_type,\n                \"probe_description\": nwb_probe_obj.probe_description,\n                \"num_shanks\": len(nwb_probe_obj.shanks),\n            }\n        )\n\n        cls._add_probe_type(new_probe_type_dict)\n\n        new_probe_dict.update(\n            {\n                \"probe_id\": nwb_probe_obj.probe_type,\n                \"probe_type\": nwb_probe_obj.probe_type,\n                \"contact_side_numbering\": \"True\"\n                if nwb_probe_obj.contact_side_numbering\n                else \"False\",\n            }\n        )\n        # go through the shanks and add each one to the Shank table\n        for shank in nwb_probe_obj.shanks.values():\n            shank_dict[shank.name] = {\n                \"probe_id\": new_probe_dict[\"probe_type\"],\n                \"probe_shank\": int(shank.name),\n            }\n\n            # go through the electrodes and add each one to the Electrode table\n            for electrode in shank.shanks_electrodes.values():\n                # the next line will need to be fixed if we have different sized\n                # contacts on a shank\n                elect_dict[electrode.name] = {\n                    \"probe_id\": new_probe_dict[\"probe_type\"],\n                    \"probe_shank\": shank_dict[shank.name][\"probe_shank\"],\n                    \"contact_size\": nwb_probe_obj.contact_size,\n                    \"probe_electrode\": int(electrode.name),\n                    \"rel_x\": electrode.rel_x,\n                    \"rel_y\": electrode.rel_y,\n                    \"rel_z\": electrode.rel_z,\n                }\n\n    @classmethod\n    def _add_probe_type(cls, new_probe_type_dict):\n        \"\"\"Check the probe type value against the values in the database.\n\n        Parameters\n        ----------\n        new_probe_type_dict : dict\n            Dictionary of probe type properties. See ProbeType for keys.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a probe type to the database when\n            prompted.\n\n        Returns\n        -------\n        probe_type : str\n            The probe type value that was added to the database.\n        \"\"\"\n        probe_type = new_probe_type_dict[\"probe_type\"]\n        all_values = ProbeType.fetch(\"probe_type\").tolist()\n        if probe_type not in all_values:\n            print(\n                f\"\\nProbe type '{probe_type}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the probe type you want to add does not \"\n                \"already exist in the database under a different name or \"\n                \"spelling. If you want to use an existing name in the \"\n                \"database, please change the corresponding Probe object in the \"\n                \"NWB file. Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add probe type '{probe_type}' to the database?\"\n                + \" (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                ProbeType.insert1(new_probe_type_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add probe type '{probe_type}' to the \"\n                + \"database.\"\n            )\n\n        # else / entry exists: check whether the values provided match the\n        # values stored in the database\n        db_dict = (ProbeType &amp; {\"probe_type\": probe_type}).fetch1()\n        if db_dict != new_probe_type_dict:\n            raise PopulateException(\n                \"\\nProbe type properties of PyNWB Probe object with name \"\n                f\"'{probe_type}': {new_probe_type_dict} do not match properties\"\n                f\" of the corresponding database entry: {db_dict}.\"\n            )\n        return probe_type\n\n    @classmethod\n    def create_from_nwbfile(\n        cls,\n        nwb_file_name: str,\n        nwb_device_name: str,\n        probe_id: str,\n        probe_type: str,\n        contact_side_numbering: bool,\n    ):\n        \"\"\"Create master/part Probe entry from the NWB file.\n\n        This method will parse the electrodes in the electrodes table, electrode\n        groups (as shanks), and devices (as probes) in the NWB file, but only\n        ones that are associated with the device that matches the given\n        `nwb_device_name`.\n\n        Note that this code assumes the relatively standard convention where the\n        NWB device corresponds to a Probe, the NWB electrode group corresponds\n        to a Shank, and the NWB electrode corresponds to an Electrode.\n\n        Example usage: ``` sgc.Probe.create_from_nwbfile(\n            nwbfile=nwb_file_name, nwb_device_name=\"Device\",\n            probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n            probe_type=\"Neuropixels 1.0\", contact_side_numbering=True\n        )\n        ```\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        nwb_device_name : str\n            The name of the PyNWB Device object that represents the probe to\n            read in the NWB file.\n        probe_id : str\n            A unique ID for the probe and its configuration, to be used as the\n            primary key for the new Probe entry.\n        probe_type : str\n            The existing ProbeType entry that represents the type of probe being\n            created. It must exist.\n        contact_side_numbering : bool\n            Whether the electrode contacts are facing you when numbering them.\n            Stored in the new Probe entry.\n        \"\"\"\n\n        from .common_nwbfile import Nwbfile\n\n        nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n        nwbfile = get_nwb_file(nwb_file_path)\n\n        query = ProbeType &amp; {\"probe_type\": probe_type}\n        if len(query) == 0:\n            print(\n                f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n            )\n            return\n\n        new_probe_dict = {\n            \"probe_id\": probe_id,\n            \"probe_type\": probe_type,\n            \"contact_side_numbering\": (\n                \"True\" if contact_side_numbering else \"False\"\n            ),\n        }\n        shank_dict = {}\n        elect_dict = {}\n\n        # iterate through the electrodes table in the NWB file\n        # and use the group column (ElectrodeGroup) to create shanks\n        # and use the device attribute of each ElectrodeGroup to create a probe\n        created_shanks = {}  # map device name to shank_index (int)\n        device_found = False\n        for elec_index in range(len(nwbfile.electrodes)):\n            electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n            eg_device_name = electrode_group.device.name\n\n            # only look at electrodes where the associated device is the one\n            # specified\n            if eg_device_name == nwb_device_name:\n                device_found = True\n\n                # if a Shank has not yet been created from the electrode group,\n                # then create it\n                if electrode_group.name not in created_shanks:\n                    shank_index = len(created_shanks)\n                    created_shanks[electrode_group.name] = shank_index\n\n                    # build the dictionary of Probe.Shank data\n                    shank_dict[shank_index] = {\n                        \"probe_id\": new_probe_dict[\"probe_id\"],\n                        \"probe_shank\": shank_index,\n                    }\n\n                # get the probe shank index associated with this Electrode\n                probe_shank = created_shanks[electrode_group.name]\n\n                # build the dictionary of Probe.Electrode data\n                elect_dict[elec_index] = {\n                    \"probe_id\": new_probe_dict[\"probe_id\"],\n                    \"probe_shank\": probe_shank,\n                    \"probe_electrode\": elec_index,\n                }\n                if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                        elec_index, \"rel_x\"\n                    ]\n                if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                        elec_index, \"rel_y\"\n                    ]\n                if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                        elec_index, \"rel_z\"\n                    ]\n\n        if not device_found:\n            print(\n                \"No electrodes in the NWB file were associated with a device \"\n                + f\"named '{nwb_device_name}'.\"\n            )\n            return\n\n        # insert the Probe, then the Shank parts, and then the Electrode parts\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert probe devices from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of probe device types found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n    \"\"\"Insert probe devices from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to\n        replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of probe device types found in the NWB file.\n    \"\"\"\n    all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n    for probe_type in all_probes_types:\n        new_probe_type_dict = dict()\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n        num_shanks = 0\n\n        if probe_type in ndx_probes:\n            # read probe properties into new_probe_dict from PyNWB extension\n            # probe object\n            nwb_probe_obj = ndx_probes[probe_type]\n            cls.__read_ndx_probe_data(\n                nwb_probe_obj,\n                new_probe_type_dict,\n                new_probe_dict,\n                shank_dict,\n                elect_dict,\n            )\n\n        # check that number of shanks is consistent\n        num_shanks = new_probe_type_dict[\"num_shanks\"]\n        assert num_shanks == 0 or num_shanks == len(\n            shank_dict\n        ), \"`num_shanks` is not equal to the number of shanks.\"\n\n        # if probe id already exists, do not overwrite anything or create\n        # new Shanks and Electrodes\n        # TODO: test whether the Shanks and Electrodes in the NWB file match\n        # the ones in the database\n        query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n        if len(query) &gt; 0:\n            print(\n                f\"Probe ID '{new_probe_dict['probe_id']}' already exists in\"\n                \" the database. Spyglass will use that and not create a new\"\n                \" Probe, Shanks, or Electrodes.\"\n            )\n            continue\n\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n    if all_probes_types:\n        print(f\"Inserted probes {all_probes_types}\")\n    else:\n        print(\"No conforming probe metadata found.\")\n\n    return all_probes_types\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe.get_all_probe_names", "title": "<code>get_all_probe_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Get a list of all device names in the NWB.</p> <p>Includes all devices, after appending/overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_probe_names(cls, nwbf, config):\n    \"\"\"Get a list of all device names in the NWB.\n\n    Includes all devices, after appending/overwriting by the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to\n        replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n\n    # make a dict mapping probe type to PyNWB object for all devices in the\n    # NWB file that are of type ndx_franklab_novela.Probe and thus have the\n    # required metadata\n    ndx_probes = {\n        device_obj.probe_type: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.Probe)\n    }\n\n    # make a dict mapping probe type to dict of device metadata from the\n    # config YAML if exists\n    config_probes = (\n        [probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]]\n        if \"Probe\" in config\n        else list()\n    )\n\n    # get all the probe types from the NWB file plus the config YAML\n    all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n    return all_probes_types, ndx_probes, config_probes\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe.create_from_nwbfile", "title": "<code>create_from_nwbfile(nwb_file_name, nwb_device_name, probe_id, probe_type, contact_side_numbering)</code>  <code>classmethod</code>", "text": "<p>Create master/part Probe entry from the NWB file.</p> <p>This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices (as probes) in the NWB file, but only ones that are associated with the device that matches the given <code>nwb_device_name</code>.</p> <p>Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe, the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.</p> <p>Example usage: <code>sgc.Probe.create_from_nwbfile(     nwbfile=nwb_file_name, nwb_device_name=\"Device\",     probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",     probe_type=\"Neuropixels 1.0\", contact_side_numbering=True )</code></p>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe.create_from_nwbfile--parameters", "title": "Parameters", "text": "<p>nwb_file_name : str     The name of the NWB file. nwb_device_name : str     The name of the PyNWB Device object that represents the probe to     read in the NWB file. probe_id : str     A unique ID for the probe and its configuration, to be used as the     primary key for the new Probe entry. probe_type : str     The existing ProbeType entry that represents the type of probe being     created. It must exist. contact_side_numbering : bool     Whether the electrode contacts are facing you when numbering them.     Stored in the new Probe entry.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef create_from_nwbfile(\n    cls,\n    nwb_file_name: str,\n    nwb_device_name: str,\n    probe_id: str,\n    probe_type: str,\n    contact_side_numbering: bool,\n):\n    \"\"\"Create master/part Probe entry from the NWB file.\n\n    This method will parse the electrodes in the electrodes table, electrode\n    groups (as shanks), and devices (as probes) in the NWB file, but only\n    ones that are associated with the device that matches the given\n    `nwb_device_name`.\n\n    Note that this code assumes the relatively standard convention where the\n    NWB device corresponds to a Probe, the NWB electrode group corresponds\n    to a Shank, and the NWB electrode corresponds to an Electrode.\n\n    Example usage: ``` sgc.Probe.create_from_nwbfile(\n        nwbfile=nwb_file_name, nwb_device_name=\"Device\",\n        probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n        probe_type=\"Neuropixels 1.0\", contact_side_numbering=True\n    )\n    ```\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    nwb_device_name : str\n        The name of the PyNWB Device object that represents the probe to\n        read in the NWB file.\n    probe_id : str\n        A unique ID for the probe and its configuration, to be used as the\n        primary key for the new Probe entry.\n    probe_type : str\n        The existing ProbeType entry that represents the type of probe being\n        created. It must exist.\n    contact_side_numbering : bool\n        Whether the electrode contacts are facing you when numbering them.\n        Stored in the new Probe entry.\n    \"\"\"\n\n    from .common_nwbfile import Nwbfile\n\n    nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n    nwbfile = get_nwb_file(nwb_file_path)\n\n    query = ProbeType &amp; {\"probe_type\": probe_type}\n    if len(query) == 0:\n        print(\n            f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n        )\n        return\n\n    new_probe_dict = {\n        \"probe_id\": probe_id,\n        \"probe_type\": probe_type,\n        \"contact_side_numbering\": (\n            \"True\" if contact_side_numbering else \"False\"\n        ),\n    }\n    shank_dict = {}\n    elect_dict = {}\n\n    # iterate through the electrodes table in the NWB file\n    # and use the group column (ElectrodeGroup) to create shanks\n    # and use the device attribute of each ElectrodeGroup to create a probe\n    created_shanks = {}  # map device name to shank_index (int)\n    device_found = False\n    for elec_index in range(len(nwbfile.electrodes)):\n        electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n        eg_device_name = electrode_group.device.name\n\n        # only look at electrodes where the associated device is the one\n        # specified\n        if eg_device_name == nwb_device_name:\n            device_found = True\n\n            # if a Shank has not yet been created from the electrode group,\n            # then create it\n            if electrode_group.name not in created_shanks:\n                shank_index = len(created_shanks)\n                created_shanks[electrode_group.name] = shank_index\n\n                # build the dictionary of Probe.Shank data\n                shank_dict[shank_index] = {\n                    \"probe_id\": new_probe_dict[\"probe_id\"],\n                    \"probe_shank\": shank_index,\n                }\n\n            # get the probe shank index associated with this Electrode\n            probe_shank = created_shanks[electrode_group.name]\n\n            # build the dictionary of Probe.Electrode data\n            elect_dict[elec_index] = {\n                \"probe_id\": new_probe_dict[\"probe_id\"],\n                \"probe_shank\": probe_shank,\n                \"probe_electrode\": elec_index,\n            }\n            if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                    elec_index, \"rel_x\"\n                ]\n            if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                    elec_index, \"rel_y\"\n                ]\n            if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                    elec_index, \"rel_z\"\n                ]\n\n    if not device_found:\n        print(\n            \"No electrodes in the NWB file were associated with a device \"\n            + f\"named '{nwb_device_name}'.\"\n        )\n        return\n\n    # insert the Probe, then the Shank parts, and then the Electrode parts\n    cls.insert1(new_probe_dict, skip_duplicates=True)\n\n    for shank in shank_dict.values():\n        cls.Shank.insert1(shank, skip_duplicates=True)\n    for electrode in elect_dict.values():\n        cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/", "title": "common_dio.py", "text": ""}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.DIOEvents", "title": "<code>DIOEvents</code>", "text": "<p>             Bases: <code>Imported</code></p> Source code in <code>src/spyglass/common/common_dio.py</code> <pre><code>@schema\nclass DIOEvents(dj.Imported):\n    definition = \"\"\"\n    -&gt; Session\n    dio_event_name: varchar(80)   # the name assigned to this DIO event\n    ---\n    dio_object_id: varchar(40)    # the object id of the data in the NWB file\n    -&gt; IntervalList               # the list of intervals for this object\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        behav_events = get_data_interface(\n            nwbf, \"behavioral_events\", pynwb.behavior.BehavioralEvents\n        )\n        if behav_events is None:\n            print(\n                \"No conforming behavioral events data interface found in \"\n                + f\"{nwb_file_name}\\n\"\n            )\n            return\n\n        # Times for these events correspond to the valid times for the raw data\n        key[\"interval_list_name\"] = (\n            Raw() &amp; {\"nwb_file_name\": nwb_file_name}\n        ).fetch1(\"interval_list_name\")\n        for event_series in behav_events.time_series.values():\n            key[\"dio_event_name\"] = event_series.name\n            key[\"dio_object_id\"] = event_series.object_id\n            self.insert1(key, skip_duplicates=True)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def plot_all_dio_events(self):\n        \"\"\"Plot all DIO events in the session.\n\n        Examples\n        --------\n        &gt; restr1 = {'nwb_file_name': 'arthur20220314_.nwb'}\n        &gt; restr2 = {'nwb_file_name': 'arthur20220316_.nwb'}\n        &gt; (DIOEvents &amp; restr1).plot_all_dio_events()\n        &gt; (DIOEvents &amp; [restr1, restr2]).plot_all_dio_events()\n\n        \"\"\"\n        behavioral_events = self.fetch_nwb()\n        nwb_file_names = np.unique(\n            [event[\"nwb_file_name\"] for event in behavioral_events]\n        )\n        epoch_valid_times = (\n            pd.DataFrame(\n                IntervalList()\n                &amp; [\n                    {\"nwb_file_name\": nwb_file_name}\n                    for nwb_file_name in nwb_file_names\n                ]\n            )\n            .set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n\n        n_events = len(behavioral_events)\n\n        _, axes = plt.subplots(\n            n_events,\n            1,\n            figsize=(15, n_events * 0.3),\n            dpi=100,\n            sharex=True,\n            constrained_layout=True,\n        )\n\n        for ind, (ax, event) in enumerate(zip(axes.flat, behavioral_events)):\n            for epoch_name, epoch in epoch_valid_times.items():\n                start_time, stop_time = epoch.squeeze()\n                ax.axvspan(start_time, stop_time, alpha=0.5)\n                if ind == 0:\n                    ax.text(\n                        start_time + (stop_time - start_time) / 2,\n                        1.001,\n                        epoch_name,\n                        ha=\"center\",\n                        va=\"bottom\",\n                    )\n            ax.step(\n                np.asarray(event[\"dio\"].timestamps),\n                np.asarray(event[\"dio\"].data),\n                where=\"post\",\n                color=\"black\",\n            )\n            ax.set_ylabel(\n                event[\"dio_event_name\"], rotation=0, ha=\"right\", va=\"center\"\n            )\n            ax.set_yticks([])\n        ax.set_xlabel(\"Time\")\n\n        if len(nwb_file_names) == 1:\n            plt.suptitle(f\"DIO events in {nwb_file_names[0]}\")\n        else:\n            plt.suptitle(f\"DIO events in {', '.join(nwb_file_names)}\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.DIOEvents.plot_all_dio_events", "title": "<code>plot_all_dio_events()</code>", "text": "<p>Plot all DIO events in the session.</p> <p>Examples:</p> <p>restr1 = {'nwb_file_name': 'arthur20220314_.nwb'} restr2 = {'nwb_file_name': 'arthur20220316_.nwb'} (DIOEvents &amp; restr1).plot_all_dio_events() (DIOEvents &amp; [restr1, restr2]).plot_all_dio_events()</p> Source code in <code>src/spyglass/common/common_dio.py</code> <pre><code>def plot_all_dio_events(self):\n    \"\"\"Plot all DIO events in the session.\n\n    Examples\n    --------\n    &gt; restr1 = {'nwb_file_name': 'arthur20220314_.nwb'}\n    &gt; restr2 = {'nwb_file_name': 'arthur20220316_.nwb'}\n    &gt; (DIOEvents &amp; restr1).plot_all_dio_events()\n    &gt; (DIOEvents &amp; [restr1, restr2]).plot_all_dio_events()\n\n    \"\"\"\n    behavioral_events = self.fetch_nwb()\n    nwb_file_names = np.unique(\n        [event[\"nwb_file_name\"] for event in behavioral_events]\n    )\n    epoch_valid_times = (\n        pd.DataFrame(\n            IntervalList()\n            &amp; [\n                {\"nwb_file_name\": nwb_file_name}\n                for nwb_file_name in nwb_file_names\n            ]\n        )\n        .set_index(\"interval_list_name\")\n        .filter(regex=r\"^[0-9]\", axis=0)\n        .valid_times\n    )\n\n    n_events = len(behavioral_events)\n\n    _, axes = plt.subplots(\n        n_events,\n        1,\n        figsize=(15, n_events * 0.3),\n        dpi=100,\n        sharex=True,\n        constrained_layout=True,\n    )\n\n    for ind, (ax, event) in enumerate(zip(axes.flat, behavioral_events)):\n        for epoch_name, epoch in epoch_valid_times.items():\n            start_time, stop_time = epoch.squeeze()\n            ax.axvspan(start_time, stop_time, alpha=0.5)\n            if ind == 0:\n                ax.text(\n                    start_time + (stop_time - start_time) / 2,\n                    1.001,\n                    epoch_name,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n        ax.step(\n            np.asarray(event[\"dio\"].timestamps),\n            np.asarray(event[\"dio\"].data),\n            where=\"post\",\n            color=\"black\",\n        )\n        ax.set_ylabel(\n            event[\"dio_event_name\"], rotation=0, ha=\"right\", va=\"center\"\n        )\n        ax.set_yticks([])\n    ax.set_xlabel(\"Time\")\n\n    if len(nwb_file_names) == 1:\n        plt.suptitle(f\"DIO events in {nwb_file_names[0]}\")\n    else:\n        plt.suptitle(f\"DIO events in {', '.join(nwb_file_names)}\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/", "title": "common_ephys.py", "text": ""}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.Electrode", "title": "<code>Electrode</code>", "text": "<p>             Bases: <code>Imported</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass Electrode(dj.Imported):\n    definition = \"\"\"\n    -&gt; ElectrodeGroup\n    electrode_id: int                      # the unique number for this electrode\n    ---\n    -&gt; [nullable] Probe.Electrode\n    -&gt; BrainRegion\n    name = \"\": varchar(200)                 # unique label for each contact\n    original_reference_electrode = -1: int  # the configured reference electrode for this electrode\n    x = NULL: float                         # the x coordinate of the electrode position in the brain\n    y = NULL: float                         # the y coordinate of the electrode position in the brain\n    z = NULL: float                         # the z coordinate of the electrode position in the brain\n    filtering: varchar(2000)                # description of the signal filtering\n    impedance = NULL: float                 # electrode impedance\n    bad_channel = \"False\": enum(\"True\", \"False\")  # if electrode is \"good\" or \"bad\" as observed during recording\n    x_warped = NULL: float                  # x coordinate of electrode position warped to common template brain\n    y_warped = NULL: float                  # y coordinate of electrode position warped to common template brain\n    z_warped = NULL: float                  # z coordinate of electrode position warped to common template brain\n    contacts: varchar(200)                  # label of electrode contacts used for a bipolar signal - current workaround\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n\n        if \"Electrode\" in config:\n            electrode_config_dicts = {\n                electrode_dict[\"electrode_id\"]: electrode_dict\n                for electrode_dict in config[\"Electrode\"]\n            }\n        else:\n            electrode_config_dicts = dict()\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for elect_id, elect_data in electrodes.iterrows():\n            key[\"electrode_id\"] = elect_id\n            key[\"name\"] = str(elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n\n            # rough check of whether the electrodes table was created by rec_to_nwb and has\n            # the appropriate custom columns used by rec_to_nwb\n            # TODO this could be better resolved by making an extension for the electrodes table\n            if (\n                isinstance(elect_data.group.device, ndx_franklab_novela.Probe)\n                and \"probe_shank\" in elect_data\n                and \"probe_electrode\" in elect_data\n                and \"bad_channel\" in elect_data\n                and \"ref_elect_id\" in elect_data\n            ):\n                key[\"probe_id\"] = elect_data.group.device.probe_type\n                key[\"probe_shank\"] = elect_data.probe_shank\n                key[\"probe_electrode\"] = elect_data.probe_electrode\n                key[\"bad_channel\"] = (\n                    \"True\" if elect_data.bad_channel else \"False\"\n                )\n                key[\"original_reference_electrode\"] = elect_data.ref_elect_id\n\n            # override with information from the config YAML based on primary key (electrode id)\n            if elect_id in electrode_config_dicts:\n                # check whether the Probe.Electrode being referenced exists\n                query = Probe.Electrode &amp; electrode_config_dicts[elect_id]\n                if len(query) == 0:\n                    warnings.warn(\n                        f\"No Probe.Electrode exists that matches the data: {electrode_config_dicts[elect_id]}. \"\n                        f\"The config YAML for Electrode with electrode_id {elect_id} will be ignored.\"\n                    )\n                else:\n                    key.update(electrode_config_dicts[elect_id])\n\n            self.insert1(key, skip_duplicates=True)\n\n    @classmethod\n    def create_from_config(cls, nwb_file_name: str):\n        \"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n        if \"Electrode\" not in config:\n            return\n\n        # map electrode id to dictionary of electrode information from config YAML\n        electrode_dicts = {\n            electrode_dict[\"electrode_id\"]: electrode_dict\n            for electrode_dict in config[\"Electrode\"]\n        }\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for nwbfile_elect_id, elect_data in electrodes.iterrows():\n            if nwbfile_elect_id in electrode_dicts:\n                # use the information in the electrodes table to start and then add (or overwrite) values from the\n                # config YAML\n                key = dict()\n                key[\"nwb_file_name\"] = nwb_file_name\n                key[\"name\"] = str(nwbfile_elect_id)\n                key[\"electrode_group_name\"] = elect_data.group_name\n                key[\"region_id\"] = BrainRegion.fetch_add(\n                    region_name=elect_data.group.location\n                )\n                key[\"x\"] = elect_data.x\n                key[\"y\"] = elect_data.y\n                key[\"z\"] = elect_data.z\n                key[\"x_warped\"] = 0\n                key[\"y_warped\"] = 0\n                key[\"z_warped\"] = 0\n                key[\"contacts\"] = \"\"\n                key[\"filtering\"] = elect_data.filtering\n                key[\"impedance\"] = elect_data.get(\"imp\")\n                key.update(electrode_dicts[nwbfile_elect_id])\n                query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n                if len(query):\n                    cls.update1(key)\n                    print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n                else:\n                    cls.insert1(\n                        key, skip_duplicates=True, allow_direct_insert=True\n                    )\n                    print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                warnings.warn(\n                    f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                    \"config YAML entry.\"\n                )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.Electrode.create_from_config", "title": "<code>create_from_config(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Create or update Electrode entries from what is specified in the config YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@classmethod\ndef create_from_config(cls, nwb_file_name: str):\n    \"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n    config = get_config(nwb_file_abspath)\n    if \"Electrode\" not in config:\n        return\n\n    # map electrode id to dictionary of electrode information from config YAML\n    electrode_dicts = {\n        electrode_dict[\"electrode_id\"]: electrode_dict\n        for electrode_dict in config[\"Electrode\"]\n    }\n\n    electrodes = nwbf.electrodes.to_dataframe()\n    for nwbfile_elect_id, elect_data in electrodes.iterrows():\n        if nwbfile_elect_id in electrode_dicts:\n            # use the information in the electrodes table to start and then add (or overwrite) values from the\n            # config YAML\n            key = dict()\n            key[\"nwb_file_name\"] = nwb_file_name\n            key[\"name\"] = str(nwbfile_elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n            key.update(electrode_dicts[nwbfile_elect_id])\n            query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n            if len(query):\n                cls.update1(key)\n                print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                cls.insert1(\n                    key, skip_duplicates=True, allow_direct_insert=True\n                )\n                print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n        else:\n            warnings.warn(\n                f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                \"config YAML entry.\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPSelection", "title": "<code>LFPSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass LFPSelection(dj.Manual):\n    definition = \"\"\"\n     -&gt; Session\n     \"\"\"\n\n    class LFPElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPSelection\n        -&gt; Electrode\n        \"\"\"\n\n    def set_lfp_electrodes(self, nwb_file_name, electrode_list):\n        \"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the nwb file for the desired session\n        electrode_list : list\n            list of electrodes to be used for LFP\n\n        \"\"\"\n        # remove the session and then recreate the session and Electrode list\n        (LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n        # check to see if the user allowed the deletion\n        if (\n            len((LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).fetch())\n            == 0\n        ):\n            LFPSelection().insert1({\"nwb_file_name\": nwb_file_name})\n\n            # TODO: do this in a better way\n            all_electrodes = (\n                Electrode() &amp; {\"nwb_file_name\": nwb_file_name}\n            ).fetch(as_dict=True)\n            primary_key = Electrode.primary_key\n            for e in all_electrodes:\n                # create a dictionary so we can insert new elects\n                if e[\"electrode_id\"] in electrode_list:\n                    lfpelectdict = {\n                        k: v for k, v in e.items() if k in primary_key\n                    }\n                    LFPSelection().LFPElectrode.insert1(\n                        lfpelectdict, replace=True\n                    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPSelection.set_lfp_electrodes", "title": "<code>set_lfp_electrodes(nwb_file_name, electrode_list)</code>", "text": "<p>Removes all electrodes for the specified nwb file and then adds back the electrodes in the list</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the nwb file for the desired session</p> required <code>electrode_list</code> <code>list</code> <p>list of electrodes to be used for LFP</p> required Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>def set_lfp_electrodes(self, nwb_file_name, electrode_list):\n    \"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the nwb file for the desired session\n    electrode_list : list\n        list of electrodes to be used for LFP\n\n    \"\"\"\n    # remove the session and then recreate the session and Electrode list\n    (LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n    # check to see if the user allowed the deletion\n    if (\n        len((LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).fetch())\n        == 0\n    ):\n        LFPSelection().insert1({\"nwb_file_name\": nwb_file_name})\n\n        # TODO: do this in a better way\n        all_electrodes = (\n            Electrode() &amp; {\"nwb_file_name\": nwb_file_name}\n        ).fetch(as_dict=True)\n        primary_key = Electrode.primary_key\n        for e in all_electrodes:\n            # create a dictionary so we can insert new elects\n            if e[\"electrode_id\"] in electrode_list:\n                lfpelectdict = {\n                    k: v for k, v in e.items() if k in primary_key\n                }\n                LFPSelection().LFPElectrode.insert1(\n                    lfpelectdict, replace=True\n                )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPBandSelection", "title": "<code>LFPBandSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass LFPBandSelection(dj.Manual):\n    definition = \"\"\"\n    -&gt; LFP\n    -&gt; FirFilterParameters                   # the filter to use for the data\n    -&gt; IntervalList.proj(target_interval_list_name='interval_list_name')  # the original set of times to be filtered\n    lfp_band_sampling_rate: int    # the sampling rate for this band\n    ---\n    min_interval_len = 1: float  # the minimum length of a valid interval to filter\n    \"\"\"\n\n    class LFPBandElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPBandSelection\n        -&gt; LFPSelection.LFPElectrode  # the LFP electrode to be filtered\n        reference_elect_id = -1: int  # the reference electrode to use; -1 for no reference\n        ---\n        \"\"\"\n\n    def set_lfp_band_electrodes(\n        self,\n        nwb_file_name,\n        electrode_list,\n        filter_name,\n        interval_list_name,\n        reference_electrode_list,\n        lfp_band_sampling_rate,\n    ):\n        \"\"\"\n        Adds an entry for each electrode in the electrode_list with the specified filter, interval_list, and\n        reference electrode.\n        Also removes any entries that have the same filter, interval list and reference electrode but are not\n        in the electrode_list.\n        :param nwb_file_name: string - the name of the nwb file for the desired session\n        :param electrode_list: list of LFP electrodes to be filtered\n        :param filter_name: the name of the filter (from the FirFilterParameters schema)\n        :param interval_name: the name of the interval list (from the IntervalList schema)\n        :param reference_electrode_list: A single electrode id corresponding to the reference to use for all\n        electrodes or a list with one element per entry in the electrode_list\n        :param lfp_band_sampling_rate: The output sampling rate to be used for the filtered data; must be an\n        integer divisor of the LFP sampling rate\n        :return: none\n        \"\"\"\n        # Error checks on parameters\n        # electrode_list\n        query = LFPSelection().LFPElectrode() &amp; {\"nwb_file_name\": nwb_file_name}\n        available_electrodes = query.fetch(\"electrode_id\")\n        if not np.all(np.isin(electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in electrode_list must be valid electrode_ids in the LFPSelection table\"\n            )\n        # sampling rate\n        lfp_sampling_rate = (LFP() &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\n            \"lfp_sampling_rate\"\n        )\n        decimation = lfp_sampling_rate // lfp_band_sampling_rate\n        if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n            raise ValueError(\n                f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n                f\"samping rate {lfp_sampling_rate}\"\n            )\n        # filter\n        query = FirFilterParameters() &amp; {\n            \"filter_name\": filter_name,\n            \"filter_sampling_rate\": lfp_sampling_rate,\n        }\n        if not query:\n            raise ValueError(\n                f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n            )\n        # interval_list\n        query = IntervalList() &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_name\": interval_list_name,\n        }\n        if not query:\n            raise ValueError(\n                f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n                \"added before this function is called\"\n            )\n        # reference_electrode_list\n        if len(reference_electrode_list) != 1 and len(\n            reference_electrode_list\n        ) != len(electrode_list):\n            raise ValueError(\n                \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n            )\n        # add a -1 element to the list to allow for the no reference option\n        available_electrodes = np.append(available_electrodes, [-1])\n        if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n                \"table\"\n            )\n\n        # make a list of all the references\n        ref_list = np.zeros((len(electrode_list),))\n        ref_list[:] = reference_electrode_list\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"filter_name\"] = filter_name\n        key[\"filter_sampling_rate\"] = lfp_sampling_rate\n        key[\"target_interval_list_name\"] = interval_list_name\n        key[\"lfp_band_sampling_rate\"] = lfp_sampling_rate // decimation\n        # insert an entry into the main LFPBandSelectionTable\n        self.insert1(key, skip_duplicates=True)\n\n        # get all of the current entries and delete any that are not in the list\n        elect_id, ref_id = (self.LFPBandElectrode() &amp; key).fetch(\n            \"electrode_id\", \"reference_elect_id\"\n        )\n        for e, r in zip(elect_id, ref_id):\n            if not len(np.where((electrode_list == e) &amp; (ref_list == r))[0]):\n                key[\"electrode_id\"] = e\n                key[\"reference_elect_id\"] = r\n                (self.LFPBandElectrode() &amp; key).delete()\n\n        # iterate through all of the new elements and add them\n        for e, r in zip(electrode_list, ref_list):\n            key[\"electrode_id\"] = e\n            query = Electrode &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"electrode_id\": e,\n            }\n            key[\"electrode_group_name\"] = query.fetch1(\"electrode_group_name\")\n            key[\"reference_elect_id\"] = r\n            self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPBandSelection.set_lfp_band_electrodes", "title": "<code>set_lfp_band_electrodes(nwb_file_name, electrode_list, filter_name, interval_list_name, reference_electrode_list, lfp_band_sampling_rate)</code>", "text": "<p>Adds an entry for each electrode in the electrode_list with the specified filter, interval_list, and reference electrode. Also removes any entries that have the same filter, interval list and reference electrode but are not in the electrode_list. :param nwb_file_name: string - the name of the nwb file for the desired session :param electrode_list: list of LFP electrodes to be filtered :param filter_name: the name of the filter (from the FirFilterParameters schema) :param interval_name: the name of the interval list (from the IntervalList schema) :param reference_electrode_list: A single electrode id corresponding to the reference to use for all electrodes or a list with one element per entry in the electrode_list :param lfp_band_sampling_rate: The output sampling rate to be used for the filtered data; must be an integer divisor of the LFP sampling rate :return: none</p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>def set_lfp_band_electrodes(\n    self,\n    nwb_file_name,\n    electrode_list,\n    filter_name,\n    interval_list_name,\n    reference_electrode_list,\n    lfp_band_sampling_rate,\n):\n    \"\"\"\n    Adds an entry for each electrode in the electrode_list with the specified filter, interval_list, and\n    reference electrode.\n    Also removes any entries that have the same filter, interval list and reference electrode but are not\n    in the electrode_list.\n    :param nwb_file_name: string - the name of the nwb file for the desired session\n    :param electrode_list: list of LFP electrodes to be filtered\n    :param filter_name: the name of the filter (from the FirFilterParameters schema)\n    :param interval_name: the name of the interval list (from the IntervalList schema)\n    :param reference_electrode_list: A single electrode id corresponding to the reference to use for all\n    electrodes or a list with one element per entry in the electrode_list\n    :param lfp_band_sampling_rate: The output sampling rate to be used for the filtered data; must be an\n    integer divisor of the LFP sampling rate\n    :return: none\n    \"\"\"\n    # Error checks on parameters\n    # electrode_list\n    query = LFPSelection().LFPElectrode() &amp; {\"nwb_file_name\": nwb_file_name}\n    available_electrodes = query.fetch(\"electrode_id\")\n    if not np.all(np.isin(electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in electrode_list must be valid electrode_ids in the LFPSelection table\"\n        )\n    # sampling rate\n    lfp_sampling_rate = (LFP() &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\n        \"lfp_sampling_rate\"\n    )\n    decimation = lfp_sampling_rate // lfp_band_sampling_rate\n    if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n        raise ValueError(\n            f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n            f\"samping rate {lfp_sampling_rate}\"\n        )\n    # filter\n    query = FirFilterParameters() &amp; {\n        \"filter_name\": filter_name,\n        \"filter_sampling_rate\": lfp_sampling_rate,\n    }\n    if not query:\n        raise ValueError(\n            f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n        )\n    # interval_list\n    query = IntervalList() &amp; {\n        \"nwb_file_name\": nwb_file_name,\n        \"interval_name\": interval_list_name,\n    }\n    if not query:\n        raise ValueError(\n            f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n            \"added before this function is called\"\n        )\n    # reference_electrode_list\n    if len(reference_electrode_list) != 1 and len(\n        reference_electrode_list\n    ) != len(electrode_list):\n        raise ValueError(\n            \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n        )\n    # add a -1 element to the list to allow for the no reference option\n    available_electrodes = np.append(available_electrodes, [-1])\n    if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n            \"table\"\n        )\n\n    # make a list of all the references\n    ref_list = np.zeros((len(electrode_list),))\n    ref_list[:] = reference_electrode_list\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"filter_name\"] = filter_name\n    key[\"filter_sampling_rate\"] = lfp_sampling_rate\n    key[\"target_interval_list_name\"] = interval_list_name\n    key[\"lfp_band_sampling_rate\"] = lfp_sampling_rate // decimation\n    # insert an entry into the main LFPBandSelectionTable\n    self.insert1(key, skip_duplicates=True)\n\n    # get all of the current entries and delete any that are not in the list\n    elect_id, ref_id = (self.LFPBandElectrode() &amp; key).fetch(\n        \"electrode_id\", \"reference_elect_id\"\n    )\n    for e, r in zip(elect_id, ref_id):\n        if not len(np.where((electrode_list == e) &amp; (ref_list == r))[0]):\n            key[\"electrode_id\"] = e\n            key[\"reference_elect_id\"] = r\n            (self.LFPBandElectrode() &amp; key).delete()\n\n    # iterate through all of the new elements and add them\n    for e, r in zip(electrode_list, ref_list):\n        key[\"electrode_id\"] = e\n        query = Electrode &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"electrode_id\": e,\n        }\n        key[\"electrode_group_name\"] = query.fetch1(\"electrode_group_name\")\n        key[\"reference_elect_id\"] = r\n        self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/", "title": "common_filter.py", "text": ""}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters", "title": "<code>FirFilterParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>@schema\nclass FirFilterParameters(dj.Manual):\n    definition = \"\"\"\n    filter_name: varchar(80)           # descriptive name of this filter\n    filter_sampling_rate: int          # sampling rate for this filter\n    ---\n    filter_type: enum(\"lowpass\", \"highpass\", \"bandpass\")\n    filter_low_stop = 0: float     # lowest freq for stop band for low filt\n    filter_low_pass = 0: float     # lowest freq for pass band of low filt\n    filter_high_pass = 0: float    # highest freq for pass band for high filt\n    filter_high_stop = 0: float    # highest freq for stop band of high filt\n    filter_comments: varchar(2000) # comments about the filter\n    filter_band_edges: blob        # numpy array of filter bands\n                                   # redundant with individual parameters\n    filter_coeff: longblob         # numpy array of filter coefficients\n    \"\"\"\n\n    def add_filter(\n        self,\n        filter_name: str,\n        fs: float,\n        filter_type: str,\n        band_edges: list,\n        comments: str = \"\",\n    ) -&gt; None:\n        \"\"\"Add filter to the Filter table.\n\n        Parameters\n        ----------\n        filter_name: str\n            The name of the filter.\n        fs: float\n            The filter sampling rate.\n        filter_type: str\n            The type of the filter ('lowpass', 'highpass', or 'bandpass').\n        band_edges: List[float]\n            The band edges for the filter.\n        comments: str, optional)\n            Additional comments for the filter. Default \"\".\n\n        Returns\n        -------\n        None\n            Returns None if there is an error in the filter type or band\n            frequencies.\n\n        Raises\n        ------\n        Exception:\n            Raises an exception if an unexpected filter type is encountered.\n        \"\"\"\n        VALID_FILTERS = {\"lowpass\": 2, \"highpass\": 2, \"bandpass\": 4}\n        FILTER_ERR = \"Error in Filter.add_filter: \"\n        FILTER_N_ERR = FILTER_ERR + \"filter {} requires {} band_frequencies.\"\n\n        # add an FIR bandpass filter of the specified type.\n        # band_edges should be as follows:\n        #   low pass : [high_pass high_stop]\n        #   high pass: [low stop low pass]\n        #   band pass: [low_stop low_pass high_pass high_stop].\n        if filter_type not in VALID_FILTERS:\n            print(\n                FILTER_ERR\n                + f\"{filter_type} not valid type: {VALID_FILTERS.keys()}\"\n            )\n            return None\n\n        if not len(band_edges) == VALID_FILTERS[filter_type]:\n            print(FILTER_N_ERR.format(filter_name, VALID_FILTERS[filter_type]))\n            return None\n\n        gsp = _import_ghostipy()\n        TRANS_SPLINE = 2  # transition spline will be quadratic\n\n        if filter_type != \"bandpass\":\n            transition_width = band_edges[1] - band_edges[0]\n\n        else:\n            # transition width is mean of left and right transition regions\n            transition_width = (\n                (band_edges[1] - band_edges[0])\n                + (band_edges[3] - band_edges[2])\n            ) / 2.0\n\n        numtaps = gsp.estimate_taps(fs, transition_width)\n        filterdict = {\n            \"filter_type\": filter_type,\n            \"filter_name\": filter_name,\n            \"filter_sampling_rate\": fs,\n            \"filter_comments\": comments,\n            \"filter_low_stop\": 0,\n            \"filter_low_pass\": 0,\n            \"filter_high_pass\": 0,\n            \"filter_high_stop\": 0,\n            \"filter_band_edges\": np.asarray(band_edges),\n        }\n\n        # set the desired frequency response\n        if filter_type == \"lowpass\":\n            desired = [1, 0]\n            pass_stop_dict = {\n                \"filter_high_pass\": band_edges[0],\n                \"filter_high_stop\": band_edges[1],\n            }\n        elif filter_type == \"highpass\":\n            desired = [0, 1]\n            pass_stop_dict = {\n                \"filter_low_stop\": band_edges[0],\n                \"filter_low_pass\": band_edges[1],\n            }\n        else:\n            desired = [0, 1, 1, 0]\n            pass_stop_dict = {\n                \"filter_low_stop\": band_edges[0],\n                \"filter_low_pass\": band_edges[1],\n                \"filter_high_pass\": band_edges[2],\n                \"filter_high_stop\": band_edges[3],\n            }\n\n        # create 1d array for coefficients\n        filterdict.update(\n            {\n                **pass_stop_dict,\n                \"filter_coeff\": np.array(\n                    gsp.firdesign(\n                        numtaps, band_edges, desired, fs=fs, p=TRANS_SPLINE\n                    ),\n                    ndmin=1,\n                ),\n            }\n        )\n\n        self.insert1(filterdict, skip_duplicates=True)\n\n    def _filter_restrict(self, filter_name, fs):\n        return (\n            self &amp; {\"filter_name\": filter_name} &amp; {\"filter_sampling_rate\": fs}\n        ).fetch1(as_dict=True)\n\n    def plot_magnitude(self, filter_name, fs):\n        filter_dict = self._filter_restrict(filter_name, fs)\n        plt.figure()\n        w, h = signal.freqz(filter_dict[\"filter_coeff\"], worN=65536)\n        magnitude = 20 * np.log10(np.abs(h))\n        plt.plot(w / np.pi * fs / 2, magnitude)\n        plt.xlabel(\"Frequency (Hz)\")\n        plt.ylabel(\"Magnitude\")\n        plt.title(\"Frequency Response\")\n        plt.xlim(0, np.max(filter_dict[\"filter_coeffand_edges\"] * 2))\n        plt.ylim(np.min(magnitude), -1 * np.min(magnitude) * 0.1)\n        plt.grid(True)\n\n    def plot_fir_filter(self, filter_name, fs):\n        filter_dict = self._filter_restrict(filter_name, fs)\n        plt.figure()\n        plt.clf()\n        plt.plot(filter_dict[\"filter_coeff\"], \"k\")\n        plt.xlabel(\"Coefficient\")\n        plt.ylabel(\"Magnitude\")\n        plt.title(\"Filter Taps\")\n        plt.grid(True)\n\n    def filter_delay(self, filter_name, fs):\n        return self.calc_filter_delay(\n            self._filter_restrict(filter_name, fs)[\"filter_coeff\"]\n        )\n\n    def _time_bound_check(self, start, stop, all, nsamples):\n        timestamp_warn = \"Interval time warning: \"\n        if start &lt; all[0]:\n            warnings.warn(\n                timestamp_warn\n                + \"start time smaller than first timestamp, \"\n                + f\"substituting first: {start} &lt; {all[0]}\"\n            )\n            start = all[0]\n\n        if stop &gt; all[-1]:\n            warnings.warn(\n                timestamp_warn\n                + \"stop time larger than last timestamp, \"\n                + f\"substituting last: {stop} &lt; {all[-1]}\"\n            )\n            stop = all[-1]\n\n        frm, to = np.searchsorted(all, (start, stop))\n        to = min(to, nsamples)\n        return frm, to\n\n    def filter_data_nwb(\n        self,\n        analysis_file_abs_path: str,\n        eseries: pynwb.ecephys.ElectricalSeries,\n        filter_coeff: np.ndarray,\n        valid_times: np.ndarray,\n        electrode_ids: list,\n        decimation: int,\n        description: str = \"filtered data\",\n        type: Union[None, str] = None,\n    ):\n        \"\"\"\n        Filter data from an NWB electrical series using the ghostipy package,\n        and save the result as a new electrical series in the analysis NWB file.\n\n        Parameters\n        ----------\n        analysis_file_abs_path : str\n            Full path to the analysis NWB file.\n        eseries : pynwb.ecephys.ElectricalSeries\n            Electrical series with data to be filtered.\n        filter_coeff : np.ndarray\n            Array with filter coefficients for FIR filter.\n        valid_times : np.ndarray\n            Array with start and stop times of intervals to be filtered.\n        electrode_ids : list\n            List of electrode IDs to filter.\n        decimation : int\n            Decimation factor.\n        description : str\n            Description of the filtered data.\n        data_type : Union[None, str]\n            Type of data (e.g., \"LFP\").\n\n        Returns\n        -------\n        tuple\n            The NWB object ID of the filtered data and a list containing the\n            first and last timestamps.\n        \"\"\"\n        MEM_USE_LIMIT = 0.9  # % of RAM use permitted\n\n        gsp = _import_ghostipy()\n\n        data_on_disk = eseries.data\n        timestamps_on_disk = eseries.timestamps\n\n        n_samples = len(timestamps_on_disk)\n        time_axis = 0 if data_on_disk.shape[0] == n_samples else 1\n        electrode_axis = 1 - time_axis\n\n        n_electrodes = data_on_disk.shape[electrode_axis]\n        input_dim_restrictions = [None] * len(data_on_disk.shape)\n\n        # Get input dimension restrictions\n        input_dim_restrictions[electrode_axis] = np.s_[\n            get_electrode_indices(eseries, electrode_ids)\n        ]\n\n        indices = []\n        output_shape_list = [0] * len(data_on_disk.shape)\n        output_shape_list[electrode_axis] = len(electrode_ids)\n        data_dtype = data_on_disk[0][0].dtype\n\n        filter_delay = self.calc_filter_delay(filter_coeff)\n\n        output_offsets = [0]\n\n        for a_start, a_stop in valid_times:\n            frm, to = self._time_bound_check(\n                a_start, a_stop, timestamps_on_disk, n_samples\n            )\n\n            indices.append((frm, to))\n\n            shape, _ = gsp.filter_data_fir(\n                data_on_disk,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[frm, to - 1],\n                output_index_bounds=[filter_delay, filter_delay + to - frm],\n                describe_dims=True,\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n            )\n            output_offsets.append(output_offsets[-1] + shape[time_axis])\n            output_shape_list[time_axis] += shape[time_axis]\n\n        # Create dynamic table region and electrode series, write/close file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n\n            # get the indices of the electrodes in the electrode table\n            elect_ind = get_electrode_indices(nwbf, electrode_ids)\n\n            electrode_table_region = nwbf.create_electrode_table_region(\n                elect_ind, \"filtered electrode table\"\n            )\n            es = pynwb.ecephys.ElectricalSeries(\n                name=\"filtered data\",\n                data=np.empty(tuple(output_shape_list), dtype=data_dtype),\n                electrodes=electrode_table_region,\n                timestamps=np.empty(output_shape_list[time_axis]),\n                description=description,\n            )\n            if type == \"LFP\":\n                ecephys_module = nwbf.create_processing_module(\n                    name=\"ecephys\", description=description\n                )\n                ecephys_module.add(pynwb.ecephys.LFP(electrical_series=es))\n            else:\n                nwbf.add_scratch(es)\n\n            io.write(nwbf)\n\n        # Reload NWB file to get h5py objects for data/timestamps\n        # NOTE: CBroz - why io context within io context? Unindenting\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            es = nwbf.objects[es.object_id]\n            filtered_data = es.data\n            new_timestamps = es.timestamps\n            indices = np.array(indices, ndmin=2)\n            # Filter and write the output dataset\n            ts_offset = 0\n\n            print(\"Filtering data\")\n            for ii, (start, stop) in enumerate(indices):\n                # Calc size of timestamps + data, check if &lt; 90% of RAM\n                interval_samples = stop - start\n                req_mem = interval_samples * (\n                    timestamps_on_disk[0].itemsize\n                    + n_electrodes * data_on_disk[0][0].itemsize\n                )\n                if req_mem &lt; MEM_USE_LIMIT * psutil.virtual_memory().available:\n                    print(f\"Interval {ii}: loading data into memory\")\n                    timestamps = np.asarray(\n                        timestamps_on_disk[start:stop],\n                        dtype=timestamps_on_disk[0].dtype,\n                    )\n                    if time_axis == 0:\n                        data = np.asarray(\n                            data_on_disk[start:stop, :], dtype=data_dtype\n                        )\n                    else:\n                        data = np.asarray(\n                            data_on_disk[:, start:stop], dtype=data_dtype\n                        )\n                    extracted_ts = timestamps[0::decimation]\n                    new_timestamps[\n                        ts_offset : ts_offset + len(extracted_ts)\n                    ] = extracted_ts\n                    ts_offset += len(extracted_ts)\n                    input_index_bounds = [0, interval_samples - 1]\n\n                else:\n                    print(f\"Interval {ii}: leaving data on disk\")\n                    data = data_on_disk\n                    timestamps = timestamps_on_disk\n                    extracted_ts = timestamps[start:stop:decimation]\n                    new_timestamps[\n                        ts_offset : ts_offset + len(extracted_ts)\n                    ] = extracted_ts\n                    ts_offset += len(extracted_ts)\n                    input_index_bounds = [start, stop]\n\n                # filter the data\n                gsp.filter_data_fir(\n                    data,\n                    filter_coeff,\n                    axis=time_axis,\n                    input_index_bounds=input_index_bounds,\n                    output_index_bounds=[\n                        filter_delay,\n                        filter_delay + stop - start,\n                    ],\n                    ds=decimation,\n                    input_dim_restrictions=input_dim_restrictions,\n                    outarray=filtered_data,\n                    output_offset=output_offsets[ii],\n                )\n\n            start_end = [new_timestamps[0], new_timestamps[-1]]\n\n            io.write(nwbf)\n\n        return es.object_id, start_end\n\n    def filter_data(\n        self,\n        timestamps,\n        data,\n        filter_coeff,\n        valid_times,\n        electrodes,\n        decimation,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        timestamps: numpy array\n            List of timestamps for data\n        data:\n            original data array\n        filter_coeff: numpy array\n            Filter coefficients for FIR filter\n        valid_times: 2D numpy array\n            Start and stop times of intervals to be filtered\n        electrodes: list\n            Electrodes to filter\n        decimation:\n            decimation factor\n\n        Return\n        ------\n        filtered_data, timestamps\n        \"\"\"\n\n        gsp = _import_ghostipy()\n\n        n_dim = len(data.shape)\n        n_samples = len(timestamps)\n        time_axis = 0 if data.shape[0] == n_samples else 1\n        electrode_axis = 1 - time_axis\n        input_dim_restrictions = [None] * n_dim\n        input_dim_restrictions[electrode_axis] = np.s_[electrodes]\n\n        indices = []\n        output_shape_list = [0] * n_dim\n        output_shape_list[electrode_axis] = len(electrodes)\n        output_offsets = [0]\n\n        filter_delay = self.calc_filter_delay(filter_coeff)\n        for a_start, a_stop in valid_times:\n            frm, to = self._time_bound_check(\n                a_start, a_stop, timestamps, n_samples\n            )\n\n            indices.append((frm, to))\n\n            shape, _ = gsp.filter_data_fir(\n                data,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[frm, to],\n                output_index_bounds=[filter_delay, filter_delay + to - frm],\n                describe_dims=True,\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n            )\n            output_offsets.append(output_offsets[-1] + shape[time_axis])\n            output_shape_list[time_axis] += shape[time_axis]\n\n        # create the dataset and the timestamps array\n        filtered_data = np.empty(tuple(output_shape_list), dtype=data.dtype)\n\n        new_timestamps = np.empty(\n            (output_shape_list[time_axis],), timestamps.dtype\n        )\n\n        indices = np.array(indices, ndmin=2)\n\n        # Filter  the output dataset\n        ts_offset = 0\n\n        for ii, (start, stop) in enumerate(indices):\n            extracted_ts = timestamps[start:stop:decimation]\n\n            # print(f\"Diffs {np.diff(extracted_ts)}\")\n            new_timestamps[\n                ts_offset : ts_offset + len(extracted_ts)\n            ] = extracted_ts\n            ts_offset += len(extracted_ts)\n\n            # finally ready to filter data!\n            gsp.filter_data_fir(\n                data,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[start, stop],\n                output_index_bounds=[filter_delay, filter_delay + stop - start],\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n                outarray=filtered_data,\n                output_offset=output_offsets[ii],\n            )\n\n        return filtered_data, new_timestamps\n\n    def calc_filter_delay(self, filter_coeff):\n        \"\"\"\n        Parameters\n        ----------\n        filter_coeff: numpy array\n\n        Return\n        ------\n        filter delay: int\n        \"\"\"\n        return (len(filter_coeff) - 1) // 2\n\n    def create_standard_filters(self):\n        \"\"\"Add standard filters to the Filter table\n\n        Includes 0-400 Hz low pass for continuous raw data -&gt; LFP\n        \"\"\"\n        self.add_filter(\n            \"LFP 0-400 Hz\",\n            20000,\n            \"lowpass\",\n            [400, 425],\n            \"standard LFP filter for 20 KHz data\",\n        )\n        self.add_filter(\n            \"LFP 0-400 Hz\",\n            30000,\n            \"lowpass\",\n            [400, 425],\n            \"standard LFP filter for 30 KHz data\",\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.add_filter", "title": "<code>add_filter(filter_name, fs, filter_type, band_edges, comments='')</code>", "text": "<p>Add filter to the Filter table.</p> <p>Parameters:</p> Name Type Description Default <code>filter_name</code> <code>str</code> <p>The name of the filter.</p> required <code>fs</code> <code>float</code> <p>The filter sampling rate.</p> required <code>filter_type</code> <code>str</code> <p>The type of the filter ('lowpass', 'highpass', or 'bandpass').</p> required <code>band_edges</code> <code>list</code> <p>The band edges for the filter.</p> required <code>comments</code> <code>str</code> <p>Additional comments for the filter. Default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>None</code> <p>Returns None if there is an error in the filter type or band frequencies.</p> <p>Raises:</p> Type Description <code>Exception:</code> <p>Raises an exception if an unexpected filter type is encountered.</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def add_filter(\n    self,\n    filter_name: str,\n    fs: float,\n    filter_type: str,\n    band_edges: list,\n    comments: str = \"\",\n) -&gt; None:\n    \"\"\"Add filter to the Filter table.\n\n    Parameters\n    ----------\n    filter_name: str\n        The name of the filter.\n    fs: float\n        The filter sampling rate.\n    filter_type: str\n        The type of the filter ('lowpass', 'highpass', or 'bandpass').\n    band_edges: List[float]\n        The band edges for the filter.\n    comments: str, optional)\n        Additional comments for the filter. Default \"\".\n\n    Returns\n    -------\n    None\n        Returns None if there is an error in the filter type or band\n        frequencies.\n\n    Raises\n    ------\n    Exception:\n        Raises an exception if an unexpected filter type is encountered.\n    \"\"\"\n    VALID_FILTERS = {\"lowpass\": 2, \"highpass\": 2, \"bandpass\": 4}\n    FILTER_ERR = \"Error in Filter.add_filter: \"\n    FILTER_N_ERR = FILTER_ERR + \"filter {} requires {} band_frequencies.\"\n\n    # add an FIR bandpass filter of the specified type.\n    # band_edges should be as follows:\n    #   low pass : [high_pass high_stop]\n    #   high pass: [low stop low pass]\n    #   band pass: [low_stop low_pass high_pass high_stop].\n    if filter_type not in VALID_FILTERS:\n        print(\n            FILTER_ERR\n            + f\"{filter_type} not valid type: {VALID_FILTERS.keys()}\"\n        )\n        return None\n\n    if not len(band_edges) == VALID_FILTERS[filter_type]:\n        print(FILTER_N_ERR.format(filter_name, VALID_FILTERS[filter_type]))\n        return None\n\n    gsp = _import_ghostipy()\n    TRANS_SPLINE = 2  # transition spline will be quadratic\n\n    if filter_type != \"bandpass\":\n        transition_width = band_edges[1] - band_edges[0]\n\n    else:\n        # transition width is mean of left and right transition regions\n        transition_width = (\n            (band_edges[1] - band_edges[0])\n            + (band_edges[3] - band_edges[2])\n        ) / 2.0\n\n    numtaps = gsp.estimate_taps(fs, transition_width)\n    filterdict = {\n        \"filter_type\": filter_type,\n        \"filter_name\": filter_name,\n        \"filter_sampling_rate\": fs,\n        \"filter_comments\": comments,\n        \"filter_low_stop\": 0,\n        \"filter_low_pass\": 0,\n        \"filter_high_pass\": 0,\n        \"filter_high_stop\": 0,\n        \"filter_band_edges\": np.asarray(band_edges),\n    }\n\n    # set the desired frequency response\n    if filter_type == \"lowpass\":\n        desired = [1, 0]\n        pass_stop_dict = {\n            \"filter_high_pass\": band_edges[0],\n            \"filter_high_stop\": band_edges[1],\n        }\n    elif filter_type == \"highpass\":\n        desired = [0, 1]\n        pass_stop_dict = {\n            \"filter_low_stop\": band_edges[0],\n            \"filter_low_pass\": band_edges[1],\n        }\n    else:\n        desired = [0, 1, 1, 0]\n        pass_stop_dict = {\n            \"filter_low_stop\": band_edges[0],\n            \"filter_low_pass\": band_edges[1],\n            \"filter_high_pass\": band_edges[2],\n            \"filter_high_stop\": band_edges[3],\n        }\n\n    # create 1d array for coefficients\n    filterdict.update(\n        {\n            **pass_stop_dict,\n            \"filter_coeff\": np.array(\n                gsp.firdesign(\n                    numtaps, band_edges, desired, fs=fs, p=TRANS_SPLINE\n                ),\n                ndmin=1,\n            ),\n        }\n    )\n\n    self.insert1(filterdict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.filter_data_nwb", "title": "<code>filter_data_nwb(analysis_file_abs_path, eseries, filter_coeff, valid_times, electrode_ids, decimation, description='filtered data', type=None)</code>", "text": "<p>Filter data from an NWB electrical series using the ghostipy package, and save the result as a new electrical series in the analysis NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_abs_path</code> <code>str</code> <p>Full path to the analysis NWB file.</p> required <code>eseries</code> <code>ElectricalSeries</code> <p>Electrical series with data to be filtered.</p> required <code>filter_coeff</code> <code>ndarray</code> <p>Array with filter coefficients for FIR filter.</p> required <code>valid_times</code> <code>ndarray</code> <p>Array with start and stop times of intervals to be filtered.</p> required <code>electrode_ids</code> <code>list</code> <p>List of electrode IDs to filter.</p> required <code>decimation</code> <code>int</code> <p>Decimation factor.</p> required <code>description</code> <code>str</code> <p>Description of the filtered data.</p> <code>'filtered data'</code> <code>data_type</code> <code>Union[None, str]</code> <p>Type of data (e.g., \"LFP\").</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The NWB object ID of the filtered data and a list containing the first and last timestamps.</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def filter_data_nwb(\n    self,\n    analysis_file_abs_path: str,\n    eseries: pynwb.ecephys.ElectricalSeries,\n    filter_coeff: np.ndarray,\n    valid_times: np.ndarray,\n    electrode_ids: list,\n    decimation: int,\n    description: str = \"filtered data\",\n    type: Union[None, str] = None,\n):\n    \"\"\"\n    Filter data from an NWB electrical series using the ghostipy package,\n    and save the result as a new electrical series in the analysis NWB file.\n\n    Parameters\n    ----------\n    analysis_file_abs_path : str\n        Full path to the analysis NWB file.\n    eseries : pynwb.ecephys.ElectricalSeries\n        Electrical series with data to be filtered.\n    filter_coeff : np.ndarray\n        Array with filter coefficients for FIR filter.\n    valid_times : np.ndarray\n        Array with start and stop times of intervals to be filtered.\n    electrode_ids : list\n        List of electrode IDs to filter.\n    decimation : int\n        Decimation factor.\n    description : str\n        Description of the filtered data.\n    data_type : Union[None, str]\n        Type of data (e.g., \"LFP\").\n\n    Returns\n    -------\n    tuple\n        The NWB object ID of the filtered data and a list containing the\n        first and last timestamps.\n    \"\"\"\n    MEM_USE_LIMIT = 0.9  # % of RAM use permitted\n\n    gsp = _import_ghostipy()\n\n    data_on_disk = eseries.data\n    timestamps_on_disk = eseries.timestamps\n\n    n_samples = len(timestamps_on_disk)\n    time_axis = 0 if data_on_disk.shape[0] == n_samples else 1\n    electrode_axis = 1 - time_axis\n\n    n_electrodes = data_on_disk.shape[electrode_axis]\n    input_dim_restrictions = [None] * len(data_on_disk.shape)\n\n    # Get input dimension restrictions\n    input_dim_restrictions[electrode_axis] = np.s_[\n        get_electrode_indices(eseries, electrode_ids)\n    ]\n\n    indices = []\n    output_shape_list = [0] * len(data_on_disk.shape)\n    output_shape_list[electrode_axis] = len(electrode_ids)\n    data_dtype = data_on_disk[0][0].dtype\n\n    filter_delay = self.calc_filter_delay(filter_coeff)\n\n    output_offsets = [0]\n\n    for a_start, a_stop in valid_times:\n        frm, to = self._time_bound_check(\n            a_start, a_stop, timestamps_on_disk, n_samples\n        )\n\n        indices.append((frm, to))\n\n        shape, _ = gsp.filter_data_fir(\n            data_on_disk,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[frm, to - 1],\n            output_index_bounds=[filter_delay, filter_delay + to - frm],\n            describe_dims=True,\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n        )\n        output_offsets.append(output_offsets[-1] + shape[time_axis])\n        output_shape_list[time_axis] += shape[time_axis]\n\n    # Create dynamic table region and electrode series, write/close file\n    with pynwb.NWBHDF5IO(\n        path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n\n        # get the indices of the electrodes in the electrode table\n        elect_ind = get_electrode_indices(nwbf, electrode_ids)\n\n        electrode_table_region = nwbf.create_electrode_table_region(\n            elect_ind, \"filtered electrode table\"\n        )\n        es = pynwb.ecephys.ElectricalSeries(\n            name=\"filtered data\",\n            data=np.empty(tuple(output_shape_list), dtype=data_dtype),\n            electrodes=electrode_table_region,\n            timestamps=np.empty(output_shape_list[time_axis]),\n            description=description,\n        )\n        if type == \"LFP\":\n            ecephys_module = nwbf.create_processing_module(\n                name=\"ecephys\", description=description\n            )\n            ecephys_module.add(pynwb.ecephys.LFP(electrical_series=es))\n        else:\n            nwbf.add_scratch(es)\n\n        io.write(nwbf)\n\n    # Reload NWB file to get h5py objects for data/timestamps\n    # NOTE: CBroz - why io context within io context? Unindenting\n    with pynwb.NWBHDF5IO(\n        path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        es = nwbf.objects[es.object_id]\n        filtered_data = es.data\n        new_timestamps = es.timestamps\n        indices = np.array(indices, ndmin=2)\n        # Filter and write the output dataset\n        ts_offset = 0\n\n        print(\"Filtering data\")\n        for ii, (start, stop) in enumerate(indices):\n            # Calc size of timestamps + data, check if &lt; 90% of RAM\n            interval_samples = stop - start\n            req_mem = interval_samples * (\n                timestamps_on_disk[0].itemsize\n                + n_electrodes * data_on_disk[0][0].itemsize\n            )\n            if req_mem &lt; MEM_USE_LIMIT * psutil.virtual_memory().available:\n                print(f\"Interval {ii}: loading data into memory\")\n                timestamps = np.asarray(\n                    timestamps_on_disk[start:stop],\n                    dtype=timestamps_on_disk[0].dtype,\n                )\n                if time_axis == 0:\n                    data = np.asarray(\n                        data_on_disk[start:stop, :], dtype=data_dtype\n                    )\n                else:\n                    data = np.asarray(\n                        data_on_disk[:, start:stop], dtype=data_dtype\n                    )\n                extracted_ts = timestamps[0::decimation]\n                new_timestamps[\n                    ts_offset : ts_offset + len(extracted_ts)\n                ] = extracted_ts\n                ts_offset += len(extracted_ts)\n                input_index_bounds = [0, interval_samples - 1]\n\n            else:\n                print(f\"Interval {ii}: leaving data on disk\")\n                data = data_on_disk\n                timestamps = timestamps_on_disk\n                extracted_ts = timestamps[start:stop:decimation]\n                new_timestamps[\n                    ts_offset : ts_offset + len(extracted_ts)\n                ] = extracted_ts\n                ts_offset += len(extracted_ts)\n                input_index_bounds = [start, stop]\n\n            # filter the data\n            gsp.filter_data_fir(\n                data,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=input_index_bounds,\n                output_index_bounds=[\n                    filter_delay,\n                    filter_delay + stop - start,\n                ],\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n                outarray=filtered_data,\n                output_offset=output_offsets[ii],\n            )\n\n        start_end = [new_timestamps[0], new_timestamps[-1]]\n\n        io.write(nwbf)\n\n    return es.object_id, start_end\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.filter_data", "title": "<code>filter_data(timestamps, data, filter_coeff, valid_times, electrodes, decimation)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>timestamps</code> <p>List of timestamps for data</p> required <code>data</code> <p>original data array</p> required <code>filter_coeff</code> <p>Filter coefficients for FIR filter</p> required <code>valid_times</code> <p>Start and stop times of intervals to be filtered</p> required <code>electrodes</code> <p>Electrodes to filter</p> required <code>decimation</code> <p>decimation factor</p> required"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.filter_data--return", "title": "Return", "text": "<p>filtered_data, timestamps</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def filter_data(\n    self,\n    timestamps,\n    data,\n    filter_coeff,\n    valid_times,\n    electrodes,\n    decimation,\n):\n    \"\"\"\n    Parameters\n    ----------\n    timestamps: numpy array\n        List of timestamps for data\n    data:\n        original data array\n    filter_coeff: numpy array\n        Filter coefficients for FIR filter\n    valid_times: 2D numpy array\n        Start and stop times of intervals to be filtered\n    electrodes: list\n        Electrodes to filter\n    decimation:\n        decimation factor\n\n    Return\n    ------\n    filtered_data, timestamps\n    \"\"\"\n\n    gsp = _import_ghostipy()\n\n    n_dim = len(data.shape)\n    n_samples = len(timestamps)\n    time_axis = 0 if data.shape[0] == n_samples else 1\n    electrode_axis = 1 - time_axis\n    input_dim_restrictions = [None] * n_dim\n    input_dim_restrictions[electrode_axis] = np.s_[electrodes]\n\n    indices = []\n    output_shape_list = [0] * n_dim\n    output_shape_list[electrode_axis] = len(electrodes)\n    output_offsets = [0]\n\n    filter_delay = self.calc_filter_delay(filter_coeff)\n    for a_start, a_stop in valid_times:\n        frm, to = self._time_bound_check(\n            a_start, a_stop, timestamps, n_samples\n        )\n\n        indices.append((frm, to))\n\n        shape, _ = gsp.filter_data_fir(\n            data,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[frm, to],\n            output_index_bounds=[filter_delay, filter_delay + to - frm],\n            describe_dims=True,\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n        )\n        output_offsets.append(output_offsets[-1] + shape[time_axis])\n        output_shape_list[time_axis] += shape[time_axis]\n\n    # create the dataset and the timestamps array\n    filtered_data = np.empty(tuple(output_shape_list), dtype=data.dtype)\n\n    new_timestamps = np.empty(\n        (output_shape_list[time_axis],), timestamps.dtype\n    )\n\n    indices = np.array(indices, ndmin=2)\n\n    # Filter  the output dataset\n    ts_offset = 0\n\n    for ii, (start, stop) in enumerate(indices):\n        extracted_ts = timestamps[start:stop:decimation]\n\n        # print(f\"Diffs {np.diff(extracted_ts)}\")\n        new_timestamps[\n            ts_offset : ts_offset + len(extracted_ts)\n        ] = extracted_ts\n        ts_offset += len(extracted_ts)\n\n        # finally ready to filter data!\n        gsp.filter_data_fir(\n            data,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[start, stop],\n            output_index_bounds=[filter_delay, filter_delay + stop - start],\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n            outarray=filtered_data,\n            output_offset=output_offsets[ii],\n        )\n\n    return filtered_data, new_timestamps\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.calc_filter_delay", "title": "<code>calc_filter_delay(filter_coeff)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>filter_coeff</code> required"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.calc_filter_delay--return", "title": "Return", "text": "<p>filter delay: int</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def calc_filter_delay(self, filter_coeff):\n    \"\"\"\n    Parameters\n    ----------\n    filter_coeff: numpy array\n\n    Return\n    ------\n    filter delay: int\n    \"\"\"\n    return (len(filter_coeff) - 1) // 2\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.create_standard_filters", "title": "<code>create_standard_filters()</code>", "text": "<p>Add standard filters to the Filter table</p> <p>Includes 0-400 Hz low pass for continuous raw data -&gt; LFP</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def create_standard_filters(self):\n    \"\"\"Add standard filters to the Filter table\n\n    Includes 0-400 Hz low pass for continuous raw data -&gt; LFP\n    \"\"\"\n    self.add_filter(\n        \"LFP 0-400 Hz\",\n        20000,\n        \"lowpass\",\n        [400, 425],\n        \"standard LFP filter for 20 KHz data\",\n    )\n    self.add_filter(\n        \"LFP 0-400 Hz\",\n        30000,\n        \"lowpass\",\n        [400, 425],\n        \"standard LFP filter for 30 KHz data\",\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/", "title": "common_interval.py", "text": ""}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start/end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n        \"\"\"Add each entry in the NWB file epochs table to the IntervalList.\n\n        The interval list name for each epoch is set to the first tag for the\n        epoch. If the epoch has no tags, then 'interval_x' will be used as the\n        interval list name, where x is the index (0-indexed) of the epoch in the\n        epochs table. The start time and stop time of the epoch are stored in\n        the valid_times field as a numpy array of [start time, stop time] for\n        each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session\n            table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n\n        epochs = nwbf.epochs.to_dataframe()\n\n        for _, epoch_data in epochs.iterrows():\n            epoch_dict = {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": epoch_data.tags[0]\n                if epoch_data.tags\n                else f\"interval_{epoch_data[0]}\",\n                \"valid_times\": np.asarray(\n                    [[epoch_data.start_time, epoch_data.stop_time]]\n                ),\n            }\n\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n    \"\"\"Add each entry in the NWB file epochs table to the IntervalList.\n\n    The interval list name for each epoch is set to the first tag for the\n    epoch. If the epoch has no tags, then 'interval_x' will be used as the\n    interval list name, where x is the index (0-indexed) of the epoch in the\n    epochs table. The start time and stop time of the epoch are stored in\n    the valid_times field as a numpy array of [start time, stop time] for\n    each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session\n        table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n\n    epochs = nwbf.epochs.to_dataframe()\n\n    for _, epoch_data in epochs.iterrows():\n        epoch_dict = {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": epoch_data.tags[0]\n            if epoch_data.tags\n            else f\"interval_{epoch_data[0]}\",\n            \"valid_times\": np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            ),\n        }\n\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.intervals_by_length", "title": "<code>intervals_by_length(interval_list, min_length=0.0, max_length=10000000000.0)</code>", "text": "<p>Select intervals of certain lengths from an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval in seconds.</p> required <code>min_length</code> <code>float</code> <p>Minimum interval length in seconds. Defaults to 0.0.</p> <code>0.0</code> <code>max_length</code> <code>float</code> <p>Maximum interval length in seconds. Defaults to 1e10.</p> <code>10000000000.0</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def intervals_by_length(interval_list, min_length=0.0, max_length=1e10):\n    \"\"\"Select intervals of certain lengths from an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval in seconds.\n    min_length : float, optional\n        Minimum interval length in seconds. Defaults to 0.0.\n    max_length : float, optional\n        Maximum interval length in seconds. Defaults to 1e10.\n    \"\"\"\n    lengths = np.ravel(np.diff(interval_list))\n    return interval_list[\n        np.logical_and(lengths &gt; min_length, lengths &lt; max_length)\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_contains_ind", "title": "<code>interval_list_contains_ind(interval_list, timestamps)</code>", "text": "<p>Find indices of list of timestamps contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval in seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_contains_ind(interval_list, timestamps):\n    \"\"\"Find indices of list of timestamps contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval in seconds.\n    timestamps : array_like\n    \"\"\"\n    ind = []\n    for interval in interval_list:\n        ind += np.ravel(\n            np.argwhere(\n                np.logical_and(\n                    timestamps &gt;= interval[0], timestamps &lt;= interval[1]\n                )\n            )\n        ).tolist()\n    return np.asarray(ind)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_contains", "title": "<code>interval_list_contains(interval_list, timestamps)</code>", "text": "<p>Find timestamps that are contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval in seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_contains(interval_list, timestamps):\n    \"\"\"Find timestamps that are contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval in seconds.\n    timestamps : array_like\n    \"\"\"\n    ind = []\n    for interval in interval_list:\n        ind += np.ravel(\n            np.argwhere(\n                np.logical_and(\n                    timestamps &gt;= interval[0], timestamps &lt;= interval[1]\n                )\n            )\n        ).tolist()\n    return timestamps[ind]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_excludes_ind", "title": "<code>interval_list_excludes_ind(interval_list, timestamps)</code>", "text": "<p>Find indices of timestamps that are not contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval in seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_excludes_ind(interval_list, timestamps):\n    \"\"\"Find indices of timestamps that are not contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval in seconds.\n    timestamps : array_like\n    \"\"\"\n\n    contained_inds = interval_list_contains_ind(interval_list, timestamps)\n    return np.setdiff1d(np.arange(len(timestamps)), contained_inds)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_excludes", "title": "<code>interval_list_excludes(interval_list, timestamps)</code>", "text": "<p>Find timestamps that are not contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval in seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_excludes(interval_list, timestamps):\n    \"\"\"Find timestamps that are not contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval in seconds.\n    timestamps : array_like\n    \"\"\"\n    contained_times = interval_list_contains(interval_list, timestamps)\n    return np.setdiff1d(timestamps, contained_times)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_intersect", "title": "<code>interval_list_intersect(interval_list1, interval_list2, min_length=0)</code>", "text": "<p>Finds the intersections between two interval lists</p> <p>Parameters:</p> Name Type Description Default <code>interval_list1</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>interval_list2</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>min_length</code> <code>float, optional.</code> <p>Minimum length of intervals to include, default 0</p> <code>0</code> <p>Each interval is (start time, stop time)</p> <p>Returns:</p> Name Type Description <code>interval_list</code> <code>(array, (N, 2))</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_intersect(interval_list1, interval_list2, min_length=0):\n    \"\"\"Finds the intersections between two interval lists\n\n    Parameters\n    ----------\n    interval_list1 : np.array, (N,2) where N = number of intervals\n    interval_list2 : np.array, (N,2) where N = number of intervals\n    min_length : float, optional.\n        Minimum length of intervals to include, default 0\n\n    Each interval is (start time, stop time)\n\n    Returns\n    -------\n    interval_list: np.array, (N,2)\n    \"\"\"\n\n    # Consolidate interval lists to disjoint int'ls by sorting &amp; applying union\n    interval_list1 = consolidate_intervals(interval_list1)\n    interval_list2 = consolidate_intervals(interval_list2)\n\n    # then do pairwise comparison and collect intersections\n    intersecting_intervals = [\n        _intersection(interval2, interval1)\n        for interval2 in interval_list2\n        for interval1 in interval_list1\n        if _intersection(interval2, interval1) is not None\n    ]\n\n    # if no intersection, then return an empty list\n    if not intersecting_intervals:\n        return []\n\n    intersecting_intervals = np.asarray(intersecting_intervals)\n    intersecting_intervals = intersecting_intervals[\n        np.argsort(intersecting_intervals[:, 0])\n    ]\n\n    return intervals_by_length(intersecting_intervals, min_length=min_length)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.union_adjacent_index", "title": "<code>union_adjacent_index(interval1, interval2)</code>", "text": "<p>Union index-adjacent intervals. If not adjacent, just concatenate.</p> <p>e.g. [a,b] and [b+1, c] is converted to [a,c]</p> <p>Parameters:</p> Name Type Description Default <code>interval1</code> <code>array</code> required <code>interval2</code> <code>array</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def union_adjacent_index(interval1, interval2):\n    \"\"\"Union index-adjacent intervals. If not adjacent, just concatenate.\n\n    e.g. [a,b] and [b+1, c] is converted to [a,c]\n\n    Parameters\n    ----------\n    interval1 : np.array\n    interval2 : np.array\n    \"\"\"\n    interval1 = np.atleast_2d(interval1)\n    interval2 = np.atleast_2d(interval2)\n\n    if (\n        interval1[-1][1] + 1 == interval2[0][0]\n        or interval2[0][1] + 1 == interval1[-1][0]\n    ):\n        x = np.array(\n            [\n                [\n                    np.min([interval1[-1][0], interval2[0][0]]),\n                    np.max([interval1[-1][1], interval2[0][1]]),\n                ]\n            ]\n        )\n        return np.concatenate((interval1[:-1], x), axis=0)\n    else:\n        return np.concatenate((interval1, interval2), axis=0)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_union", "title": "<code>interval_list_union(interval_list1, interval_list2, min_length=0.0, max_length=10000000000.0)</code>", "text": "<p>Finds the union (all times in one or both) for two interval lists</p> <p>Parameters:</p> Name Type Description Default <code>interval_list1</code> <code>ndarray</code> <p>The first interval list [start, stop]</p> required <code>interval_list2</code> <code>ndarray</code> <p>The second interval list [start, stop]</p> required <code>min_length</code> <code>float</code> <p>Minimum length of interval for inclusion in output, default 0.0</p> <code>0.0</code> <code>max_length</code> <code>float</code> <p>Maximum length of interval for inclusion in output, default 1e10</p> <code>10000000000.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of intervals [start, stop]</p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_union(\n    interval_list1: np.ndarray,\n    interval_list2: np.ndarray,\n    min_length: float = 0.0,\n    max_length: float = 1e10,\n) -&gt; np.ndarray:\n    \"\"\"Finds the union (all times in one or both) for two interval lists\n\n    Parameters\n    ----------\n    interval_list1 : np.ndarray\n        The first interval list [start, stop]\n    interval_list2 : np.ndarray\n        The second interval list [start, stop]\n    min_length : float, optional\n        Minimum length of interval for inclusion in output, default 0.0\n    max_length : float, optional\n        Maximum length of interval for inclusion in output, default 1e10\n\n    Returns\n    -------\n    np.ndarray\n        Array of intervals [start, stop]\n    \"\"\"\n\n    il1, il1_start_end = _parallel_union(interval_list1)\n    il2, il2_start_end = _parallel_union(interval_list2)\n\n    # Concatenate the two lists so we can resort the intervals and apply the\n    # same sorting to the start-end arrays\n    combined_intervals = np.concatenate((il1, il2))\n    ss = np.concatenate((il1_start_end, il2_start_end))\n    sort_ind = np.argsort(combined_intervals)\n    combined_intervals = combined_intervals[sort_ind]\n\n    # a cumulative sum of 1 indicates the beginning of a joint interval; a\n    # cumulative sum of 0 indicates the end\n    union_starts = np.ravel(np.array(np.where(np.cumsum(ss[sort_ind]) == 1)))\n    union_stops = np.ravel(np.array(np.where(np.cumsum(ss[sort_ind]) == 0)))\n    union = [\n        [combined_intervals[start], combined_intervals[stop]]\n        for start, stop in zip(union_starts, union_stops)\n    ]\n\n    return np.asarray(union)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_censor", "title": "<code>interval_list_censor(interval_list, timestamps)</code>", "text": "<p>Returns new interval list that starts/ends at first/last timestamp</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>numpy array of intervals [start, stop]</code> <p>interval list from IntervalList valid times</p> required <code>timestamps</code> <code>numpy array or list</code> required <p>Returns:</p> Type Description <code>interval_list (numpy array of intervals [start, stop])</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_censor(interval_list, timestamps):\n    \"\"\"Returns new interval list that starts/ends at first/last timestamp\n\n    Parameters\n    ----------\n    interval_list : numpy array of intervals [start, stop]\n        interval list from IntervalList valid times\n    timestamps : numpy array or list\n\n    Returns\n    -------\n    interval_list (numpy array of intervals [start, stop])\n    \"\"\"\n    # check that all timestamps are in the interval list\n    if len(interval_list_contains_ind(interval_list, timestamps)) != len(\n        timestamps\n    ):\n        raise ValueError(\"Interval_list must contain all timestamps\")\n\n    timestamps_interval = np.asarray([[timestamps[0], timestamps[-1]]])\n    return interval_list_intersect(interval_list, timestamps_interval)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_from_inds", "title": "<code>interval_from_inds(list_frames)</code>", "text": "<p>Converts a list of indices to a list of intervals.</p> <p>e.g. [2,3,4,6,7,8,9,10] -&gt; [[2,4],[6,10]]</p> <p>Parameters:</p> Name Type Description Default <code>list_frames</code> <code>array_like of int</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_from_inds(list_frames):\n    \"\"\"Converts a list of indices to a list of intervals.\n\n    e.g. [2,3,4,6,7,8,9,10] -&gt; [[2,4],[6,10]]\n\n    Parameters\n    ----------\n    list_frames : array_like of int\n    \"\"\"\n    list_frames = np.unique(list_frames)\n    interval_list = []\n    for key, group in itertools.groupby(\n        enumerate(list_frames), lambda t: t[1] - t[0]\n    ):\n        group = list(group)\n        interval_list.append([group[0][1], group[-1][1]])\n    return np.asarray(interval_list)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_set_difference_inds", "title": "<code>interval_set_difference_inds(intervals1, intervals2)</code>", "text": "<p>e.g. intervals1 = [(0, 5), (8, 10)] intervals2 = [(1, 2), (3, 4), (6, 9)]</p> <p>result = [(0, 1), (4, 5), (9, 10)]</p> <p>Parameters:</p> Name Type Description Default <code>intervals1</code> <code>_type_</code> <p>description</p> required <code>intervals2</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_set_difference_inds(intervals1, intervals2):\n    \"\"\"\n    e.g.\n    intervals1 = [(0, 5), (8, 10)]\n    intervals2 = [(1, 2), (3, 4), (6, 9)]\n\n    result = [(0, 1), (4, 5), (9, 10)]\n\n    Parameters\n    ----------\n    intervals1 : _type_\n        _description_\n    intervals2 : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    result = []\n    i = j = 0\n    while i &lt; len(intervals1) and j &lt; len(intervals2):\n        if intervals1[i][1] &lt;= intervals2[j][0]:\n            result.append(intervals1[i])\n            i += 1\n        elif intervals2[j][1] &lt;= intervals1[i][0]:\n            j += 1\n        else:\n            if intervals1[i][0] &lt; intervals2[j][0]:\n                result.append((intervals1[i][0], intervals2[j][0]))\n            if intervals1[i][1] &gt; intervals2[j][1]:\n                intervals1[i] = (intervals2[j][1], intervals1[i][1])\n                j += 1\n            else:\n                i += 1\n    result += intervals1[i:]\n    return result\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/", "title": "common_lab.py", "text": "<p>Schema for institution, lab team/name/members. Session-independent.</p>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabMember", "title": "<code>LabMember</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabMember(dj.Manual):\n    definition = \"\"\"\n    lab_member_name: varchar(80)\n    ---\n    first_name: varchar(200)\n    last_name: varchar(200)\n    \"\"\"\n\n    # NOTE that names must be unique here. If there are two neuroscientists\n    # named Jack Black that have data in this database, this will create an\n    # incorrect linkage. NWB does not yet provide unique IDs for names.\n\n    class LabMemberInfo(dj.Part):\n        definition = \"\"\"\n        # Information about lab member in the context of Frank lab network\n        -&gt; LabMember\n        ---\n        google_user_name: varchar(200)         # For permission to curate\n        datajoint_user_name = \"\": varchar(200) # For permission to delete ns\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n        \"\"\"Insert lab member information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf: pynwb.NWBFile\n            The NWB file with experimenter information.\n        \"\"\"\n        if isinstance(nwbf, str):\n            nwb_file_abspath = Nwbfile.get_abs_path(nwbf, new_file=True)\n            nwbf = get_nwb_file(nwb_file_abspath)\n\n        if nwbf.experimenter is None:\n            print(\"No experimenter metadata found.\\n\")\n            return\n\n        for experimenter in nwbf.experimenter:\n            cls.insert_from_name(experimenter)\n\n            # each person is by default the member of their own LabTeam\n            # (same as their name)\n\n            full_name, _, _ = decompose_name(experimenter)\n            LabTeam.create_new_team(\n                team_name=full_name, team_members=[full_name]\n            )\n\n    @classmethod\n    def insert_from_name(cls, full_name):\n        \"\"\"Insert a lab member by name.\n\n        Parameters\n        ----------\n        full_name : str\n            The name to be added.\n        \"\"\"\n        _, first, last = decompose_name(full_name)\n        cls.insert1(\n            dict(\n                lab_member_name=f\"{first} {last}\",\n                first_name=first,\n                last_name=last,\n            ),\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabMember.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab member information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <p>The NWB file with experimenter information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n    \"\"\"Insert lab member information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf: pynwb.NWBFile\n        The NWB file with experimenter information.\n    \"\"\"\n    if isinstance(nwbf, str):\n        nwb_file_abspath = Nwbfile.get_abs_path(nwbf, new_file=True)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n    if nwbf.experimenter is None:\n        print(\"No experimenter metadata found.\\n\")\n        return\n\n    for experimenter in nwbf.experimenter:\n        cls.insert_from_name(experimenter)\n\n        # each person is by default the member of their own LabTeam\n        # (same as their name)\n\n        full_name, _, _ = decompose_name(experimenter)\n        LabTeam.create_new_team(\n            team_name=full_name, team_members=[full_name]\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabMember.insert_from_name", "title": "<code>insert_from_name(full_name)</code>  <code>classmethod</code>", "text": "<p>Insert a lab member by name.</p> <p>Parameters:</p> Name Type Description Default <code>full_name</code> <code>str</code> <p>The name to be added.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_name(cls, full_name):\n    \"\"\"Insert a lab member by name.\n\n    Parameters\n    ----------\n    full_name : str\n        The name to be added.\n    \"\"\"\n    _, first, last = decompose_name(full_name)\n    cls.insert1(\n        dict(\n            lab_member_name=f\"{first} {last}\",\n            first_name=first,\n            last_name=last,\n        ),\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabTeam", "title": "<code>LabTeam</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabTeam(dj.Manual):\n    definition = \"\"\"\n    team_name: varchar(80)\n    ---\n    team_description = \"\": varchar(2000)\n    \"\"\"\n\n    class LabTeamMember(dj.Part):\n        definition = \"\"\"\n        -&gt; LabTeam\n        -&gt; LabMember\n        \"\"\"\n\n    @classmethod\n    def create_new_team(\n        cls, team_name: str, team_members: list, team_description: str = \"\"\n    ):\n        \"\"\"Create a new team with a list of team members.\n\n        If the lab member does not exist in the database, they will be added.\n\n        Parameters\n        ----------\n        team_name : str\n            The name of the team.\n        team_members : str\n            The full names of the lab members that are part of the team.\n        team_description: str\n            The description of the team.\n        \"\"\"\n        labteam_dict = {\n            \"team_name\": team_name,\n            \"team_description\": team_description,\n        }\n        cls.insert1(labteam_dict, skip_duplicates=True)\n\n        for team_member in team_members:\n            LabMember.insert_from_name(team_member)\n            query = (\n                LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if not query:\n                print(\n                    f\"Please add the Google user ID for {team_member} in \"\n                    + \"LabMember.LabMemberInfo to help manage permissions.\"\n                )\n            labteammember_dict = {\n                \"team_name\": team_name,\n                \"lab_member_name\": team_member,\n            }\n            cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabTeam.create_new_team", "title": "<code>create_new_team(team_name, team_members, team_description='')</code>  <code>classmethod</code>", "text": "<p>Create a new team with a list of team members.</p> <p>If the lab member does not exist in the database, they will be added.</p> <p>Parameters:</p> Name Type Description Default <code>team_name</code> <code>str</code> <p>The name of the team.</p> required <code>team_members</code> <code>str</code> <p>The full names of the lab members that are part of the team.</p> required <code>team_description</code> <code>str</code> <p>The description of the team.</p> <code>''</code> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef create_new_team(\n    cls, team_name: str, team_members: list, team_description: str = \"\"\n):\n    \"\"\"Create a new team with a list of team members.\n\n    If the lab member does not exist in the database, they will be added.\n\n    Parameters\n    ----------\n    team_name : str\n        The name of the team.\n    team_members : str\n        The full names of the lab members that are part of the team.\n    team_description: str\n        The description of the team.\n    \"\"\"\n    labteam_dict = {\n        \"team_name\": team_name,\n        \"team_description\": team_description,\n    }\n    cls.insert1(labteam_dict, skip_duplicates=True)\n\n    for team_member in team_members:\n        LabMember.insert_from_name(team_member)\n        query = (\n            LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if not query:\n            print(\n                f\"Please add the Google user ID for {team_member} in \"\n                + \"LabMember.LabMemberInfo to help manage permissions.\"\n            )\n        labteammember_dict = {\n            \"team_name\": team_name,\n            \"lab_member_name\": team_member,\n        }\n        cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Institution", "title": "<code>Institution</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass Institution(dj.Manual):\n    definition = \"\"\"\n    institution_name: varchar(80)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n        \"\"\"Insert institution information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The NWB file with institution information.\n        \"\"\"\n        if nwbf.institution is None:\n            print(\"No institution metadata found.\\n\")\n            return\n\n        cls.insert1(\n            dict(institution_name=nwbf.institution), skip_duplicates=True\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Institution.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert institution information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The NWB file with institution information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n    \"\"\"Insert institution information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The NWB file with institution information.\n    \"\"\"\n    if nwbf.institution is None:\n        print(\"No institution metadata found.\\n\")\n        return\n\n    cls.insert1(\n        dict(institution_name=nwbf.institution), skip_duplicates=True\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Lab", "title": "<code>Lab</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"\n    lab_name: varchar(80)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n        \"\"\"Insert lab name information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The NWB file with lab name information.\n        \"\"\"\n        if nwbf.lab is None:\n            print(\"No lab metadata found.\\n\")\n            return\n        cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Lab.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab name information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The NWB file with lab name information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n    \"\"\"Insert lab name information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The NWB file with lab name information.\n    \"\"\"\n    if nwbf.lab is None:\n        print(\"No lab metadata found.\\n\")\n        return\n    cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.decompose_name", "title": "<code>decompose_name(full_name)</code>", "text": "<p>Return the full, first and last name parts of a full name.</p> <p>Names capitalized. Decomposes either comma- or space-separated.</p> <p>Parameters:</p> Name Type Description Default <code>full_name</code> <code>str</code> <p>The full name to decompose.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of the full, first, last name parts.</p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>def decompose_name(full_name: str) -&gt; tuple:\n    \"\"\"Return the full, first and last name parts of a full name.\n\n    Names capitalized. Decomposes either comma- or space-separated.\n\n    Parameters\n    ----------\n    full_name: str\n        The full name to decompose.\n\n    Returns\n    -------\n    tuple\n        A tuple of the full, first, last name parts.\n    \"\"\"\n    if full_name.count(\", \") != 1 and full_name.count(\" \") != 1:\n        raise ValueError(\n            f\"Names should be stored as 'last, first'. Skipping {full_name}\"\n        )\n    elif \", \" in full_name:\n        last, first = full_name.title().split(\", \")\n    else:\n        first, last = full_name.title().split(\" \")\n\n    full = f\"{first} {last}\"\n\n    return full, first, last\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/", "title": "common_nwbfile.py", "text": ""}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n        \"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name, new_file=True)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @classmethod\n    def _get_file_name(cls, nwb_file_name: str) -&gt; str:\n        \"\"\"Get valid nwb file name given substring.\"\"\"\n        query = cls &amp; f'nwb_file_name LIKE \"%{nwb_file_name}%\"'\n\n        if len(query) == 1:\n            return query.fetch1(\"nwb_file_name\")\n\n        raise ValueError(\n            f\"Found {len(query)} matches for {nwb_file_name} in Nwbfile table:\"\n            + f\" \\n{query}\"\n        )\n\n    @classmethod\n    def get_file_key(cls, nwb_file_name: str) -&gt; dict:\n        \"\"\"Return primary key using nwb_file_name substring.\"\"\"\n        return {\"nwb_file_name\": cls._get_file_name(nwb_file_name)}\n\n    @classmethod\n    def get_abs_path(cls, nwb_file_name, new_file=False) -&gt; str:\n        \"\"\"Return absolute path for a stored raw NWB file given file name.\n\n        The SPYGLASS_BASE_DIR must be set, either as an environment or part of\n        dj.config['custom']. See spyglass.settings.load_config\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile()\n            table. May be file substring. May include % wildcard(s).\n        new_file : bool, optional\n            Adding a new file to Nwbfile table. Defaults to False.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        if new_file:\n            return raw_dir + \"/\" + nwb_file_name\n\n        return raw_dir + \"/\" + cls._get_file_name(nwb_file_name)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n        \"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n        \"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n    \"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name, new_file=True)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.get_file_key", "title": "<code>get_file_key(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Return primary key using nwb_file_name substring.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_file_key(cls, nwb_file_name: str) -&gt; dict:\n    \"\"\"Return primary key using nwb_file_name substring.\"\"\"\n    return {\"nwb_file_name\": cls._get_file_name(nwb_file_name)}\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name, new_file=False)</code>  <code>classmethod</code>", "text": "<p>Return absolute path for a stored raw NWB file given file name.</p> <p>The SPYGLASS_BASE_DIR must be set, either as an environment or part of dj.config['custom']. See spyglass.settings.load_config</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() table. May be file substring. May include % wildcard(s).</p> required <code>new_file</code> <code>bool</code> <p>Adding a new file to Nwbfile table. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_abs_path(cls, nwb_file_name, new_file=False) -&gt; str:\n    \"\"\"Return absolute path for a stored raw NWB file given file name.\n\n    The SPYGLASS_BASE_DIR must be set, either as an environment or part of\n    dj.config['custom']. See spyglass.settings.load_config\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile()\n        table. May be file substring. May include % wildcard(s).\n    new_file : bool, optional\n        Adding a new file to Nwbfile table. Defaults to False.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    if new_file:\n        return raw_dir + \"/\" + nwb_file_name\n\n    return raw_dir + \"/\" + cls._get_file_name(nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n    \"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n    \"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n        \"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that\n        # string and redo if by some miracle it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n        \"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n        \"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n        \"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n        \"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n        \"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n        \"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n        \"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n        \"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n        \"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n    \"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n    \"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n    \"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n    \"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n    \"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n    \"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n    \"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n    \"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n    \"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n    \"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/", "title": "common_position.py", "text": ""}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.PositionInfoParameters", "title": "<code>PositionInfoParameters</code>", "text": "<p>             Bases: <code>Lookup</code></p> <p>Parameters for extracting the smoothed position, orientation and velocity.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass PositionInfoParameters(dj.Lookup):\n    \"\"\"\n    Parameters for extracting the smoothed position, orientation and velocity.\n    \"\"\"\n\n    definition = \"\"\"\n    position_info_param_name : varchar(80) # name for this set of parameters\n    ---\n    max_separation = 9.0  : float   # max distance (in cm) between head LEDs\n    max_speed = 300.0     : float   # max speed (in cm / s) of animal\n    position_smoothing_duration = 0.125 : float # size of moving window (s)\n    speed_smoothing_std_dev = 0.100 : float # smoothing standard deviation (s)\n    head_orient_smoothing_std_dev = 0.001 : float # smoothing std deviation (s)\n    led1_is_front = 1 : int # 1 if 1st LED is front LED, else 1st LED is back\n    is_upsampled = 0 : int # upsample the position to higher sampling rate\n    upsampling_sampling_rate = NULL : float # The rate to be upsampled to\n    upsampling_interpolation_method = linear : varchar(80) # see\n        # pandas.DataFrame.interpolation for list of methods\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalPositionInfoSelection", "title": "<code>IntervalPositionInfoSelection</code>", "text": "<p>             Bases: <code>Lookup</code></p> <p>Combines the parameters for position extraction and a time interval to extract the smoothed position on.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass IntervalPositionInfoSelection(dj.Lookup):\n    \"\"\"Combines the parameters for position extraction and a time interval to\n    extract the smoothed position on.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionInfoParameters\n    -&gt; IntervalList\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalPositionInfo", "title": "<code>IntervalPositionInfo</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Computes the smoothed head position, orientation and velocity for a given interval.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass IntervalPositionInfo(dj.Computed):\n    \"\"\"Computes the smoothed head position, orientation and velocity for a given\n    interval.\"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalPositionInfoSelection\n    ---\n    -&gt; AnalysisNwbfile\n    head_position_object_id : varchar(40)\n    head_orientation_object_id : varchar(40)\n    head_velocity_object_id : varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing position for: {key}\")\n\n        analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n\n        raw_position = RawPosition.PosObject &amp; key\n        spatial_series = raw_position.fetch_nwb()[0][\"raw_position\"]\n        spatial_df = raw_position.fetch1_dataframe()\n\n        position_info_parameters = (PositionInfoParameters() &amp; key).fetch1()\n\n        position_info = self.calculate_position_info(\n            spatial_df=spatial_df,\n            meters_to_pixels=spatial_series.conversion,\n            **position_info_parameters,\n        )\n\n        key.update(\n            dict(\n                analysis_file_name=analysis_file_name,\n                **self.generate_pos_components(\n                    spatial_series=spatial_series,\n                    position_info=position_info,\n                    analysis_fname=analysis_file_name,\n                ),\n            )\n        )\n\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n        self.insert1(key)\n\n    @staticmethod\n    def generate_pos_components(\n        spatial_series,\n        position_info,\n        analysis_fname,\n        prefix=\"head_\",\n        add_frame_ind=False,\n        video_frame_ind=None,\n    ):\n        \"\"\"Generate position, orientation and velocity components.\"\"\"\n        METERS_PER_CM = 0.01\n\n        position = pynwb.behavior.Position()\n        orientation = pynwb.behavior.CompassDirection()\n        velocity = pynwb.behavior.BehavioralTimeSeries()\n\n        # NOTE: CBroz1 removed a try/except ValueError that surrounded all\n        #       .create_X_series methods. dpeg22 could not recall purpose\n\n        time_comments = dict(\n            comments=spatial_series.comments,\n            timestamps=position_info[\"time\"],\n        )\n        time_comments_ref = dict(\n            **time_comments,\n            reference_frame=spatial_series.reference_frame,\n        )\n\n        # create nwb objects for insertion into analysis nwb file\n        position.create_spatial_series(\n            name=f\"{prefix}position\",\n            conversion=METERS_PER_CM,\n            data=position_info[\"position\"],\n            description=f\"{prefix}x_position, {prefix}y_position\",\n            **time_comments_ref,\n        )\n\n        orientation.create_spatial_series(\n            name=f\"{prefix}orientation\",\n            conversion=1.0,\n            data=position_info[\"orientation\"],\n            description=f\"{prefix}orientation\",\n            **time_comments_ref,\n        )\n\n        velocity.create_timeseries(\n            name=f\"{prefix}velocity\",\n            conversion=METERS_PER_CM,\n            unit=\"m/s\",\n            data=np.concatenate(\n                (\n                    position_info[\"velocity\"],\n                    position_info[\"speed\"][:, np.newaxis],\n                ),\n                axis=1,\n            ),\n            description=f\"{prefix}x_velocity, {prefix}y_velocity, \"\n            + f\"{prefix}speed\",\n            **time_comments,\n        )\n\n        if add_frame_ind:\n            if video_frame_ind is not None:\n                velocity.create_timeseries(\n                    name=\"video_frame_ind\",\n                    unit=\"index\",\n                    data=video_frame_ind.to_numpy(),\n                    description=\"video_frame_ind\",\n                    **time_comments,\n                )\n            else:\n                print(\n                    \"No video frame index found. Assuming all camera frames \"\n                    + \"are present.\"\n                )\n                velocity.create_timeseries(\n                    name=\"video_frame_ind\",\n                    unit=\"index\",\n                    data=np.arange(len(position_info[\"time\"])),\n                    description=\"video_frame_ind\",\n                    **time_comments,\n                )\n\n        # Insert into analysis nwb file\n        nwba = AnalysisNwbfile()\n\n        return {\n            f\"{prefix}position_object_id\": nwba.add_nwb_object(\n                analysis_fname, position\n            ),\n            f\"{prefix}orientation_object_id\": nwba.add_nwb_object(\n                analysis_fname, orientation\n            ),\n            f\"{prefix}velocity_object_id\": nwba.add_nwb_object(\n                analysis_fname, velocity\n            ),\n        }\n\n    @staticmethod\n    def calculate_position_info(\n        spatial_df: pd.DataFrame,\n        meters_to_pixels: float,\n        position_smoothing_duration,\n        led1_is_front,\n        is_upsampled,\n        upsampling_sampling_rate,\n        upsampling_interpolation_method,\n        orient_smoothing_std_dev=None,\n        speed_smoothing_std_dev=None,\n        max_LED_separation=None,\n        max_plausible_speed=None,\n        **kwargs,\n    ):\n        CM_TO_METERS = 100\n\n        if not orient_smoothing_std_dev:\n            orient_smoothing_std_dev = kwargs.get(\n                \"head_orient_smoothing_std_dev\"\n            )\n        if not speed_smoothing_std_dev:\n            speed_smoothing_std_dev = kwargs.get(\"head_speed_smoothing_std_dev\")\n        if not max_LED_separation:\n            max_LED_separation = kwargs.get(\"max_separation\")\n        if not max_plausible_speed:\n            max_plausible_speed = kwargs.get(\"max_speed\")\n        if not all(\n            [speed_smoothing_std_dev, max_LED_separation, max_plausible_speed]\n        ):\n            raise ValueError(\n                \"Missing required parameters:\\n\\t\"\n                + f\"speed_smoothing_std_dev: {speed_smoothing_std_dev}\\n\\t\"\n                + f\"max_LED_separation: {max_LED_separation}\\n\\t\"\n                + f\"max_plausible_speed: {max_plausible_speed}\"\n            )\n\n        # Accepts x/y 'loc' or 'loc1' format for first pos. Renames to 'loc'\n        DEFAULT_COLS = [\"xloc\", \"yloc\", \"xloc2\", \"yloc2\", \"xloc1\", \"yloc1\"]\n        ALTERNATIVE_COLS = [\"xloc1\", \"xloc2\", \"yloc1\", \"yloc2\"]\n\n        if all([c in spatial_df.columns for c in DEFAULT_COLS[:4]]):\n            # move the 4 position columns to front, continue\n            spatial_df = spatial_df[DEFAULT_COLS[:4]]\n        elif all([c in spatial_df.columns for c in ALTERNATIVE_COLS]):\n            # move the 4 position columns to front, rename to default, continue\n            spatial_df = spatial_df[ALTERNATIVE_COLS]\n            spatial_df.columns = DEFAULT_COLS[:4]\n        else:\n            cols = list(spatial_df.columns)\n            if len(cols) != 4 or not all([c in DEFAULT_COLS for c in cols]):\n                choice = dj.utils.user_choice(\n                    \"Unexpected columns in raw position. Assume \"\n                    + f\"{DEFAULT_COLS[:4]}?\\n{spatial_df}\\n\"\n                )\n                if choice.lower() not in [\"yes\", \"y\"]:\n                    raise ValueError(\n                        f\"Unexpected columns in raw position: {cols}\"\n                    )\n            # rename first 4 columns, keep rest. Rest dropped below\n            spatial_df.columns = DEFAULT_COLS[:4] + cols[4:]\n        # Get spatial series properties\n        time = np.asarray(spatial_df.index)  # seconds\n        position = np.asarray(spatial_df.iloc[:, :4])  # meters\n\n        # remove NaN times\n        is_nan_time = np.isnan(time)\n        position = position[~is_nan_time]\n        time = time[~is_nan_time]\n\n        dt = np.median(np.diff(time))\n        sampling_rate = 1 / dt\n\n        # Define LEDs\n        if led1_is_front:\n            front_LED = position[:, [0, 1]].astype(float)\n            back_LED = position[:, [2, 3]].astype(float)\n        else:\n            back_LED = position[:, [0, 1]].astype(float)\n            front_LED = position[:, [2, 3]].astype(float)\n\n        # Convert to cm\n        back_LED *= meters_to_pixels * CM_TO_METERS\n        front_LED *= meters_to_pixels * CM_TO_METERS\n\n        # Set points to NaN where the front and back LEDs are too separated\n        dist_between_LEDs = get_distance(back_LED, front_LED)\n        is_too_separated = dist_between_LEDs &gt;= max_LED_separation\n\n        back_LED[is_too_separated] = np.nan\n        front_LED[is_too_separated] = np.nan\n\n        # Calculate speed\n        front_LED_speed = get_speed(\n            front_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n        back_LED_speed = get_speed(\n            back_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n\n        # Set to points to NaN where the speed is too fast\n        is_too_fast = (front_LED_speed &gt; max_plausible_speed) | (\n            back_LED_speed &gt; max_plausible_speed\n        )\n        back_LED[is_too_fast] = np.nan\n        front_LED[is_too_fast] = np.nan\n\n        # Interpolate the NaN points\n        back_LED = interpolate_nan(back_LED)\n        front_LED = interpolate_nan(front_LED)\n\n        # Smooth\n        moving_average_window = int(position_smoothing_duration * sampling_rate)\n        back_LED = bottleneck.move_mean(\n            back_LED, window=moving_average_window, axis=0, min_count=1\n        )\n        front_LED = bottleneck.move_mean(\n            front_LED, window=moving_average_window, axis=0, min_count=1\n        )\n\n        if is_upsampled:\n            position_df = pd.DataFrame(\n                {\n                    \"time\": time,\n                    \"back_LED_x\": back_LED[:, 0],\n                    \"back_LED_y\": back_LED[:, 1],\n                    \"front_LED_x\": front_LED[:, 0],\n                    \"front_LED_y\": front_LED[:, 1],\n                }\n            ).set_index(\"time\")\n\n            upsampling_start_time = time[0]\n            upsampling_end_time = time[-1]\n\n            n_samples = (\n                int(\n                    np.ceil(\n                        (upsampling_end_time - upsampling_start_time)\n                        * upsampling_sampling_rate\n                    )\n                )\n                + 1\n            )\n            new_time = np.linspace(\n                upsampling_start_time, upsampling_end_time, n_samples\n            )\n            new_index = pd.Index(\n                np.unique(np.concatenate((position_df.index, new_time))),\n                name=\"time\",\n            )\n            position_df = (\n                position_df.reindex(index=new_index)\n                .interpolate(method=upsampling_interpolation_method)\n                .reindex(index=new_time)\n            )\n\n            time = np.asarray(position_df.index)\n            back_LED = np.asarray(\n                position_df.loc[:, [\"back_LED_x\", \"back_LED_y\"]]\n            )\n            front_LED = np.asarray(\n                position_df.loc[:, [\"front_LED_x\", \"front_LED_y\"]]\n            )\n\n            sampling_rate = upsampling_sampling_rate\n\n        # Calculate position, orientation, velocity, speed\n        position = get_centriod(back_LED, front_LED)  # cm\n\n        orientation = get_angle(back_LED, front_LED)  # radians\n        is_nan = np.isnan(orientation)\n\n        # Unwrap orientation before smoothing\n        orientation[~is_nan] = np.unwrap(orientation[~is_nan])\n        orientation[~is_nan] = gaussian_smooth(\n            orientation[~is_nan],\n            orient_smoothing_std_dev,\n            sampling_rate,\n            axis=0,\n            truncate=8,\n        )\n        # convert back to between -pi and pi\n        orientation[~is_nan] = np.angle(np.exp(1j * orientation[~is_nan]))\n\n        velocity = get_velocity(\n            position,\n            time=time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )  # cm/s\n        speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n\n        return {\n            \"time\": time,\n            \"position\": position,\n            \"orientation\": orientation,\n            \"velocity\": velocity,\n            \"speed\": speed,\n        }\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self._data_to_df(self.fetch_nwb()[0])\n\n    @staticmethod\n    def _data_to_df(data, prefix=\"head_\", add_frame_ind=False):\n        pos, ori, vel = [\n            prefix + c for c in [\"position\", \"orientation\", \"velocity\"]\n        ]\n\n        COLUMNS = [\n            f\"{pos}_x\",\n            f\"{pos}_y\",\n            ori,\n            f\"{vel}_x\",\n            f\"{vel}_y\",\n            f\"{prefix}speed\",\n        ]\n\n        df = pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(data[pos].get_spatial_series().data),\n                    np.asarray(data[ori].get_spatial_series().data)[\n                        :, np.newaxis\n                    ],\n                    np.asarray(data[vel].time_series[vel].data),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=pd.Index(\n                np.asarray(data[pos].get_spatial_series().timestamps),\n                name=\"time\",\n            ),\n        )\n\n        if add_frame_ind:\n            df.insert(\n                0,\n                \"video_frame_ind\",\n                np.asarray(\n                    data[vel].time_series[\"video_frame_ind\"].data,\n                    dtype=int,\n                ),\n            )\n\n        return df\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalPositionInfo.generate_pos_components", "title": "<code>generate_pos_components(spatial_series, position_info, analysis_fname, prefix='head_', add_frame_ind=False, video_frame_ind=None)</code>  <code>staticmethod</code>", "text": "<p>Generate position, orientation and velocity components.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@staticmethod\ndef generate_pos_components(\n    spatial_series,\n    position_info,\n    analysis_fname,\n    prefix=\"head_\",\n    add_frame_ind=False,\n    video_frame_ind=None,\n):\n    \"\"\"Generate position, orientation and velocity components.\"\"\"\n    METERS_PER_CM = 0.01\n\n    position = pynwb.behavior.Position()\n    orientation = pynwb.behavior.CompassDirection()\n    velocity = pynwb.behavior.BehavioralTimeSeries()\n\n    # NOTE: CBroz1 removed a try/except ValueError that surrounded all\n    #       .create_X_series methods. dpeg22 could not recall purpose\n\n    time_comments = dict(\n        comments=spatial_series.comments,\n        timestamps=position_info[\"time\"],\n    )\n    time_comments_ref = dict(\n        **time_comments,\n        reference_frame=spatial_series.reference_frame,\n    )\n\n    # create nwb objects for insertion into analysis nwb file\n    position.create_spatial_series(\n        name=f\"{prefix}position\",\n        conversion=METERS_PER_CM,\n        data=position_info[\"position\"],\n        description=f\"{prefix}x_position, {prefix}y_position\",\n        **time_comments_ref,\n    )\n\n    orientation.create_spatial_series(\n        name=f\"{prefix}orientation\",\n        conversion=1.0,\n        data=position_info[\"orientation\"],\n        description=f\"{prefix}orientation\",\n        **time_comments_ref,\n    )\n\n    velocity.create_timeseries(\n        name=f\"{prefix}velocity\",\n        conversion=METERS_PER_CM,\n        unit=\"m/s\",\n        data=np.concatenate(\n            (\n                position_info[\"velocity\"],\n                position_info[\"speed\"][:, np.newaxis],\n            ),\n            axis=1,\n        ),\n        description=f\"{prefix}x_velocity, {prefix}y_velocity, \"\n        + f\"{prefix}speed\",\n        **time_comments,\n    )\n\n    if add_frame_ind:\n        if video_frame_ind is not None:\n            velocity.create_timeseries(\n                name=\"video_frame_ind\",\n                unit=\"index\",\n                data=video_frame_ind.to_numpy(),\n                description=\"video_frame_ind\",\n                **time_comments,\n            )\n        else:\n            print(\n                \"No video frame index found. Assuming all camera frames \"\n                + \"are present.\"\n            )\n            velocity.create_timeseries(\n                name=\"video_frame_ind\",\n                unit=\"index\",\n                data=np.arange(len(position_info[\"time\"])),\n                description=\"video_frame_ind\",\n                **time_comments,\n            )\n\n    # Insert into analysis nwb file\n    nwba = AnalysisNwbfile()\n\n    return {\n        f\"{prefix}position_object_id\": nwba.add_nwb_object(\n            analysis_fname, position\n        ),\n        f\"{prefix}orientation_object_id\": nwba.add_nwb_object(\n            analysis_fname, orientation\n        ),\n        f\"{prefix}velocity_object_id\": nwba.add_nwb_object(\n            analysis_fname, velocity\n        ),\n    }\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.LinearizationParameters", "title": "<code>LinearizationParameters</code>", "text": "<p>             Bases: <code>Lookup</code></p> <p>Choose whether to use an HMM to linearize position.</p> <p>This can help when the euclidean distances between separate arms are too close and the previous position has some information about which arm the animal is on.</p> <p>route_euclidean_distance_scaling: How much to prefer route distances between successive time points that are closer to the euclidean distance. Smaller numbers mean the route distance is more likely to be close to the euclidean distance.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass LinearizationParameters(dj.Lookup):\n    \"\"\"Choose whether to use an HMM to linearize position.\n\n    This can help when the euclidean distances between separate arms are too\n    close and the previous position has some information about which arm the\n    animal is on.\n\n    route_euclidean_distance_scaling: How much to prefer route distances between\n    successive time points that are closer to the euclidean distance. Smaller\n    numbers mean the route distance is more likely to be close to the euclidean\n    distance.\n    \"\"\"\n\n    definition = \"\"\"\n    linearization_param_name : varchar(80)   # name for this set of parameters\n    ---\n    use_hmm = 0 : int   # use HMM to determine linearization\n    route_euclidean_distance_scaling = 1.0 : float # Preference for euclidean.\n    sensor_std_dev = 5.0 : float   # Uncertainty of position sensor (in cm).\n    # Biases the transition matrix to prefer the current track segment.\n    diagonal_bias = 0.5 : float\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.TrackGraph", "title": "<code>TrackGraph</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Graph representation of track representing the spatial environment.</p> <p>Used for linearizing position.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass TrackGraph(dj.Manual):\n    \"\"\"Graph representation of track representing the spatial environment.\n\n    Used for linearizing position.\n    \"\"\"\n\n    definition = \"\"\"\n    track_graph_name : varchar(80)\n    ----\n    environment : varchar(80)  # Type of Environment\n    node_positions : blob      # 2D position of nodes, (n_nodes, 2)\n    edges: blob                # shape (n_edges, 2)\n    linear_edge_order : blob   # order of edges in linear space, (n_edges, 2)\n    linear_edge_spacing : blob # space btwn edges in linear space, (n_edges,)\n    \"\"\"\n\n    def get_networkx_track_graph(self, track_graph_parameters=None):\n        if track_graph_parameters is None:\n            track_graph_parameters = self.fetch1()\n        return make_track_graph(\n            node_positions=track_graph_parameters[\"node_positions\"],\n            edges=track_graph_parameters[\"edges\"],\n        )\n\n    def plot_track_graph(self, ax=None, draw_edge_labels=False, **kwds):\n        \"\"\"Plot the track graph in 2D position space.\"\"\"\n        track_graph = self.get_networkx_track_graph()\n        plot_track_graph(\n            track_graph, ax=ax, draw_edge_labels=draw_edge_labels, **kwds\n        )\n\n    def plot_track_graph_as_1D(\n        self,\n        ax=None,\n        axis=\"x\",\n        other_axis_start=0.0,\n        draw_edge_labels=False,\n        node_size=300,\n        node_color=\"#1f77b4\",\n    ):\n        \"\"\"Plot the track graph in 1D to see how the linearization is set up.\"\"\"\n        track_graph_parameters = self.fetch1()\n        track_graph = self.get_networkx_track_graph(\n            track_graph_parameters=track_graph_parameters\n        )\n        plot_graph_as_1D(\n            track_graph,\n            edge_order=track_graph_parameters[\"linear_edge_order\"],\n            edge_spacing=track_graph_parameters[\"linear_edge_spacing\"],\n            ax=ax,\n            axis=axis,\n            other_axis_start=other_axis_start,\n            draw_edge_labels=draw_edge_labels,\n            node_size=node_size,\n            node_color=node_color,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.TrackGraph.plot_track_graph", "title": "<code>plot_track_graph(ax=None, draw_edge_labels=False, **kwds)</code>", "text": "<p>Plot the track graph in 2D position space.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>def plot_track_graph(self, ax=None, draw_edge_labels=False, **kwds):\n    \"\"\"Plot the track graph in 2D position space.\"\"\"\n    track_graph = self.get_networkx_track_graph()\n    plot_track_graph(\n        track_graph, ax=ax, draw_edge_labels=draw_edge_labels, **kwds\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.TrackGraph.plot_track_graph_as_1D", "title": "<code>plot_track_graph_as_1D(ax=None, axis='x', other_axis_start=0.0, draw_edge_labels=False, node_size=300, node_color='#1f77b4')</code>", "text": "<p>Plot the track graph in 1D to see how the linearization is set up.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>def plot_track_graph_as_1D(\n    self,\n    ax=None,\n    axis=\"x\",\n    other_axis_start=0.0,\n    draw_edge_labels=False,\n    node_size=300,\n    node_color=\"#1f77b4\",\n):\n    \"\"\"Plot the track graph in 1D to see how the linearization is set up.\"\"\"\n    track_graph_parameters = self.fetch1()\n    track_graph = self.get_networkx_track_graph(\n        track_graph_parameters=track_graph_parameters\n    )\n    plot_graph_as_1D(\n        track_graph,\n        edge_order=track_graph_parameters[\"linear_edge_order\"],\n        edge_spacing=track_graph_parameters[\"linear_edge_spacing\"],\n        ax=ax,\n        axis=axis,\n        other_axis_start=other_axis_start,\n        draw_edge_labels=draw_edge_labels,\n        node_size=node_size,\n        node_color=node_color,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalLinearizedPosition", "title": "<code>IntervalLinearizedPosition</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Linearized position for a given interval</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass IntervalLinearizedPosition(dj.Computed):\n    \"\"\"Linearized position for a given interval\"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalLinearizationSelection\n    ---\n    -&gt; AnalysisNwbfile\n    linearized_position_object_id : varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing linear position for: {key}\")\n\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n\n        position_nwb = (\n            IntervalPositionInfo\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n                \"position_info_param_name\": key[\"position_info_param_name\"],\n            }\n        ).fetch_nwb()[0]\n\n        position = np.asarray(\n            position_nwb[\"head_position\"].get_spatial_series().data\n        )\n        time = np.asarray(\n            position_nwb[\"head_position\"].get_spatial_series().timestamps\n        )\n\n        linearization_parameters = (\n            LinearizationParameters()\n            &amp; {\"linearization_param_name\": key[\"linearization_param_name\"]}\n        ).fetch1()\n        track_graph_info = (\n            TrackGraph() &amp; {\"track_graph_name\": key[\"track_graph_name\"]}\n        ).fetch1()\n\n        track_graph = make_track_graph(\n            node_positions=track_graph_info[\"node_positions\"],\n            edges=track_graph_info[\"edges\"],\n        )\n\n        linear_position_df = get_linearized_position(\n            position=position,\n            track_graph=track_graph,\n            edge_spacing=track_graph_info[\"linear_edge_spacing\"],\n            edge_order=track_graph_info[\"linear_edge_order\"],\n            use_HMM=linearization_parameters[\"use_hmm\"],\n            route_euclidean_distance_scaling=linearization_parameters[\n                \"route_euclidean_distance_scaling\"\n            ],\n            sensor_std_dev=linearization_parameters[\"sensor_std_dev\"],\n            diagonal_bias=linearization_parameters[\"diagonal_bias\"],\n        )\n\n        linear_position_df[\"time\"] = time\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n\n        key[\"linearized_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=linear_position_df,\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_nwb()[0][\"linearized_position\"].set_index(\"time\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.NodePicker", "title": "<code>NodePicker</code>", "text": "<p>Interactive creation of track graph by looking at video frames.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>class NodePicker:\n    \"\"\"Interactive creation of track graph by looking at video frames.\"\"\"\n\n    def __init__(\n        self, ax=None, video_filename=None, node_color=\"#1f78b4\", node_size=100\n    ):\n        if ax is None:\n            ax = plt.gca()\n        self.ax = ax\n        self.canvas = ax.get_figure().canvas\n        self.cid = None\n        self._nodes = []\n        self.node_color = node_color\n        self._nodes_plot = ax.scatter(\n            [], [], zorder=5, s=node_size, color=node_color\n        )\n        self.edges = [[]]\n        self.video_filename = video_filename\n\n        if video_filename is not None:\n            self.video = cv2.VideoCapture(video_filename)\n            frame = self.get_video_frame()\n            ax.imshow(frame, picker=True)\n            ax.set_title(\n                \"Left click to place node.\\nRight click to remove node.\"\n                \"\\nShift+Left click to clear nodes.\"\n                \"\\nCntrl+Left click two nodes to place an edge\"\n            )\n\n        self.connect()\n\n    @property\n    def node_positions(self):\n        return np.asarray(self._nodes)\n\n    def connect(self):\n        if self.cid is None:\n            self.cid = self.canvas.mpl_connect(\n                \"button_press_event\", self.click_event\n            )\n\n    def disconnect(self):\n        if self.cid is not None:\n            self.canvas.mpl_disconnect(self.cid)\n            self.cid = None\n\n    def click_event(self, event):\n        if not event.inaxes:\n            return\n        if (event.key not in [\"control\", \"shift\"]) &amp; (\n            event.button == 1\n        ):  # left click\n            self._nodes.append((event.xdata, event.ydata))\n        if (event.key not in [\"control\", \"shift\"]) &amp; (\n            event.button == 3\n        ):  # right click\n            self.remove_point((event.xdata, event.ydata))\n        if (event.key == \"shift\") &amp; (event.button == 1):\n            self.clear()\n        if (event.key == \"control\") &amp; (event.button == 1):\n            point = (event.xdata, event.ydata)\n            distance_to_nodes = np.linalg.norm(\n                self.node_positions - point, axis=1\n            )\n            closest_node_ind = np.argmin(distance_to_nodes)\n            if len(self.edges[-1]) &lt; 2:\n                self.edges[-1].append(closest_node_ind)\n            else:\n                self.edges.append([closest_node_ind])\n\n        self.redraw()\n\n    def redraw(self):\n        # Draw Node Circles\n        if len(self.node_positions) &gt; 0:\n            self._nodes_plot.set_offsets(self.node_positions)\n        else:\n            self._nodes_plot.set_offsets([])\n\n        # Draw Node Numbers\n        self.ax.texts = []\n        for ind, (x, y) in enumerate(self.node_positions):\n            self.ax.text(\n                x,\n                y,\n                ind,\n                zorder=6,\n                fontsize=12,\n                horizontalalignment=\"center\",\n                verticalalignment=\"center\",\n                clip_on=True,\n                bbox=None,\n                transform=self.ax.transData,\n            )\n        # Draw Edges\n        self.ax.lines = []  # clears the existing lines\n        for edge in self.edges:\n            if len(edge) &gt; 1:\n                x1, y1 = self.node_positions[edge[0]]\n                x2, y2 = self.node_positions[edge[1]]\n                self.ax.plot(\n                    [x1, x2], [y1, y2], color=self.node_color, linewidth=2\n                )\n\n        self.canvas.draw_idle()\n\n    def remove_point(self, point):\n        if len(self._nodes) &gt; 0:\n            distance_to_nodes = np.linalg.norm(\n                self.node_positions - point, axis=1\n            )\n            closest_node_ind = np.argmin(distance_to_nodes)\n            self._nodes.pop(closest_node_ind)\n\n    def clear(self):\n        self._nodes = []\n        self.edges = [[]]\n        self.redraw()\n\n    def get_video_frame(self):\n        is_grabbed, frame = self.video.read()\n        if is_grabbed:\n            return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.PositionVideo", "title": "<code>PositionVideo</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass PositionVideo(dj.Computed):\n    \"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalPositionInfo\n    ---\n    \"\"\"\n\n    def make(self, key):\n        M_TO_CM = 100\n\n        print(\"Loading position data...\")\n        raw_position_df = (\n            RawPosition()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch1_dataframe()\n        position_info_df = (\n            IntervalPositionInfo()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n                \"position_info_param_name\": key[\"position_info_param_name\"],\n            }\n        ).fetch1_dataframe()\n\n        print(\"Loading video data...\")\n        epoch = (\n            int(\n                key[\"interval_list_name\"]\n                .replace(\"pos \", \"\")\n                .replace(\" valid times\", \"\")\n            )\n            + 1\n        )\n        video_info = (\n            VideoFile()\n            &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch}\n        ).fetch1()\n        io = pynwb.NWBHDF5IO(raw_dir() + video_info[\"nwb_file_name\"], \"r\")\n        nwb_file = io.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filename = nwb_video.external_file.value[0]\n\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        output_video_filename = (\n            f\"{nwb_base_filename}_{epoch:02d}_\"\n            f'{key[\"position_info_param_name\"]}.mp4'\n        )\n\n        centroids = {\n            \"red\": np.asarray(raw_position_df[[\"xloc\", \"yloc\"]]),\n            \"green\": np.asarray(raw_position_df[[\"xloc2\", \"yloc2\"]]),\n        }\n        head_position_mean = np.asarray(\n            position_info_df[[\"head_position_x\", \"head_position_y\"]]\n        )\n        head_orientation_mean = np.asarray(\n            position_info_df[[\"head_orientation\"]]\n        )\n        video_time = np.asarray(nwb_video.timestamps)\n        position_time = np.asarray(position_info_df.index)\n        cm_per_pixel = nwb_video.device.meters_per_pixel * M_TO_CM\n\n        print(\"Making video...\")\n        self.make_video(\n            video_filename,\n            centroids,\n            head_position_mean,\n            head_orientation_mean,\n            video_time,\n            position_time,\n            output_video_filename=output_video_filename,\n            cm_to_pixels=cm_per_pixel,\n            disable_progressbar=False,\n        )\n\n    @staticmethod\n    def convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n        \"\"\"Converts from cm to pixels and flips the y-axis.\n        Parameters\n        ----------\n        data : ndarray, shape (n_time, 2)\n        frame_size : array_like, shape (2,)\n        cm_to_pixels : float\n\n        Returns\n        -------\n        converted_data : ndarray, shape (n_time, 2)\n        \"\"\"\n        return data / cm_to_pixels\n\n    @staticmethod\n    def fill_nan(variable, video_time, variable_time):\n        video_ind = np.digitize(variable_time, video_time[1:])\n\n        n_video_time = len(video_time)\n        try:\n            n_variable_dims = variable.shape[1]\n            filled_variable = np.full((n_video_time, n_variable_dims), np.nan)\n        except IndexError:\n            filled_variable = np.full((n_video_time,), np.nan)\n        filled_variable[video_ind] = variable\n\n        return filled_variable\n\n    def make_video(\n        self,\n        video_filename,\n        centroids,\n        head_position_mean,\n        head_orientation_mean,\n        video_time,\n        position_time,\n        output_video_filename=\"output.mp4\",\n        cm_to_pixels=1.0,\n        disable_progressbar=False,\n        arrow_radius=15,\n        circle_radius=8,\n    ):\n        RGB_PINK = (234, 82, 111)\n        RGB_YELLOW = (253, 231, 76)\n        RGB_WHITE = (255, 255, 255)\n\n        video = cv2.VideoCapture(video_filename)\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        frame_size = (int(video.get(3)), int(video.get(4)))\n        frame_rate = video.get(5)\n        n_frames = int(head_orientation_mean.shape[0])\n\n        out = cv2.VideoWriter(\n            output_video_filename, fourcc, frame_rate, frame_size, True\n        )\n\n        centroids = {\n            color: self.fill_nan(data, video_time, position_time)\n            for color, data in centroids.items()\n        }\n        head_position_mean = self.fill_nan(\n            head_position_mean, video_time, position_time\n        )\n        head_orientation_mean = self.fill_nan(\n            head_orientation_mean, video_time, position_time\n        )\n\n        for time_ind in tqdm(\n            range(n_frames - 1), desc=\"frames\", disable=disable_progressbar\n        ):\n            is_grabbed, frame = video.read()\n            if is_grabbed:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                red_centroid = centroids[\"red\"][time_ind]\n                green_centroid = centroids[\"green\"][time_ind]\n\n                head_position = head_position_mean[time_ind]\n                head_position = self.convert_to_pixels(\n                    head_position, frame_size, cm_to_pixels\n                )\n                head_orientation = head_orientation_mean[time_ind]\n\n                if np.all(~np.isnan(red_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(red_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_YELLOW,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(green_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(green_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_PINK,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(head_position)) &amp; np.all(\n                    ~np.isnan(head_orientation)\n                ):\n                    arrow_tip = (\n                        int(\n                            head_position[0]\n                            + arrow_radius * np.cos(head_orientation)\n                        ),\n                        int(\n                            head_position[1]\n                            + arrow_radius * np.sin(head_orientation)\n                        ),\n                    )\n                    cv2.arrowedLine(\n                        img=frame,\n                        pt1=tuple(head_position.astype(int)),\n                        pt2=arrow_tip,\n                        color=RGB_WHITE,\n                        thickness=4,\n                        line_type=8,\n                        shift=cv2.CV_8U,\n                        tipLength=0.25,\n                    )\n\n                if np.all(~np.isnan(head_position)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(head_position.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_WHITE,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n                out.write(frame)\n            else:\n                break\n\n        video.release()\n        out.release()\n        cv2.destroyAllWindows()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.PositionVideo.convert_to_pixels", "title": "<code>convert_to_pixels(data, frame_size, cm_to_pixels=1.0)</code>  <code>staticmethod</code>", "text": "<p>Converts from cm to pixels and flips the y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>(ndarray, shape(n_time, 2))</code> required <code>frame_size</code> <code>(array_like, shape(2))</code> required <code>cm_to_pixels</code> <code>float</code> <code>1.0</code> <p>Returns:</p> Name Type Description <code>converted_data</code> <code>(ndarray, shape(n_time, 2))</code> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@staticmethod\ndef convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n    \"\"\"Converts from cm to pixels and flips the y-axis.\n    Parameters\n    ----------\n    data : ndarray, shape (n_time, 2)\n    frame_size : array_like, shape (2,)\n    cm_to_pixels : float\n\n    Returns\n    -------\n    converted_data : ndarray, shape (n_time, 2)\n    \"\"\"\n    return data / cm_to_pixels\n</code></pre>"}, {"location": "api/src/spyglass/common/common_region/", "title": "common_region.py", "text": ""}, {"location": "api/src/spyglass/common/common_region/#src.spyglass.common.common_region.BrainRegion", "title": "<code>BrainRegion</code>", "text": "<p>             Bases: <code>Lookup</code></p> Source code in <code>src/spyglass/common/common_region.py</code> <pre><code>@schema\nclass BrainRegion(dj.Lookup):\n    definition = \"\"\"\n    region_id: smallint auto_increment\n    ---\n    region_name: varchar(200)             # the name of the brain region\n    subregion_name=NULL: varchar(200)     # subregion name\n    subsubregion_name=NULL: varchar(200)  # subregion within subregion\n    \"\"\"\n\n    # TODO consider making (region_name, subregion_name, subsubregion_name) a\n    # primary key subregion_name='' and subsubregion_name='' will be necessary\n    # but that seems OK\n\n    @classmethod\n    def fetch_add(\n        cls, region_name, subregion_name=None, subsubregion_name=None\n    ):\n        \"\"\"Return the region ID for names. If no match, add to the BrainRegion.\n\n        The combination of (region_name, subregion_name, subsubregion_name) is\n        effectively unique, then.\n\n        Parameters\n        ----------\n        region_name : str\n            The name of the brain region.\n        subregion_name : str, optional\n            The name of the subregion within the brain region.\n        subsubregion_name : str, optional\n            The name of the subregion within the subregion.\n\n        Returns\n        -------\n        region_id : int\n            The index of the region in the BrainRegion table.\n        \"\"\"\n        key = dict(\n            region_name=region_name,\n            subregion_name=subregion_name,\n            subsubregion_name=subsubregion_name,\n        )\n        query = BrainRegion &amp; key\n        if not query:\n            cls.insert1(key)\n            query = BrainRegion &amp; key\n        return query.fetch1(\"region_id\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_region/#src.spyglass.common.common_region.BrainRegion.fetch_add", "title": "<code>fetch_add(region_name, subregion_name=None, subsubregion_name=None)</code>  <code>classmethod</code>", "text": "<p>Return the region ID for names. If no match, add to the BrainRegion.</p> <p>The combination of (region_name, subregion_name, subsubregion_name) is effectively unique, then.</p> <p>Parameters:</p> Name Type Description Default <code>region_name</code> <code>str</code> <p>The name of the brain region.</p> required <code>subregion_name</code> <code>str</code> <p>The name of the subregion within the brain region.</p> <code>None</code> <code>subsubregion_name</code> <code>str</code> <p>The name of the subregion within the subregion.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>region_id</code> <code>int</code> <p>The index of the region in the BrainRegion table.</p> Source code in <code>src/spyglass/common/common_region.py</code> <pre><code>@classmethod\ndef fetch_add(\n    cls, region_name, subregion_name=None, subsubregion_name=None\n):\n    \"\"\"Return the region ID for names. If no match, add to the BrainRegion.\n\n    The combination of (region_name, subregion_name, subsubregion_name) is\n    effectively unique, then.\n\n    Parameters\n    ----------\n    region_name : str\n        The name of the brain region.\n    subregion_name : str, optional\n        The name of the subregion within the brain region.\n    subsubregion_name : str, optional\n        The name of the subregion within the subregion.\n\n    Returns\n    -------\n    region_id : int\n        The index of the region in the BrainRegion table.\n    \"\"\"\n    key = dict(\n        region_name=region_name,\n        subregion_name=subregion_name,\n        subsubregion_name=subsubregion_name,\n    )\n    query = BrainRegion &amp; key\n    if not query:\n        cls.insert1(key)\n        query = BrainRegion &amp; key\n    return query.fetch1(\"region_id\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/", "title": "common_ripple.py", "text": ""}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleLFPSelection", "title": "<code>RippleLFPSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@schema\nclass RippleLFPSelection(dj.Manual):\n    definition = \"\"\"\n     -&gt; LFPBand\n     group_name = 'CA1' : varchar(80)\n     \"\"\"\n\n    class RippleLFPElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; RippleLFPSelection\n        -&gt; LFPBandSelection.LFPBandElectrode\n        \"\"\"\n\n    def insert1(self, key, **kwargs):\n        filter_name = (LFPBand &amp; key).fetch1(\"filter_name\")\n        if \"ripple\" not in filter_name.lower():\n            raise UserWarning(\"Please use a ripple filter\")\n        super().insert1(key, **kwargs)\n\n    @staticmethod\n    def set_lfp_electrodes(\n        key,\n        electrode_list=None,\n        group_name=\"CA1\",\n        **kwargs,\n    ):\n        \"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n        Parameters\n        ----------\n        key : dict\n            dictionary corresponding to the LFPBand entry to use for ripple detection\n        electrode_list : list\n            list of electrodes from LFPBandSelection.LFPBandElectrode\n            to be used as the ripple LFP during detection\n        group_name : str, optional\n            description of the electrode group, by default \"CA1\"\n        \"\"\"\n\n        RippleLFPSelection().insert1(\n            {**key, \"group_name\": group_name},\n            skip_duplicates=True,\n            **kwargs,\n        )\n        if not electrode_list:\n            electrode_list = (\n                (LFPBandSelection.LFPBandElectrode() &amp; key)\n                .fetch(\"electrode_id\")\n                .tolist()\n            )\n        electrode_list.sort()\n        electrode_keys = (\n            pd.DataFrame(LFPBandSelection.LFPBandElectrode() &amp; key)\n            .set_index(\"electrode_id\")\n            .loc[electrode_list]\n            .reset_index()\n            .loc[:, LFPBandSelection.LFPBandElectrode.primary_key]\n        )\n        electrode_keys[\"group_name\"] = group_name\n        electrode_keys = electrode_keys.sort_values(by=[\"electrode_id\"])\n        RippleLFPSelection().RippleLFPElectrode.insert(\n            electrode_keys.to_dict(orient=\"records\"),\n            replace=True,\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleLFPSelection.set_lfp_electrodes", "title": "<code>set_lfp_electrodes(key, electrode_list=None, group_name='CA1', **kwargs)</code>  <code>staticmethod</code>", "text": "<p>Removes all electrodes for the specified nwb file and then adds back the electrodes in the list</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary corresponding to the LFPBand entry to use for ripple detection</p> required <code>electrode_list</code> <code>list</code> <p>list of electrodes from LFPBandSelection.LFPBandElectrode to be used as the ripple LFP during detection</p> <code>None</code> <code>group_name</code> <code>str</code> <p>description of the electrode group, by default \"CA1\"</p> <code>'CA1'</code> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@staticmethod\ndef set_lfp_electrodes(\n    key,\n    electrode_list=None,\n    group_name=\"CA1\",\n    **kwargs,\n):\n    \"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n    Parameters\n    ----------\n    key : dict\n        dictionary corresponding to the LFPBand entry to use for ripple detection\n    electrode_list : list\n        list of electrodes from LFPBandSelection.LFPBandElectrode\n        to be used as the ripple LFP during detection\n    group_name : str, optional\n        description of the electrode group, by default \"CA1\"\n    \"\"\"\n\n    RippleLFPSelection().insert1(\n        {**key, \"group_name\": group_name},\n        skip_duplicates=True,\n        **kwargs,\n    )\n    if not electrode_list:\n        electrode_list = (\n            (LFPBandSelection.LFPBandElectrode() &amp; key)\n            .fetch(\"electrode_id\")\n            .tolist()\n        )\n    electrode_list.sort()\n    electrode_keys = (\n        pd.DataFrame(LFPBandSelection.LFPBandElectrode() &amp; key)\n        .set_index(\"electrode_id\")\n        .loc[electrode_list]\n        .reset_index()\n        .loc[:, LFPBandSelection.LFPBandElectrode.primary_key]\n    )\n    electrode_keys[\"group_name\"] = group_name\n    electrode_keys = electrode_keys.sort_values(by=[\"electrode_id\"])\n    RippleLFPSelection().RippleLFPElectrode.insert(\n        electrode_keys.to_dict(orient=\"records\"),\n        replace=True,\n        **kwargs,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleParameters", "title": "<code>RippleParameters</code>", "text": "<p>             Bases: <code>Lookup</code></p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@schema\nclass RippleParameters(dj.Lookup):\n    definition = \"\"\"\n    ripple_param_name : varchar(80) # a name for this set of parameters\n    ----\n    ripple_param_dict : BLOB    # dictionary of parameters\n    \"\"\"\n\n    def insert_default(self):\n        \"\"\"Insert the default parameter set\"\"\"\n        default_dict = {\n            \"speed_name\": \"head_speed\",\n            \"ripple_detection_algorithm\": \"Kay_ripple_detector\",\n            \"ripple_detection_params\": dict(\n                speed_threshold=4.0,  # cm/s\n                minimum_duration=0.015,  # sec\n                zscore_threshold=2.0,  # std\n                smoothing_sigma=0.004,  # sec\n                close_ripple_threshold=0.0,  # sec\n            ),\n        }\n        self.insert1(\n            {\"ripple_param_name\": \"default\", \"ripple_param_dict\": default_dict},\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default parameter set</p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>def insert_default(self):\n    \"\"\"Insert the default parameter set\"\"\"\n    default_dict = {\n        \"speed_name\": \"head_speed\",\n        \"ripple_detection_algorithm\": \"Kay_ripple_detector\",\n        \"ripple_detection_params\": dict(\n            speed_threshold=4.0,  # cm/s\n            minimum_duration=0.015,  # sec\n            zscore_threshold=2.0,  # std\n            smoothing_sigma=0.004,  # sec\n            close_ripple_threshold=0.0,  # sec\n        ),\n    }\n    self.insert1(\n        {\"ripple_param_name\": \"default\", \"ripple_param_dict\": default_dict},\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleTimes", "title": "<code>RippleTimes</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@schema\nclass RippleTimes(dj.Computed):\n    definition = \"\"\"\n    -&gt; RippleParameters\n    -&gt; RippleLFPSelection\n    -&gt; IntervalPositionInfo\n    ---\n    -&gt; AnalysisNwbfile\n    ripple_times_object_id : varchar(40)\n     \"\"\"\n\n    def make(self, key):\n        print(f\"Computing ripple times for: {key}\")\n        ripple_params = (\n            RippleParameters &amp; {\"ripple_param_name\": key[\"ripple_param_name\"]}\n        ).fetch1(\"ripple_param_dict\")\n\n        ripple_detection_algorithm = ripple_params[\"ripple_detection_algorithm\"]\n        ripple_detection_params = ripple_params[\"ripple_detection_params\"]\n\n        (\n            speed,\n            interval_ripple_lfps,\n            sampling_frequency,\n        ) = self.get_ripple_lfps_and_position_info(key)\n\n        ripple_times = RIPPLE_DETECTION_ALGORITHMS[ripple_detection_algorithm](\n            time=np.asarray(interval_ripple_lfps.index),\n            filtered_lfps=np.asarray(interval_ripple_lfps),\n            speed=np.asarray(speed),\n            sampling_frequency=sampling_frequency,\n            **ripple_detection_params,\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n        key[\"ripple_times_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=ripple_times,\n        )\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        \"\"\"Convenience function for returning the marks in a readable format\"\"\"\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [data[\"ripple_times\"] for data in self.fetch_nwb()]\n\n    @staticmethod\n    def get_ripple_lfps_and_position_info(key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        interval_list_name = key[\"target_interval_list_name\"]\n        position_info_param_name = key[\"position_info_param_name\"]\n        ripple_params = (\n            RippleParameters &amp; {\"ripple_param_name\": key[\"ripple_param_name\"]}\n        ).fetch1(\"ripple_param_dict\")\n\n        speed_name = ripple_params[\"speed_name\"]\n\n        electrode_keys = (RippleLFPSelection.RippleLFPElectrode() &amp; key).fetch(\n            \"electrode_id\"\n        )\n\n        # warn/validate that there is only one wire per electrode\n        lfp_key = key.copy()\n        del lfp_key[\"interval_list_name\"]\n        ripple_lfp_nwb = (LFPBand &amp; lfp_key).fetch_nwb()[0]\n        ripple_lfp_electrodes = ripple_lfp_nwb[\"filtered_data\"].electrodes.data[\n            :\n        ]\n        elec_mask = np.full_like(ripple_lfp_electrodes, 0, dtype=bool)\n        elec_mask[\n            [\n                ind\n                for ind, elec in enumerate(ripple_lfp_electrodes)\n                if elec in electrode_keys\n            ]\n        ] = True\n        ripple_lfp = pd.DataFrame(\n            ripple_lfp_nwb[\"filtered_data\"].data,\n            index=pd.Index(\n                ripple_lfp_nwb[\"filtered_data\"].timestamps, name=\"time\"\n            ),\n        )\n        sampling_frequency = ripple_lfp_nwb[\"lfp_band_sampling_rate\"]\n\n        ripple_lfp = ripple_lfp.loc[:, elec_mask]\n\n        position_valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        position_info = (\n            IntervalPositionInfo()\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": interval_list_name,\n                \"position_info_param_name\": position_info_param_name,\n            }\n        ).fetch1_dataframe()\n\n        position_info = pd.concat(\n            [\n                position_info.loc[slice(valid_time[0], valid_time[1])]\n                for valid_time in position_valid_times\n            ],\n            axis=1,\n        )\n        interval_ripple_lfps = pd.concat(\n            [\n                ripple_lfp.loc[slice(valid_time[0], valid_time[1])]\n                for valid_time in position_valid_times\n            ],\n            axis=1,\n        )\n\n        position_info = interpolate_to_new_time(\n            position_info, interval_ripple_lfps.index\n        )\n\n        return (\n            position_info[speed_name],\n            interval_ripple_lfps,\n            sampling_frequency,\n        )\n\n    @staticmethod\n    def get_Kay_ripple_consensus_trace(\n        ripple_filtered_lfps, sampling_frequency, smoothing_sigma=0.004\n    ):\n        ripple_consensus_trace = np.full_like(ripple_filtered_lfps, np.nan)\n        not_null = np.all(pd.notnull(ripple_filtered_lfps), axis=1)\n\n        ripple_consensus_trace[not_null] = get_envelope(\n            np.asarray(ripple_filtered_lfps)[not_null]\n        )\n        ripple_consensus_trace = np.sum(ripple_consensus_trace**2, axis=1)\n        ripple_consensus_trace[not_null] = gaussian_smooth(\n            ripple_consensus_trace[not_null],\n            smoothing_sigma,\n            sampling_frequency,\n        )\n        return pd.DataFrame(\n            np.sqrt(ripple_consensus_trace), index=ripple_filtered_lfps.index\n        )\n\n    @staticmethod\n    def plot_ripple_consensus_trace(\n        ripple_consensus_trace,\n        ripple_times,\n        ripple_label=1,\n        offset=0.100,\n        relative=True,\n        ax=None,\n    ):\n        ripple_start = ripple_times.loc[ripple_label].start_time\n        ripple_end = ripple_times.loc[ripple_label].end_time\n        time_slice = slice(ripple_start - offset, ripple_end + offset)\n\n        start_offset = ripple_start if relative else 0\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(12, 1))\n        ax.plot(\n            ripple_consensus_trace.loc[time_slice].index - start_offset,\n            ripple_consensus_trace.loc[time_slice],\n        )\n        ax.axvspan(\n            ripple_start - start_offset,\n            ripple_end - start_offset,\n            zorder=-1,\n            alpha=0.5,\n            color=\"lightgrey\",\n        )\n        ax.set_xlabel(\"Time [s]\")\n        ax.set_xlim(\n            (time_slice.start - start_offset, time_slice.stop - start_offset)\n        )\n\n    @staticmethod\n    def plot_ripple(\n        lfps, ripple_times, ripple_label=1, offset=0.100, relative=True, ax=None\n    ):\n        lfp_labels = lfps.columns\n        n_lfps = len(lfp_labels)\n        ripple_start = ripple_times.loc[ripple_label].start_time\n        ripple_end = ripple_times.loc[ripple_label].end_time\n        time_slice = slice(ripple_start - offset, ripple_end + offset)\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(12, n_lfps * 0.20))\n\n        start_offset = ripple_start if relative else 0\n\n        for lfp_ind, lfp_label in enumerate(lfp_labels):\n            lfp = lfps.loc[time_slice, lfp_label]\n            ax.plot(\n                lfp.index - start_offset,\n                lfp_ind + (lfp - lfp.mean()) / (lfp.max() - lfp.min()),\n                color=\"black\",\n            )\n\n        ax.axvspan(\n            ripple_start - start_offset,\n            ripple_end - start_offset,\n            zorder=-1,\n            alpha=0.5,\n            color=\"lightgrey\",\n        )\n        ax.set_ylim((-1, n_lfps))\n        ax.set_xlim(\n            (time_slice.start - start_offset, time_slice.stop - start_offset)\n        )\n        ax.set_ylabel(\"LFPs\")\n        ax.set_xlabel(\"Time [s]\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleTimes.fetch1_dataframe", "title": "<code>fetch1_dataframe()</code>", "text": "<p>Convenience function for returning the marks in a readable format</p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>def fetch1_dataframe(self):\n    \"\"\"Convenience function for returning the marks in a readable format\"\"\"\n    return self.fetch_dataframe()[0]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/", "title": "common_sensors.py", "text": "<p>Schema for headstage or other environmental sensors.</p>"}, {"location": "api/src/spyglass/common/common_session/", "title": "common_session.py", "text": ""}, {"location": "api/src/spyglass/common/common_subject/", "title": "common_subject.py", "text": ""}, {"location": "api/src/spyglass/common/common_subject/#src.spyglass.common.common_subject.Subject", "title": "<code>Subject</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_subject.py</code> <pre><code>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id: varchar(80)\n    ---\n    age = NULL: varchar(200)\n    description = NULL: varchar(2000)\n    genotype = NULL: varchar(2000)\n    sex = \"U\": enum(\"M\", \"F\", \"U\")\n    species = NULL: varchar(200)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n        \"\"\"Get the subject information from the NWBFile and insert it into the Subject table.\"\"\"\n        sub = nwbf.subject\n        if sub is None:\n            print(\"No subject metadata found.\\n\")\n            return\n        subject_dict = dict()\n        subject_dict[\"subject_id\"] = sub.subject_id\n        subject_dict[\"age\"] = sub.age\n        subject_dict[\"description\"] = sub.description\n        subject_dict[\"genotype\"] = sub.genotype\n        if sub.sex in (\"Male\", \"male\", \"M\", \"m\"):\n            sex = \"M\"\n        elif sub.sex in (\"Female\", \"female\", \"F\", \"f\"):\n            sex = \"F\"\n        else:\n            sex = \"U\"\n        subject_dict[\"sex\"] = sex\n        subject_dict[\"species\"] = sub.species\n        cls.insert1(subject_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_subject/#src.spyglass.common.common_subject.Subject.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Get the subject information from the NWBFile and insert it into the Subject table.</p> Source code in <code>src/spyglass/common/common_subject.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n    \"\"\"Get the subject information from the NWBFile and insert it into the Subject table.\"\"\"\n    sub = nwbf.subject\n    if sub is None:\n        print(\"No subject metadata found.\\n\")\n        return\n    subject_dict = dict()\n    subject_dict[\"subject_id\"] = sub.subject_id\n    subject_dict[\"age\"] = sub.age\n    subject_dict[\"description\"] = sub.description\n    subject_dict[\"genotype\"] = sub.genotype\n    if sub.sex in (\"Male\", \"male\", \"M\", \"m\"):\n        sex = \"M\"\n    elif sub.sex in (\"Female\", \"female\", \"F\", \"f\"):\n        sex = \"F\"\n    else:\n        sex = \"U\"\n    subject_dict[\"sex\"] = sex\n    subject_dict[\"species\"] = sub.species\n    cls.insert1(subject_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/", "title": "common_task.py", "text": ""}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task", "title": "<code>Task</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@schema\nclass Task(dj.Manual):\n    definition = \"\"\"\n     task_name: varchar(80)\n     ---\n     task_description = NULL: varchar(2000)    # description of this task\n     task_type = NULL: varchar(2000)           # type of task\n     task_subtype = NULL: varchar(2000)        # subtype of task\n     \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n        \"\"\"Insert tasks from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        \"\"\"\n        tasks_mod = nwbf.processing.get(\"tasks\")\n        if tasks_mod is None:\n            print(f\"No tasks processing module found in {nwbf}\\n\")\n            return\n        for task in tasks_mod.data_interfaces.values():\n            if cls.check_task_table(task):\n                cls.insert_from_task_table(task)\n\n    @classmethod\n    def insert_from_task_table(cls, task_table):\n        \"\"\"Insert tasks from a pynwb DynamicTable containing task metadata.\n\n        Duplicate tasks will not be added.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n        \"\"\"\n        taskdf = task_table.to_dataframe()\n        for task_entry in taskdf.iterrows():\n            task_dict = dict()\n            task_dict[\"task_name\"] = task_entry[1].task_name\n            task_dict[\"task_description\"] = task_entry[1].task_description\n            cls.insert1(task_dict, skip_duplicates=True)\n\n    @classmethod\n    def check_task_table(cls, task_table):\n        \"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n\n        The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name' and\n        'task_description'.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n\n        Returns\n        -------\n        bool\n            Whether the DynamicTable conforms to the expected format for loading data into the Task table.\n        \"\"\"\n        return (\n            isinstance(task_table, pynwb.core.DynamicTable)\n            and hasattr(task_table, \"task_name\")\n            and hasattr(task_table, \"task_description\")\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert tasks from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n    \"\"\"Insert tasks from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    \"\"\"\n    tasks_mod = nwbf.processing.get(\"tasks\")\n    if tasks_mod is None:\n        print(f\"No tasks processing module found in {nwbf}\\n\")\n        return\n    for task in tasks_mod.data_interfaces.values():\n        if cls.check_task_table(task):\n            cls.insert_from_task_table(task)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task.insert_from_task_table", "title": "<code>insert_from_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Insert tasks from a pynwb DynamicTable containing task metadata.</p> <p>Duplicate tasks will not be added.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>DynamicTable</code> <p>The table representing task metadata.</p> required Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef insert_from_task_table(cls, task_table):\n    \"\"\"Insert tasks from a pynwb DynamicTable containing task metadata.\n\n    Duplicate tasks will not be added.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n    \"\"\"\n    taskdf = task_table.to_dataframe()\n    for task_entry in taskdf.iterrows():\n        task_dict = dict()\n        task_dict[\"task_name\"] = task_entry[1].task_name\n        task_dict[\"task_description\"] = task_entry[1].task_description\n        cls.insert1(task_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task.check_task_table", "title": "<code>check_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.</p> <p>The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name' and 'task_description'.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>DynamicTable</code> <p>The table representing task metadata.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the DynamicTable conforms to the expected format for loading data into the Task table.</p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef check_task_table(cls, task_table):\n    \"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n\n    The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name' and\n    'task_description'.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n\n    Returns\n    -------\n    bool\n        Whether the DynamicTable conforms to the expected format for loading data into the Task table.\n    \"\"\"\n    return (\n        isinstance(task_table, pynwb.core.DynamicTable)\n        and hasattr(task_table, \"task_name\")\n        and hasattr(task_table, \"task_description\")\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.TaskEpoch", "title": "<code>TaskEpoch</code>", "text": "<p>             Bases: <code>Imported</code></p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@schema\nclass TaskEpoch(dj.Imported):\n    # Tasks, session and time intervals\n    definition = \"\"\"\n     -&gt; Session\n     epoch: int  # the session epoch for this task and apparatus(1 based)\n     ---\n     -&gt; Task\n     -&gt; [nullable] CameraDevice\n     -&gt; IntervalList\n     task_environment = NULL: varchar(200)  # the environment the animal was in\n     camera_names : blob # list of keys corresponding to entry in CameraDevice\n     \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile().get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        camera_names = dict()\n        # the tasks refer to the camera_id which is unique for the NWB file but not for CameraDevice schema, so we\n        # need to look up the right camera\n        # map camera ID (in camera name) to camera_name\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                # get the camera ID\n                camera_id = int(str.split(device.name)[1])\n                camera_names[camera_id] = device.camera_name\n\n        # find the task modules and for each one, add the task to the Task schema if it isn't there\n        # and then add an entry for each epoch\n        tasks_mod = nwbf.processing.get(\"tasks\")\n        if tasks_mod is None:\n            print(f\"No tasks processing module found in {nwbf}\\n\")\n            return\n\n        for task in tasks_mod.data_interfaces.values():\n            if self.check_task_table(task):\n                # check if the task is in the Task table and if not, add it\n                Task.insert_from_task_table(task)\n                key[\"task_name\"] = task.task_name[0]\n\n                # get the CameraDevice used for this task (primary key is camera name so we need\n                # to map from ID to name)\n                camera_ids = task.camera_id[0]\n                valid_camera_ids = [\n                    camera_id\n                    for camera_id in camera_ids\n                    if camera_id in camera_names.keys()\n                ]\n                if valid_camera_ids:\n                    key[\"camera_names\"] = [\n                        {\"camera_name\": camera_names[camera_id]}\n                        for camera_id in valid_camera_ids\n                    ]\n                else:\n                    print(\n                        f\"No camera device found with ID {camera_ids} in NWB file {nwbf}\\n\"\n                    )\n                # Add task environment\n                if hasattr(task, \"task_environment\"):\n                    key[\"task_environment\"] = task.task_environment[0]\n\n                # get the interval list for this task, which corresponds to the matching epoch for the raw data.\n                # Users should define more restrictive intervals as required for analyses\n                session_intervals = (\n                    IntervalList() &amp; {\"nwb_file_name\": nwb_file_name}\n                ).fetch(\"interval_list_name\")\n                for epoch in task.task_epochs[0]:\n                    # TODO in beans file, task_epochs[0] is 1x2 dset of ints, so epoch would be an int\n                    key[\"epoch\"] = epoch\n                    target_interval = str(epoch).zfill(2)\n                    for interval in session_intervals:\n                        if (\n                            target_interval in interval\n                        ):  # TODO this is not true for the beans file\n                            break\n                    # TODO case when interval is not found is not handled\n                    key[\"interval_list_name\"] = interval\n                    self.insert1(key)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_names\"):\n                continue\n            row[\"camera_names\"] = [\n                {\"camera_name\": (cls &amp; row).fetch1(\"camera_name\")}\n            ]\n            cls.update1(row=row)\n\n    @classmethod\n    def check_task_table(cls, task_table):\n        \"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n        The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n        'task_description', 'camera_id', 'and 'task_epochs'.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n\n        Returns\n        -------\n        bool\n            Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n        \"\"\"\n\n        # TODO this could be more strict and check data types, but really it should be schematized\n        return (\n            Task.check_task_table(task_table)\n            and hasattr(task_table, \"camera_id\")\n            and hasattr(task_table, \"task_epochs\")\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.TaskEpoch.check_task_table", "title": "<code>check_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.</p> <p>The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name', 'task_description', 'camera_id', 'and 'task_epochs'.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>DynamicTable</code> <p>The table representing task metadata.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.</p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef check_task_table(cls, task_table):\n    \"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n    The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n    'task_description', 'camera_id', 'and 'task_epochs'.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n\n    Returns\n    -------\n    bool\n        Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n    \"\"\"\n\n    # TODO this could be more strict and check data types, but really it should be schematized\n    return (\n        Task.check_task_table(task_table)\n        and hasattr(task_table, \"camera_id\")\n        and hasattr(task_table, \"task_epochs\")\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/errors/", "title": "errors.py", "text": ""}, {"location": "api/src/spyglass/common/populate_all_common/", "title": "populate_all_common.py", "text": ""}, {"location": "api/src/spyglass/common/signal_processing/", "title": "signal_processing.py", "text": ""}, {"location": "api/src/spyglass/common/signal_processing/#src.spyglass.common.signal_processing.hilbert_decomp", "title": "<code>hilbert_decomp(lfp_band_object, sampling_rate=1)</code>", "text": "<p>generates the analytical decomposition of the signals in the lfp_band_object</p> <p>:param lfp_band_object: bandpass filtered LFP :type lfp_band_object: pynwb electrical series :param sampling_rate: bandpass filtered LFP sampling rate (defaults to 1; only used for instantaneous frequency) :type sampling_rate: int :return: envelope, phase, frequency :rtype: pynwb electrical series objects</p> Source code in <code>src/spyglass/common/signal_processing.py</code> <pre><code>def hilbert_decomp(lfp_band_object, sampling_rate=1):\n    \"\"\"generates the analytical decomposition of the signals in the lfp_band_object\n\n    :param lfp_band_object: bandpass filtered LFP\n    :type lfp_band_object: pynwb electrical series\n    :param sampling_rate: bandpass filtered LFP sampling rate (defaults to 1; only used for instantaneous frequency)\n    :type sampling_rate: int\n    :return: envelope, phase, frequency\n    :rtype: pynwb electrical series objects\n    \"\"\"\n    analytical_signal = signal.hilbert(lfp_band_object.data, axis=0)\n\n    eseries_name = \"envelope\"\n    envelope = pynwb.ecephys.ElectricalSeries(\n        name=eseries_name,\n        data=np.abs(analytical_signal),\n        electrodes=lfp_band_object.electrodes,\n        timestamps=lfp_band_object.timestamps,\n    )\n\n    eseries_name = \"phase\"\n    instantaneous_phase = np.unwrap(np.angle(analytical_signal))\n    phase = pynwb.ecephys.ElectricalSeries(\n        name=eseries_name,\n        data=instantaneous_phase,\n        electrodes=lfp_band_object.electrodes,\n        timestamps=lfp_band_object.timestamps,\n    )\n\n    eseries_name = \"frequency\"\n    instantaneous_frequency = (\n        np.diff(instantaneous_phase) / (2.0 * np.pi) * sampling_rate\n    )\n    frequency = pynwb.ecephys.ElectricalSeries(\n        name=eseries_name,\n        data=instantaneous_frequency,\n        electrodes=lfp_band_object.electrodes,\n        timestamps=lfp_band_object.timestamps,\n    )\n    return envelope, phase, frequency\n</code></pre>"}, {"location": "api/src/spyglass/common/prepopulate/prepopulate/", "title": "prepopulate.py", "text": ""}, {"location": "api/src/spyglass/common/prepopulate/prepopulate/#src.spyglass.common.prepopulate.prepopulate.prepopulate_default", "title": "<code>prepopulate_default()</code>", "text": "<p>Populate the database with default values in SPYGLASS_BASE_DIR/entries.yaml</p> Source code in <code>src/spyglass/common/prepopulate/prepopulate.py</code> <pre><code>def prepopulate_default():\n    \"\"\"\n    Populate the database with default values in SPYGLASS_BASE_DIR/entries.yaml\n    \"\"\"\n    yaml_path = pathlib.Path(base_dir) / \"entries.yaml\"\n    if os.path.exists(yaml_path):\n        populate_from_yaml(yaml_path)\n</code></pre>"}, {"location": "api/src/spyglass/common/prepopulate/prepopulate/#src.spyglass.common.prepopulate.prepopulate.populate_from_yaml", "title": "<code>populate_from_yaml(yaml_path)</code>", "text": "<p>Populate the database from specially formatted YAML files.</p> Source code in <code>src/spyglass/common/prepopulate/prepopulate.py</code> <pre><code>def populate_from_yaml(yaml_path: str):\n    \"\"\"Populate the database from specially formatted YAML files.\"\"\"\n    if not os.path.exists(yaml_path):\n        raise ValueError(f\"There is no file found with the path: {yaml_path}\")\n    with open(yaml_path, \"r\") as stream:\n        d = yaml.safe_load(stream)\n\n    for table_name, table_entries in d.items():\n        table_cls = _get_table_cls(table_name)\n        for entry_dict in table_entries:\n            # test whether an entity with the primary key(s) already exists in the table\n            if not issubclass(table_cls, (dj.Manual, dj.Lookup, dj.Part)):\n                raise ValueError(\n                    f\"Prepopulate YAML ('{yaml_path}') contains table '{table_name}' that cannot be \"\n                    \"prepopulated. Only Manual and Lookup tables can be prepopulated.\"\n                )\n            if hasattr(table_cls, \"fetch_add\"):\n                # if the table has defined a fetch_add method, use that instead of insert1. this is useful for\n                # tables where the primary key is an ID that auto-increments.\n                # first check whether an entry exists with the same information.\n                query = table_cls &amp; entry_dict\n                if not query:\n                    print(\n                        f\"Populate: Populating table {table_cls.__name__} with data {entry_dict} using fetch_add.\"\n                    )\n                    table_cls.fetch_add(**entry_dict)\n                continue\n\n            primary_key_values = {\n                k: v\n                for k, v in entry_dict.items()\n                if k in table_cls.primary_key\n            }\n            if not primary_key_values:\n                print(\n                    f\"Populate: No primary key provided in data {entry_dict} for table {table_cls.__name__}\"\n                )\n                continue\n            if primary_key_values not in table_cls.fetch(\n                *table_cls.primary_key, as_dict=True\n            ):\n                print(\n                    f\"Populate: Populating table {table_cls.__name__} with data {entry_dict} using insert1.\"\n                )\n                table_cls.insert1(entry_dict)\n            else:\n                logging.info(\n                    f\"Populate: Entry in {table_cls.__name__} with primary keys {primary_key_values} already exists.\"\n                )\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/", "title": "insert_sessions.py", "text": ""}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.data_import.insert_sessions.insert_sessions", "title": "<code>insert_sessions(nwb_file_names)</code>", "text": "<p>Populate the dj database with new sessions.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_names</code> <code>str or List of str</code> <p>File names in raw directory ($SPYGLASS_RAW_DIR) pointing to existing .nwb files. Each file represents a session. Also accepts strings with glob wildcards (e.g., *) so long as the wildcard specifies exactly one file.</p> required Source code in <code>src/spyglass/data_import/insert_sessions.py</code> <pre><code>def insert_sessions(nwb_file_names: Union[str, List[str]]):\n    \"\"\"\n    Populate the dj database with new sessions.\n\n    Parameters\n    ----------\n    nwb_file_names : str or List of str\n        File names in raw directory ($SPYGLASS_RAW_DIR) pointing to\n        existing .nwb files. Each file represents a session. Also accepts\n        strings with glob wildcards (e.g., *) so long as the wildcard specifies\n        exactly one file.\n    \"\"\"\n\n    if not isinstance(nwb_file_names, list):\n        nwb_file_names = [nwb_file_names]\n\n    for nwb_file_name in nwb_file_names:\n        if \"/\" in nwb_file_name:\n            nwb_file_name = nwb_file_name.split(\"/\")[-1]\n\n        nwb_file_abs_path = Path(\n            Nwbfile.get_abs_path(nwb_file_name, new_file=True)\n        )\n\n        if not nwb_file_abs_path.exists():\n            possible_matches = sorted(Path(raw_dir).glob(f\"*{nwb_file_name}*\"))\n\n            if len(possible_matches) == 1:\n                nwb_file_abs_path = possible_matches[0]\n                nwb_file_name = nwb_file_abs_path.name\n\n            else:\n                raise FileNotFoundError(\n                    f\"File not found: {nwb_file_abs_path}\\n\\t\"\n                    + f\"{len(possible_matches)} possible matches:\"\n                    + f\"{possible_matches}\"\n                )\n\n        # file name for the copied raw data\n        out_nwb_file_name = get_nwb_copy_filename(nwb_file_abs_path.name)\n\n        # Check whether the file already exists in the Nwbfile table\n        if len(Nwbfile() &amp; {\"nwb_file_name\": out_nwb_file_name}):\n            warnings.warn(\n                f\"Cannot insert data from {nwb_file_name}: {out_nwb_file_name}\"\n                + \" is already in Nwbfile table.\"\n            )\n            continue\n\n        # Make a copy of the NWB file that ends with '_'.\n        # This has everything except the raw data but has a link to\n        # the raw data in the original file\n        copy_nwb_link_raw_ephys(nwb_file_name, out_nwb_file_name)\n        Nwbfile().insert_from_relative_file_name(out_nwb_file_name)\n        populate_all_common(out_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.data_import.insert_sessions.copy_nwb_link_raw_ephys", "title": "<code>copy_nwb_link_raw_ephys(nwb_file_name, out_nwb_file_name)</code>", "text": "<p>Copies an NWB file with a link to raw ephys data.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file to be copied.</p> required <code>out_nwb_file_name</code> <code>str</code> <p>The name of the new NWB file with the link to raw ephys data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute path of the new NWB file.</p> Source code in <code>src/spyglass/data_import/insert_sessions.py</code> <pre><code>def copy_nwb_link_raw_ephys(nwb_file_name, out_nwb_file_name):\n    \"\"\"Copies an NWB file with a link to raw ephys data.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file to be copied.\n    out_nwb_file_name : str\n        The name of the new NWB file with the link to raw ephys data.\n\n    Returns\n    -------\n    str\n        The absolute path of the new NWB file.\n    \"\"\"\n    print(\n        f\"Creating a copy of NWB file {nwb_file_name} \"\n        + f\"with link to raw ephys data: {out_nwb_file_name}\"\n    )\n\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name, new_file=True)\n\n    if not os.path.exists(nwb_file_abs_path):\n        raise FileNotFoundError(f\"Could not find raw file: {nwb_file_abs_path}\")\n\n    out_nwb_file_abs_path = Nwbfile.get_abs_path(\n        out_nwb_file_name, new_file=True\n    )\n\n    if os.path.exists(out_nwb_file_abs_path):\n        if debug_mode:\n            return out_nwb_file_abs_path\n        warnings.warn(\n            f\"Output file {out_nwb_file_abs_path} exists and will be \"\n            + \"overwritten.\"\n        )\n\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abs_path, mode=\"r\", load_namespaces=True\n    ) as input_io:\n        nwbf = input_io.read()\n\n        # pop off acquisition electricalseries\n        eseries_list = get_raw_eseries(nwbf)\n        for eseries in eseries_list:\n            nwbf.acquisition.pop(eseries.name)\n\n        # pop off analog processing module\n        analog_processing = nwbf.processing.get(\"analog\")\n        if analog_processing:\n            nwbf.processing.pop(\"analog\")\n\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=out_nwb_file_abs_path, mode=\"w\", manager=input_io.manager\n        ) as export_io:\n            export_io.export(input_io, nwbf)\n\n    # add link from new file back to raw ephys data in raw data file using fresh build manager and container cache\n    # where the acquisition electricalseries objects have not been removed\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abs_path, mode=\"r\", load_namespaces=True\n    ) as input_io:\n        nwbf_raw = input_io.read()\n        eseries_list = get_raw_eseries(nwbf_raw)\n        analog_processing = nwbf_raw.processing.get(\"analog\")\n\n        with pynwb.NWBHDF5IO(\n            path=out_nwb_file_abs_path, mode=\"a\", manager=input_io.manager\n        ) as export_io:\n            nwbf_export = export_io.read()\n\n            # add link to raw ephys ElectricalSeries in raw data file\n            for eseries in eseries_list:\n                nwbf_export.add_acquisition(eseries)\n\n            # add link to processing module in raw data file\n            if analog_processing:\n                nwbf_export.add_processing_module(analog_processing)\n\n            nwbf_export.set_modified()\n            export_io.write(nwbf_export)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(out_nwb_file_abs_path, permissions)\n\n    return out_nwb_file_abs_path\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/", "title": "clusterless.py", "text": "<p>Pipeline for decoding the animal's mental position and some category of interest from unclustered spikes and spike waveform features. See [1] for details.</p>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless--references", "title": "References", "text": "<p>[1] Denovellis, E. L. et al. Hippocampal replay of experience at real-world speeds. eLife 10, e64505 (2021).</p>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MarkParameters", "title": "<code>MarkParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Defines the type of spike waveform feature computed for a given spike time.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MarkParameters(dj.Manual):\n    \"\"\"Defines the type of spike waveform feature computed for a given spike\n    time.\"\"\"\n\n    definition = \"\"\"\n    mark_param_name : varchar(80) # a name for this set of parameters\n    ---\n    # the type of mark. Currently only 'amplitude' is supported\n    mark_type = 'amplitude':  varchar(40)\n    mark_param_dict:    BLOB    # dictionary of parameters for the mark extraction function\n    \"\"\"\n\n    def insert_default(self):\n        \"\"\"Insert the default parameter set\n\n        Examples\n        --------\n        {'peak_sign': 'neg', 'threshold' : 100}\n        corresponds to negative going waveforms of at least 100 uV size\n        \"\"\"\n        default_dict = {}\n        self.insert1(\n            {\"mark_param_name\": \"default\", \"mark_param_dict\": default_dict},\n            skip_duplicates=True,\n        )\n\n    @staticmethod\n    def supported_mark_type(mark_type):\n        \"\"\"checks whether the requested mark type is supported.\n        Currently only 'amplitude\" is supported.\n\n        Parameters\n        ----------\n        mark_type : str\n\n        \"\"\"\n        supported_types = [\"amplitude\"]\n        if mark_type in supported_types:\n            return True\n        return False\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MarkParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default parameter set</p> <p>Examples:</p> <p>{'peak_sign': 'neg', 'threshold' : 100} corresponds to negative going waveforms of at least 100 uV size</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def insert_default(self):\n    \"\"\"Insert the default parameter set\n\n    Examples\n    --------\n    {'peak_sign': 'neg', 'threshold' : 100}\n    corresponds to negative going waveforms of at least 100 uV size\n    \"\"\"\n    default_dict = {}\n    self.insert1(\n        {\"mark_param_name\": \"default\", \"mark_param_dict\": default_dict},\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MarkParameters.supported_mark_type", "title": "<code>supported_mark_type(mark_type)</code>  <code>staticmethod</code>", "text": "<p>checks whether the requested mark type is supported. Currently only 'amplitude\" is supported.</p> <p>Parameters:</p> Name Type Description Default <code>mark_type</code> <code>str</code> required Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@staticmethod\ndef supported_mark_type(mark_type):\n    \"\"\"checks whether the requested mark type is supported.\n    Currently only 'amplitude\" is supported.\n\n    Parameters\n    ----------\n    mark_type : str\n\n    \"\"\"\n    supported_types = [\"amplitude\"]\n    if mark_type in supported_types:\n        return True\n    return False\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarks", "title": "<code>UnitMarks</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>For each spike time, compute a spike waveform feature associated with that spike. Used for clusterless decoding.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass UnitMarks(dj.Computed):\n    \"\"\"For each spike time, compute a spike waveform feature associated with that\n    spike. Used for clusterless decoding.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarkParameters\n    ---\n    -&gt; AnalysisNwbfile\n    marks_object_id: varchar(40) # the NWB object that stores the marks\n    \"\"\"\n\n    def make(self, key):\n        # get the list of mark parameters\n        mark_param = (MarkParameters &amp; key).fetch1()\n\n        # check that the mark type is supported\n        if not MarkParameters().supported_mark_type(mark_param[\"mark_type\"]):\n            Warning(\n                f'Mark type {mark_param[\"mark_type\"]} not supported; skipping'\n            )\n            return\n\n        # retrieve the units from the NWB file\n        nwb_units = (CuratedSpikeSorting() &amp; key).fetch_nwb()[0][\"units\"]\n\n        recording = Curation.get_recording(key)\n        if recording.get_num_segments() &gt; 1:\n            recording = si.concatenate_recordings([recording])\n        sorting = Curation.get_curated_sorting(key)\n        waveform_extractor_name = (\n            f'{key[\"nwb_file_name\"]}_{str(uuid.uuid4())[0:8]}_'\n            f'{key[\"curation_id\"]}_clusterless_waveforms'\n        )\n        waveform_extractor_path = str(\n            Path(os.environ[\"SPYGLASS_WAVEFORMS_DIR\"])\n            / Path(waveform_extractor_name)\n        )\n        if os.path.exists(waveform_extractor_path):\n            shutil.rmtree(waveform_extractor_path)\n\n        WAVEFORM_PARAMS = {\n            \"ms_before\": 0.5,\n            \"ms_after\": 0.5,\n            \"max_spikes_per_unit\": None,\n            \"n_jobs\": 5,\n            \"total_memory\": \"5G\",\n        }\n        waveform_extractor = si.extract_waveforms(\n            recording=recording,\n            sorting=sorting,\n            folder=waveform_extractor_path,\n            **WAVEFORM_PARAMS,\n        )\n\n        if mark_param[\"mark_type\"] == \"amplitude\":\n            sorter = (CuratedSpikeSorting() &amp; key).fetch1(\"sorter\")\n            if sorter == \"clusterless_thresholder\":\n                estimate_peak_time = False\n            else:\n                estimate_peak_time = True\n\n            try:\n                peak_sign = mark_param[\"mark_param_dict\"][\"peak_sign\"]\n            except KeyError:\n                peak_sign = \"neg\"\n\n            marks = np.concatenate(\n                [\n                    UnitMarks._get_peak_amplitude(\n                        waveform=waveform_extractor.get_waveforms(unit_id),\n                        peak_sign=peak_sign,\n                        estimate_peak_time=estimate_peak_time,\n                    )\n                    for unit_id in nwb_units.index\n                ],\n                axis=0,\n            )\n\n            timestamps = np.concatenate(np.asarray(nwb_units[\"spike_times\"]))\n            sorted_timestamp_ind = np.argsort(timestamps)\n            marks = marks[sorted_timestamp_ind]\n            timestamps = timestamps[sorted_timestamp_ind]\n\n        if \"threshold\" in mark_param[\"mark_param_dict\"]:\n            timestamps, marks = UnitMarks._threshold(\n                timestamps, marks, mark_param[\"mark_param_dict\"]\n            )\n\n        # create a new AnalysisNwbfile and a timeseries for the marks and save\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        nwb_object = pynwb.TimeSeries(\n            name=\"marks\",\n            data=marks,\n            unit=\"uV\",\n            timestamps=timestamps,\n            description=\"spike features for clusterless decoding\",\n        )\n        key[\"marks_object_id\"] = AnalysisNwbfile().add_nwb_object(\n            key[\"analysis_file_name\"], nwb_object\n        )\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        \"\"\"Convenience function for returning the marks in a readable format\"\"\"\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [self._convert_to_dataframe(data) for data in self.fetch_nwb()]\n\n    @staticmethod\n    def _convert_to_dataframe(nwb_data):\n        n_marks = nwb_data[\"marks\"].data.shape[1]\n        columns = [f\"amplitude_{ind:04d}\" for ind in range(n_marks)]\n        return pd.DataFrame(\n            nwb_data[\"marks\"].data,\n            index=pd.Index(nwb_data[\"marks\"].timestamps, name=\"time\"),\n            columns=columns,\n        )\n\n    @staticmethod\n    def _get_peak_amplitude(\n        waveform, peak_sign=\"neg\", estimate_peak_time=False\n    ):\n        \"\"\"Returns the amplitudes of all channels at the time of the peak\n        amplitude across channels.\n\n        Parameters\n        ----------\n        waveform : array-like, shape (n_spikes, n_time, n_channels)\n        peak_sign : ('pos', 'neg', 'both'), optional\n            Direction of the peak in the waveform\n        estimate_peak_time : bool, optional\n            Find the peak times for each spike because some spikesorters do not\n            align the spike time (at index n_time // 2) to the peak\n\n        Returns\n        -------\n        peak_amplitudes : array-like, shape (n_spikes, n_channels)\n\n        \"\"\"\n        if estimate_peak_time:\n            if peak_sign == \"neg\":\n                peak_inds = np.argmin(np.min(waveform, axis=2), axis=1)\n            elif peak_sign == \"pos\":\n                peak_inds = np.argmax(np.max(waveform, axis=2), axis=1)\n            elif peak_sign == \"both\":\n                peak_inds = np.argmax(np.max(np.abs(waveform), axis=2), axis=1)\n\n            # Get mode of peaks to find the peak time\n            values, counts = np.unique(peak_inds, return_counts=True)\n            spike_peak_ind = values[counts.argmax()]\n        else:\n            spike_peak_ind = waveform.shape[1] // 2\n\n        return waveform[:, spike_peak_ind]\n\n    @staticmethod\n    def _threshold(timestamps, marks, mark_param_dict):\n        \"\"\"Filter the marks by an amplitude threshold\n\n        Parameters\n        ----------\n        timestamps : array-like, shape (n_time,)\n        marks : array-like, shape (n_time, n_channels)\n        mark_param_dict : dict\n\n        Returns\n        -------\n        filtered_timestamps : array-like, shape (n_filtered_time,)\n        filtered_marks : array-like, shape (n_filtered_time, n_channels)\n\n        \"\"\"\n        if mark_param_dict[\"peak_sign\"] == \"neg\":\n            include = np.min(marks, axis=1) &lt;= -1 * mark_param_dict[\"threshold\"]\n        elif mark_param_dict[\"peak_sign\"] == \"pos\":\n            include = np.max(marks, axis=1) &gt;= mark_param_dict[\"threshold\"]\n        elif mark_param_dict[\"peak_sign\"] == \"both\":\n            include = (\n                np.max(np.abs(marks), axis=1) &gt;= mark_param_dict[\"threshold\"]\n            )\n        return timestamps[include], marks[include]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarks.fetch1_dataframe", "title": "<code>fetch1_dataframe()</code>", "text": "<p>Convenience function for returning the marks in a readable format</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def fetch1_dataframe(self):\n    \"\"\"Convenience function for returning the marks in a readable format\"\"\"\n    return self.fetch_dataframe()[0]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicatorSelection", "title": "<code>UnitMarksIndicatorSelection</code>", "text": "<p>             Bases: <code>Lookup</code></p> <p>Bins the spike times and associated spike waveform features for a given time interval into regular time bins determined by the sampling rate.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass UnitMarksIndicatorSelection(dj.Lookup):\n    \"\"\"Bins the spike times and associated spike waveform features for a given\n    time interval into regular time bins determined by the sampling rate.\"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarks\n    -&gt; IntervalList\n    sampling_rate=500 : float\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicator", "title": "<code>UnitMarksIndicator</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Bins the spike times and associated spike waveform features into regular time bins according to the sampling rate. Features that fall into the same time bin are averaged.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass UnitMarksIndicator(dj.Computed):\n    \"\"\"Bins the spike times and associated spike waveform features into regular\n    time bins according to the sampling rate. Features that fall into the same\n    time bin are averaged.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarks\n    -&gt; UnitMarksIndicatorSelection\n    ---\n    -&gt; AnalysisNwbfile\n    marks_indicator_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        # TODO: intersection of sort interval and interval list\n        interval_times = (IntervalList &amp; key).fetch1(\"valid_times\")\n\n        sampling_rate = (UnitMarksIndicatorSelection &amp; key).fetch(\n            \"sampling_rate\"\n        )\n\n        marks_df = (UnitMarks &amp; key).fetch1_dataframe()\n\n        time = self.get_time_bins_from_interval(interval_times, sampling_rate)\n\n        # Bin marks into time bins. No spike bins will have NaN\n        marks_df = marks_df.loc[time.min() : time.max()]\n        time_index = np.digitize(marks_df.index, time[1:-1])\n        marks_indicator_df = (\n            marks_df.groupby(time[time_index])\n            .mean()\n            .reindex(index=pd.Index(time, name=\"time\"))\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n\n        key[\"marks_indicator_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=marks_indicator_df.reset_index(),\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def get_time_bins_from_interval(interval_times, sampling_rate):\n        \"\"\"Picks the superset of the interval\"\"\"\n        start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n        n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n        return np.linspace(start_time, end_time, n_samples)\n\n    @staticmethod\n    def plot_all_marks(\n        marks_indicators: xr.DataArray, plot_size=5, s=10, plot_limit=None\n    ):\n        \"\"\"Plots 2D slices of each of the spike features against each other\n        for all electrodes.\n\n        Parameters\n        ----------\n        marks_indicators : xr.DataArray, shape (n_time, n_electrodes, n_features)\n            Spike times and associated spike waveform features binned into\n        plot_size : int, optional\n            Default 5. Matplotlib figure size for each mark.\n        s : int, optional\n            Default 10. Marker size\n        plot_limit : int, optional\n            Default None. Limits to first N electrodes.\n        \"\"\"\n        if not plot_limit:\n            plot_limit = len(marks_indicators.electrodes)\n\n        for electrode_ind in marks_indicators.electrodes[:plot_limit]:\n            marks = (\n                marks_indicators.sel(electrodes=electrode_ind)\n                .dropna(\"time\", how=\"all\")\n                .dropna(\"marks\")\n            )\n            n_features = len(marks.marks)\n            fig, axes = plt.subplots(\n                n_features,\n                n_features,\n                constrained_layout=True,\n                sharex=True,\n                sharey=True,\n                figsize=(plot_size * n_features, plot_size * n_features),\n            )\n            for ax_ind1, feature1 in enumerate(marks.marks):\n                for ax_ind2, feature2 in enumerate(marks.marks):\n                    try:\n                        axes[ax_ind1, ax_ind2].scatter(\n                            marks.sel(marks=feature1),\n                            marks.sel(marks=feature2),\n                            s=s,\n                        )\n                    except TypeError:\n                        axes.scatter(\n                            marks.sel(marks=feature1),\n                            marks.sel(marks=feature2),\n                            s=s,\n                        )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [\n            data[\"marks_indicator\"].set_index(\"time\")\n            for data in self.fetch_nwb()\n        ]\n\n    def fetch_xarray(self):\n        # sort_group_electrodes = (\n        #     SortGroup.SortGroupElectrode() &amp;\n        #     pd.DataFrame(self).to_dict('records'))\n        # brain_region = (sort_group_electrodes * Electrode *\n        #                 BrainRegion).fetch('region_name')\n\n        marks_indicators = (\n            xr.concat(\n                [\n                    df.to_xarray().to_array(\"marks\")\n                    for df in self.fetch_dataframe()\n                ],\n                dim=\"electrodes\",\n            )\n            .transpose(\"time\", \"marks\", \"electrodes\")\n            .assign_coords({\"electrodes\": self.fetch(\"sort_group_id\")})\n            .sortby([\"electrodes\", \"marks\"])\n        )\n\n        # hacky way to keep the marks in order\n        def reformat_name(name):\n            mark_type, number = name.split(\"_\")\n            return f\"{mark_type}_{int(number):04d}\"\n\n        new_mark_names = [\n            reformat_name(name) for name in marks_indicators.marks.values\n        ]\n\n        return marks_indicators.assign_coords({\"marks\": new_mark_names}).sortby(\n            [\"electrodes\", \"marks\"]\n        )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicator.get_time_bins_from_interval", "title": "<code>get_time_bins_from_interval(interval_times, sampling_rate)</code>  <code>staticmethod</code>", "text": "<p>Picks the superset of the interval</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@staticmethod\ndef get_time_bins_from_interval(interval_times, sampling_rate):\n    \"\"\"Picks the superset of the interval\"\"\"\n    start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n    n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n    return np.linspace(start_time, end_time, n_samples)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicator.plot_all_marks", "title": "<code>plot_all_marks(marks_indicators, plot_size=5, s=10, plot_limit=None)</code>  <code>staticmethod</code>", "text": "<p>Plots 2D slices of each of the spike features against each other for all electrodes.</p> <p>Parameters:</p> Name Type Description Default <code>marks_indicators</code> <code>(DataArray, shape(n_time, n_electrodes, n_features))</code> <p>Spike times and associated spike waveform features binned into</p> required <code>plot_size</code> <code>int</code> <p>Default 5. Matplotlib figure size for each mark.</p> <code>5</code> <code>s</code> <code>int</code> <p>Default 10. Marker size</p> <code>10</code> <code>plot_limit</code> <code>int</code> <p>Default None. Limits to first N electrodes.</p> <code>None</code> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@staticmethod\ndef plot_all_marks(\n    marks_indicators: xr.DataArray, plot_size=5, s=10, plot_limit=None\n):\n    \"\"\"Plots 2D slices of each of the spike features against each other\n    for all electrodes.\n\n    Parameters\n    ----------\n    marks_indicators : xr.DataArray, shape (n_time, n_electrodes, n_features)\n        Spike times and associated spike waveform features binned into\n    plot_size : int, optional\n        Default 5. Matplotlib figure size for each mark.\n    s : int, optional\n        Default 10. Marker size\n    plot_limit : int, optional\n        Default None. Limits to first N electrodes.\n    \"\"\"\n    if not plot_limit:\n        plot_limit = len(marks_indicators.electrodes)\n\n    for electrode_ind in marks_indicators.electrodes[:plot_limit]:\n        marks = (\n            marks_indicators.sel(electrodes=electrode_ind)\n            .dropna(\"time\", how=\"all\")\n            .dropna(\"marks\")\n        )\n        n_features = len(marks.marks)\n        fig, axes = plt.subplots(\n            n_features,\n            n_features,\n            constrained_layout=True,\n            sharex=True,\n            sharey=True,\n            figsize=(plot_size * n_features, plot_size * n_features),\n        )\n        for ax_ind1, feature1 in enumerate(marks.marks):\n            for ax_ind2, feature2 in enumerate(marks.marks):\n                try:\n                    axes[ax_ind1, ax_ind2].scatter(\n                        marks.sel(marks=feature1),\n                        marks.sel(marks=feature2),\n                        s=s,\n                    )\n                except TypeError:\n                    axes.scatter(\n                        marks.sel(marks=feature1),\n                        marks.sel(marks=feature2),\n                        s=s,\n                    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.ClusterlessClassifierParameters", "title": "<code>ClusterlessClassifierParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Decodes the animal's mental position and some category of interest from unclustered spikes and spike waveform features</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass ClusterlessClassifierParameters(dj.Manual):\n    \"\"\"Decodes the animal's mental position and some category of interest\n    from unclustered spikes and spike waveform features\n    \"\"\"\n\n    definition = \"\"\"\n    classifier_param_name : varchar(80) # a name for this set of parameters\n    ---\n    classifier_params :   BLOB    # initialization parameters\n    fit_params :          BLOB    # fit parameters\n    predict_params :      BLOB    # prediction parameters\n    \"\"\"\n\n    def insert_default(self):\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_cpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_cpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_gpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_gpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n    def insert1(self, key, **kwargs):\n        super().insert1(convert_classes_to_dict(key), **kwargs)\n\n    def fetch1(self, *args, **kwargs):\n        return restore_classes(super().fetch1(*args, **kwargs))\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MultiunitFiringRate", "title": "<code>MultiunitFiringRate</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Computes the population multiunit firing rate from the spikes in MarksIndicator.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MultiunitFiringRate(dj.Computed):\n    \"\"\"Computes the population multiunit firing rate from the spikes in\n    MarksIndicator.\"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarksIndicator\n    ---\n    -&gt; AnalysisNwbfile\n    multiunit_firing_rate_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        marks = (UnitMarksIndicator &amp; key).fetch_xarray()\n        multiunit_spikes = (np.any(~np.isnan(marks.values), axis=1)).astype(\n            float\n        )\n        multiunit_firing_rate = pd.DataFrame(\n            get_multiunit_population_firing_rate(\n                multiunit_spikes, key[\"sampling_rate\"]\n            ),\n            index=marks.time,\n            columns=[\"firing_rate\"],\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n\n        key[\n            \"multiunit_firing_rate_object_id\"\n        ] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=multiunit_firing_rate.reset_index(),\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [\n            data[\"multiunit_firing_rate\"].set_index(\"time\")\n            for data in self.fetch_nwb()\n        ]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MultiunitHighSynchronyEventsParameters", "title": "<code>MultiunitHighSynchronyEventsParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Parameters for extracting times of high mulitunit activity during immobility.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MultiunitHighSynchronyEventsParameters(dj.Manual):\n    \"\"\"Parameters for extracting times of high mulitunit activity during immobility.\"\"\"\n\n    definition = \"\"\"\n    param_name : varchar(80) # a name for this set of parameters\n    ---\n    minimum_duration = 0.015 :  float # minimum duration of event (in seconds)\n    zscore_threshold = 2.0 : float    # threshold event must cross to be considered (in std. dev.)\n    close_event_threshold = 0.0 :  float # events closer than this will be excluded (in seconds)\n    \"\"\"\n\n    def insert_default(self):\n        self.insert1(\n            {\n                \"param_name\": \"default\",\n                \"minimum_duration\": 0.015,\n                \"zscore_threshold\": 2.0,\n                \"close_event_threshold\": 0.0,\n            },\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MultiunitHighSynchronyEvents", "title": "<code>MultiunitHighSynchronyEvents</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Finds times of high mulitunit activity during immobility.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MultiunitHighSynchronyEvents(dj.Computed):\n    \"\"\"Finds times of high mulitunit activity during immobility.\"\"\"\n\n    definition = \"\"\"\n    -&gt; MultiunitHighSynchronyEventsParameters\n    -&gt; UnitMarksIndicator\n    -&gt; IntervalPositionInfo\n    ---\n    -&gt; AnalysisNwbfile\n    multiunit_hse_times_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        marks = (UnitMarksIndicator &amp; key).fetch_xarray()\n        multiunit_spikes = (np.any(~np.isnan(marks.values), axis=1)).astype(\n            float\n        )\n        position_info = (IntervalPositionInfo() &amp; key).fetch1_dataframe()\n\n        params = (MultiunitHighSynchronyEventsParameters &amp; key).fetch1()\n\n        multiunit_high_synchrony_times = multiunit_HSE_detector(\n            marks.time.values,\n            multiunit_spikes,\n            position_info.head_speed.values,\n            sampling_frequency=key[\"sampling_rate\"],\n            **params,\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n\n        key[\"multiunit_hse_times_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=multiunit_high_synchrony_times.reset_index(),\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.get_decoding_data_for_epoch", "title": "<code>get_decoding_data_for_epoch(nwb_file_name, interval_list_name, position_info_param_name='default_decoding', additional_mark_keys={})</code>", "text": "<p>Collects necessary data for decoding.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>interval_list_name</code> <code>str</code> required <code>position_info_param_name</code> <code>str</code> <code>'default_decoding'</code> <code>additional_mark_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>(DataFrame, shape(n_time, n_columns))</code> <code>marks</code> <code>(DataArray, shape(n_time, n_marks, n_electrodes))</code> <code>valid_slices</code> <code>list[slice]</code> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def get_decoding_data_for_epoch(\n    nwb_file_name: str,\n    interval_list_name: str,\n    position_info_param_name: str = \"default_decoding\",\n    additional_mark_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, xr.DataArray, list[slice]]:\n    \"\"\"Collects necessary data for decoding.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    interval_list_name : str\n    position_info_param_name : str, optional\n    additional_mark_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_columns)\n    marks : xr.DataArray, shape (n_time, n_marks, n_electrodes)\n    valid_slices : list[slice]\n\n    \"\"\"\n\n    valid_ephys_position_times_by_epoch = (\n        get_valid_ephys_position_times_by_epoch(nwb_file_name)\n    )\n    valid_ephys_position_times = valid_ephys_position_times_by_epoch[\n        interval_list_name\n    ]\n    valid_slices = convert_valid_times_to_slice(valid_ephys_position_times)\n    position_interval_name = (\n        convert_epoch_interval_name_to_position_interval_name(\n            {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": interval_list_name,\n            }\n        )\n    )\n\n    position_info = (\n        IntervalPositionInfo()\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": position_interval_name,\n            \"position_info_param_name\": position_info_param_name,\n        }\n    ).fetch1_dataframe()\n\n    position_info = pd.concat(\n        [position_info.loc[times] for times in valid_slices]\n    )\n\n    marks = (\n        (\n            UnitMarksIndicator()\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": position_interval_name,\n                **additional_mark_keys,\n            }\n        )\n    ).fetch_xarray()\n\n    marks = xr.concat(\n        [marks.sel(time=times) for times in valid_slices], dim=\"time\"\n    )\n\n    return position_info, marks, valid_slices\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.get_data_for_multiple_epochs", "title": "<code>get_data_for_multiple_epochs(nwb_file_name, epoch_names, position_info_param_name='default_decoding', additional_mark_keys={})</code>", "text": "<p>Collects necessary data for decoding multiple environments</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>epoch_names</code> <code>list[str]</code> required <code>position_info_param_name</code> <code>str</code> <code>'default_decoding'</code> <code>additional_mark_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>(DataFrame, shape(n_time, n_columns))</code> <code>marks</code> <code>(DataArray, shape(n_time, n_marks, n_electrodes))</code> <code>valid_slices</code> <code>dict[str, list[slice]]</code> <code>environment_labels</code> <code>(ndarray, shape(n_time))</code> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def get_data_for_multiple_epochs(\n    nwb_file_name: str,\n    epoch_names: list[str],\n    position_info_param_name=\"default_decoding\",\n    additional_mark_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, xr.DataArray, dict[str, list[slice]], np.ndarray]:\n    \"\"\"Collects necessary data for decoding multiple environments\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    epoch_names : list[str]\n    position_info_param_name : str, optional\n    additional_mark_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_columns)\n    marks : xr.DataArray, shape (n_time, n_marks, n_electrodes)\n    valid_slices : dict[str, list[slice]]\n    environment_labels : np.ndarray, shape (n_time,)\n\n    \"\"\"\n    data = []\n    environment_labels = []\n\n    for epoch in epoch_names:\n        data.append(\n            get_decoding_data_for_epoch(\n                nwb_file_name,\n                epoch,\n                position_info_param_name=position_info_param_name,\n                additional_mark_keys=additional_mark_keys,\n            )\n        )\n        n_time = data[-1][0].shape[0]\n        environment_labels.append([epoch] * n_time)\n\n    environment_labels = np.concatenate(environment_labels, axis=0)\n    position_info, marks, valid_slices = list(zip(*data))\n    position_info = pd.concat(position_info, axis=0)\n    marks = xr.concat(marks, dim=\"time\")\n    valid_slices = {\n        epoch: valid_slice\n        for epoch, valid_slice in zip(epoch_names, valid_slices)\n    }\n\n    assert position_info.shape[0] == marks.shape[0]\n\n    return position_info, marks, valid_slices, environment_labels\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.populate_mark_indicators", "title": "<code>populate_mark_indicators(spikesorting_selection_keys, mark_param_name='default', position_info_param_name='default_decoding')</code>", "text": "<p>Populate mark indicators for all units in the given spike sorting selection.</p> <p>This function is a way to do several pipeline steps at once. It will:   1. Populate the SpikeSortingSelection table   2. Populate the SpikeSorting table   3. Populate the Curation table   4. Populate the CuratedSpikeSortingSelection table   5. Populate UnitMarks   6. Compute UnitMarksIndicator for each position epoch</p> <p>Parameters:</p> Name Type Description Default <code>spikesorting_selection_keys</code> <code>dict</code> required <code>mark_param_name</code> <code>str</code> <code>'default'</code> <code>position_info_param_name</code> <code>str</code> <code>'default_decoding'</code> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def populate_mark_indicators(\n    spikesorting_selection_keys: dict,\n    mark_param_name: str = \"default\",\n    position_info_param_name: str = \"default_decoding\",\n):\n    \"\"\"Populate mark indicators for all units in the given spike sorting selection.\n\n    This function is a way to do several pipeline steps at once. It will:\n      1. Populate the SpikeSortingSelection table\n      2. Populate the SpikeSorting table\n      3. Populate the Curation table\n      4. Populate the CuratedSpikeSortingSelection table\n      5. Populate UnitMarks\n      6. Compute UnitMarksIndicator for each position epoch\n\n    Parameters\n    ----------\n    spikesorting_selection_keys : dict\n    mark_param_name : str, optional\n    position_info_param_name : str, optional\n    \"\"\"\n    spikesorting_selection_keys = deepcopy(spikesorting_selection_keys)\n    # Populate spike sorting\n    SpikeSortingSelection().insert(\n        spikesorting_selection_keys,\n        skip_duplicates=True,\n    )\n    SpikeSorting.populate(spikesorting_selection_keys)\n\n    # Skip any curation\n    curation_keys = [\n        Curation.insert_curation(key) for key in spikesorting_selection_keys\n    ]\n\n    CuratedSpikeSortingSelection().insert(curation_keys, skip_duplicates=True)\n    CuratedSpikeSorting.populate(CuratedSpikeSortingSelection() &amp; curation_keys)\n\n    # Populate marks\n    mark_parameters_keys = pd.DataFrame(CuratedSpikeSorting &amp; curation_keys)\n    mark_parameters_keys[\"mark_param_name\"] = mark_param_name\n    mark_parameters_keys = mark_parameters_keys.loc[\n        :, UnitMarkParameters.primary_key\n    ].to_dict(\"records\")\n    UnitMarkParameters().insert(mark_parameters_keys, skip_duplicates=True)\n    UnitMarks.populate(UnitMarkParameters &amp; mark_parameters_keys)\n\n    # Compute mark indicators for each position epoch\n    nwb_file_name = spikesorting_selection_keys[0][\"nwb_file_name\"]\n    position_interval_names = (\n        IntervalPositionInfo()\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"position_info_param_name\": position_info_param_name,\n        }\n    ).fetch(\"interval_list_name\")\n\n    for interval_name in tqdm(position_interval_names):\n        position_interval = IntervalList &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": interval_name,\n        }\n\n        marks_selection = (UnitMarks &amp; mark_parameters_keys) * position_interval\n        marks_selection = (\n            pd.DataFrame(marks_selection)\n            .loc[:, marks_selection.primary_key]\n            .to_dict(\"records\")\n        )\n        UnitMarksIndicatorSelection.insert(\n            marks_selection, skip_duplicates=True\n        )\n        UnitMarksIndicator.populate(marks_selection)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/", "title": "core.py", "text": ""}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.get_valid_ephys_position_times_from_interval", "title": "<code>get_valid_ephys_position_times_from_interval(interval_list_name, nwb_file_name)</code>", "text": "<p>Finds the intersection of the valid times for the interval list, the valid times for the ephys data, and the valid times for the position data.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list_name</code> <code>str</code> required <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>valid_ephys_position_times</code> <code>(ndarray, shape(n_valid_times, 2))</code> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def get_valid_ephys_position_times_from_interval(\n    interval_list_name: str, nwb_file_name: str\n) -&gt; np.ndarray:\n    \"\"\"Finds the intersection of the valid times for the interval list, the valid times for the ephys data,\n    and the valid times for the position data.\n\n    Parameters\n    ----------\n    interval_list_name : str\n    nwb_file_name : str\n\n    Returns\n    -------\n    valid_ephys_position_times : np.ndarray, shape (n_valid_times, 2)\n\n    \"\"\"\n    interval_valid_times = (\n        IntervalList\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": interval_list_name,\n        }\n    ).fetch1(\"valid_times\")\n\n    position_interval_names = (\n        RawPosition\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n        }\n    ).fetch(\"interval_list_name\")\n    position_interval_names = position_interval_names[\n        np.argsort(\n            [\n                int(name.strip(\"pos valid time\"))\n                for name in position_interval_names\n            ]\n        )\n    ]\n    valid_pos_times = [\n        (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": pos_interval_name,\n            }\n        ).fetch1(\"valid_times\")\n        for pos_interval_name in position_interval_names\n    ]\n\n    valid_ephys_times = (\n        IntervalList\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": \"raw data valid times\",\n        }\n    ).fetch1(\"valid_times\")\n\n    return interval_list_intersect(\n        interval_list_intersect(interval_valid_times, valid_ephys_times),\n        np.concatenate(valid_pos_times),\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.get_epoch_interval_names", "title": "<code>get_epoch_interval_names(nwb_file_name)</code>", "text": "<p>Find the interval names that are epochs.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>epoch_names</code> <code>list[str]</code> <p>List of interval names that are epochs.</p> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def get_epoch_interval_names(nwb_file_name: str) -&gt; list[str]:\n    \"\"\"Find the interval names that are epochs.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n\n    Returns\n    -------\n    epoch_names : list[str]\n        List of interval names that are epochs.\n    \"\"\"\n    interval_list = pd.DataFrame(\n        IntervalList() &amp; {\"nwb_file_name\": nwb_file_name}\n    )\n\n    interval_list = interval_list.loc[\n        interval_list.interval_list_name.str.contains(\n            r\"^(?:\\d+)_(?:\\w+)$\", regex=True, na=False\n        )\n    ]\n\n    return interval_list.interval_list_name.tolist()\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.get_valid_ephys_position_times_by_epoch", "title": "<code>get_valid_ephys_position_times_by_epoch(nwb_file_name)</code>", "text": "<p>Get the valid ephys position times for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>valid_ephys_position_times_by_epoch</code> <code>dict[str, ndarray]</code> <p>Dictionary of epoch names and valid ephys position times.</p> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def get_valid_ephys_position_times_by_epoch(\n    nwb_file_name: str,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Get the valid ephys position times for each epoch.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n\n    Returns\n    -------\n    valid_ephys_position_times_by_epoch : dict[str, np.ndarray]\n        Dictionary of epoch names and valid ephys position times.\n\n    \"\"\"\n    return {\n        epoch: get_valid_ephys_position_times_from_interval(\n            epoch, nwb_file_name\n        )\n        for epoch in get_epoch_interval_names(nwb_file_name)\n    }\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.convert_valid_times_to_slice", "title": "<code>convert_valid_times_to_slice(valid_times)</code>", "text": "<p>Converts the valid times to a list of slices so that arrays can be indexed easily.</p> <p>Parameters:</p> Name Type Description Default <code>valid_times</code> <code>(ndarray, shape(n_valid_times, 2))</code> <p>Start and end times for each valid time.</p> required <p>Returns:</p> Name Type Description <code>valid_time_slices</code> <code>list[slice]</code> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def convert_valid_times_to_slice(valid_times: np.ndarray) -&gt; list[slice]:\n    \"\"\"Converts the valid times to a list of slices so that arrays can be indexed easily.\n\n    Parameters\n    ----------\n    valid_times : np.ndarray, shape (n_valid_times, 2)\n        Start and end times for each valid time.\n\n    Returns\n    -------\n    valid_time_slices : list[slice]\n\n    \"\"\"\n    return [slice(times[0], times[1]) for times in valid_times]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.create_model_for_multiple_epochs", "title": "<code>create_model_for_multiple_epochs(epoch_names, env_kwargs)</code>", "text": "<p>Creates the observation model, environment, and continuous transition types for multiple epochs for decoding</p> <p>Parameters:</p> Name Type Description Default <code>epoch_names</code> <code>(list[str], length(n_epochs))</code> required <code>env_kwargs</code> <code>dict</code> <p>Environment keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>observation_models</code> <code>tuple[list[ObservationModel]</code> <p>Observation model for each epoch.</p> <code>environments</code> <code>list[Environment]</code> <p>Environment for each epoch.</p> <code>continuous_transition_types</code> <code>list[list[object]]]</code> <p>Continuous transition types for each epoch.</p> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def create_model_for_multiple_epochs(\n    epoch_names: list[str], env_kwargs: dict\n) -&gt; tuple[list[ObservationModel], list[Environment], list[list[object]]]:\n    \"\"\"Creates the observation model, environment, and continuous transition types for multiple epochs for decoding\n\n    Parameters\n    ----------\n    epoch_names : list[str], length (n_epochs)\n    env_kwargs : dict\n        Environment keyword arguments.\n\n    Returns\n    -------\n    observation_models: tuple[list[ObservationModel]\n        Observation model for each epoch.\n    environments : list[Environment]\n        Environment for each epoch.\n    continuous_transition_types : list[list[object]]]\n        Continuous transition types for each epoch.\n\n    \"\"\"\n    observation_models = []\n    environments = []\n    continuous_transition_types = []\n\n    for epoch in epoch_names:\n        observation_models.append(ObservationModel(epoch))\n        environments.append(Environment(epoch, **env_kwargs))\n\n    for epoch1 in epoch_names:\n        continuous_transition_types.append([])\n        for epoch2 in epoch_names:\n            if epoch1 == epoch2:\n                continuous_transition_types[-1].append(\n                    RandomWalk(epoch1, use_diffusion=False)\n                )\n            else:\n                continuous_transition_types[-1].append(Uniform(epoch1, epoch2))\n\n    return observation_models, environments, continuous_transition_types\n</code></pre>"}, {"location": "api/src/spyglass/decoding/dj_decoder_conversion/", "title": "dj_decoder_conversion.py", "text": "<p>Converts decoder classes into dictionaries and dictionaries into classes so that datajoint can store them in tables.</p>"}, {"location": "api/src/spyglass/decoding/dj_decoder_conversion/#src.spyglass.decoding.dj_decoder_conversion.restore_classes", "title": "<code>restore_classes(params)</code>", "text": "<p>Converts a dictionary of parameters into a dictionary of classes since datajoint cannot handle classes</p> Source code in <code>src/spyglass/decoding/dj_decoder_conversion.py</code> <pre><code>def restore_classes(params: dict) -&gt; dict:\n    \"\"\"Converts a dictionary of parameters into a dictionary of classes since datajoint cannot handle classes\"\"\"\n    continuous_state_transition_types = {\n        \"RandomWalk\": RandomWalk,\n        \"RandomWalkDirection1\": RandomWalkDirection1,\n        \"RandomWalkDirection2\": RandomWalkDirection2,\n        \"Uniform\": Uniform,\n        \"Identity\": Identity,\n    }\n\n    discrete_state_transition_types = {\n        \"DiagonalDiscrete\": DiagonalDiscrete,\n        \"UniformDiscrete\": UniformDiscrete,\n        \"RandomDiscrete\": RandomDiscrete,\n        \"UserDefinedDiscrete\": UserDefinedDiscrete,\n    }\n\n    initial_conditions_types = {\n        \"UniformInitialConditions\": UniformInitialConditions,\n        \"UniformOneEnvironmentInitialConditions\": UniformOneEnvironmentInitialConditions,\n    }\n\n    params[\"classifier_params\"][\"continuous_transition_types\"] = [\n        [\n            _convert_dict_to_class(st, continuous_state_transition_types)\n            for st in sts\n        ]\n        for sts in params[\"classifier_params\"][\"continuous_transition_types\"]\n    ]\n    params[\"classifier_params\"][\"environments\"] = [\n        _convert_env_dict(env_params)\n        for env_params in params[\"classifier_params\"][\"environments\"]\n    ]\n    params[\"classifier_params\"][\n        \"discrete_transition_type\"\n    ] = _convert_dict_to_class(\n        params[\"classifier_params\"][\"discrete_transition_type\"],\n        discrete_state_transition_types,\n    )\n    params[\"classifier_params\"][\n        \"initial_conditions_type\"\n    ] = _convert_dict_to_class(\n        params[\"classifier_params\"][\"initial_conditions_type\"],\n        initial_conditions_types,\n    )\n\n    if params[\"classifier_params\"][\"observation_models\"] is not None:\n        params[\"classifier_params\"][\"observation_models\"] = [\n            ObservationModel(obs)\n            for obs in params[\"classifier_params\"][\"observation_models\"]\n        ]\n\n    return params\n</code></pre>"}, {"location": "api/src/spyglass/decoding/dj_decoder_conversion/#src.spyglass.decoding.dj_decoder_conversion.convert_classes_to_dict", "title": "<code>convert_classes_to_dict(key)</code>", "text": "<p>Converts the classifier parameters into a dictionary so that datajoint can store it.</p> Source code in <code>src/spyglass/decoding/dj_decoder_conversion.py</code> <pre><code>def convert_classes_to_dict(key: dict) -&gt; dict:\n    \"\"\"Converts the classifier parameters into a dictionary so that datajoint can store it.\"\"\"\n    try:\n        key[\"classifier_params\"][\"environments\"] = [\n            _convert_environment_to_dict(env)\n            for env in key[\"classifier_params\"][\"environments\"]\n        ]\n    except TypeError:\n        key[\"classifier_params\"][\"environments\"] = [\n            _convert_environment_to_dict(\n                key[\"classifier_params\"][\"environments\"]\n            )\n        ]\n    key[\"classifier_params\"][\n        \"continuous_transition_types\"\n    ] = _convert_transitions_to_dict(\n        key[\"classifier_params\"][\"continuous_transition_types\"]\n    )\n    key[\"classifier_params\"][\"discrete_transition_type\"] = _to_dict(\n        key[\"classifier_params\"][\"discrete_transition_type\"]\n    )\n    key[\"classifier_params\"][\"initial_conditions_type\"] = _to_dict(\n        key[\"classifier_params\"][\"initial_conditions_type\"]\n    )\n\n    if key[\"classifier_params\"][\"observation_models\"] is not None:\n        key[\"classifier_params\"][\"observation_models\"] = [\n            vars(obs) for obs in key[\"classifier_params\"][\"observation_models\"]\n        ]\n\n    try:\n        key[\"classifier_params\"][\n            \"clusterless_algorithm_params\"\n        ] = _convert_algorithm_params(\n            key[\"classifier_params\"][\"clusterless_algorithm_params\"]\n        )\n    except KeyError:\n        pass\n\n    return key\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/", "title": "sorted_spikes.py", "text": "<p>Pipeline for decoding the animal's mental position and some category of interest from clustered spikes times. See [1] for details.</p>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes--references", "title": "References", "text": "<p>[1] Denovellis, E. L. et al. Hippocampal replay of experience at real-world speeds. eLife 10, e64505 (2021).</p>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesIndicatorSelection", "title": "<code>SortedSpikesIndicatorSelection</code>", "text": "<p>             Bases: <code>Lookup</code></p> <p>Bins spike times into regular intervals given by the sampling rate. Start and stop time of the interval are defined by the interval list.</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@schema\nclass SortedSpikesIndicatorSelection(dj.Lookup):\n    \"\"\"Bins spike times into regular intervals given by the sampling rate.\n    Start and stop time of the interval are defined by the interval list.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; CuratedSpikeSorting\n    -&gt; IntervalList\n    sampling_rate=500 : float\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesIndicator", "title": "<code>SortedSpikesIndicator</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Bins spike times into regular intervals given by the sampling rate. Useful for GLMs and for decoding.</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@schema\nclass SortedSpikesIndicator(dj.Computed):\n    \"\"\"Bins spike times into regular intervals given by the sampling rate.\n    Useful for GLMs and for decoding.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; SortedSpikesIndicatorSelection\n    ---\n    -&gt; AnalysisNwbfile\n    spike_indicator_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        pprint.pprint(key)\n        # TODO: intersection of sort interval and interval list\n        interval_times = (IntervalList &amp; key).fetch1(\"valid_times\")\n\n        sampling_rate = (SortedSpikesIndicatorSelection &amp; key).fetch(\n            \"sampling_rate\"\n        )\n\n        time = self.get_time_bins_from_interval(interval_times, sampling_rate)\n\n        spikes_nwb = (CuratedSpikeSorting &amp; key).fetch_nwb()\n        # restrict to cases with units\n        spikes_nwb = [entry for entry in spikes_nwb if \"units\" in entry]\n        spike_times_list = [\n            np.asarray(n_trode[\"units\"][\"spike_times\"])\n            for n_trode in spikes_nwb\n        ]\n        if len(spike_times_list) &gt; 0:  # if units\n            spikes = np.concatenate(spike_times_list)\n\n            # Bin spikes into time bins\n            spike_indicator = []\n            for spike_times in spikes:\n                spike_times = spike_times[\n                    (spike_times &gt; time[0]) &amp; (spike_times &lt;= time[-1])\n                ]\n                spike_indicator.append(\n                    np.bincount(\n                        np.digitize(spike_times, time[1:-1]),\n                        minlength=time.shape[0],\n                    )\n                )\n\n            column_names = np.concatenate(\n                [\n                    [\n                        f'{n_trode[\"sort_group_id\"]:04d}_{unit_number:04d}'\n                        for unit_number in n_trode[\"units\"].index\n                    ]\n                    for n_trode in spikes_nwb\n                ]\n            )\n            spike_indicator = pd.DataFrame(\n                np.stack(spike_indicator, axis=1),\n                index=pd.Index(time, name=\"time\"),\n                columns=column_names,\n            )\n\n            # Insert into analysis nwb file\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\"analysis_file_name\"] = nwb_analysis_file.create(\n                key[\"nwb_file_name\"]\n            )\n\n            key[\"spike_indicator_object_id\"] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=spike_indicator.reset_index(),\n            )\n\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n\n            self.insert1(key)\n\n    @staticmethod\n    def get_time_bins_from_interval(interval_times, sampling_rate):\n        \"\"\"Gets the superset of the interval.\"\"\"\n        start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n        n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n        return np.linspace(start_time, end_time, n_samples)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return pd.concat(\n            [\n                data[\"spike_indicator\"].set_index(\"time\")\n                for data in self.fetch_nwb()\n            ],\n            axis=1,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesIndicator.get_time_bins_from_interval", "title": "<code>get_time_bins_from_interval(interval_times, sampling_rate)</code>  <code>staticmethod</code>", "text": "<p>Gets the superset of the interval.</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@staticmethod\ndef get_time_bins_from_interval(interval_times, sampling_rate):\n    \"\"\"Gets the superset of the interval.\"\"\"\n    start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n    n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n    return np.linspace(start_time, end_time, n_samples)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesClassifierParameters", "title": "<code>SortedSpikesClassifierParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Stores parameters for decoding with sorted spikes</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@schema\nclass SortedSpikesClassifierParameters(dj.Manual):\n    \"\"\"Stores parameters for decoding with sorted spikes\"\"\"\n\n    definition = \"\"\"\n    classifier_param_name : varchar(80) # a name for this set of parameters\n    ---\n    classifier_params :   BLOB    # initialization parameters\n    fit_params :          BLOB    # fit parameters\n    predict_params :      BLOB    # prediction parameters\n    \"\"\"\n\n    def insert_default(self):\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_cpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_cpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_gpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_gpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n    def insert1(self, key, **kwargs):\n        super().insert1(convert_classes_to_dict(key), **kwargs)\n\n    def fetch1(self, *args, **kwargs):\n        return restore_classes(super().fetch1(*args, **kwargs))\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.get_spike_indicator", "title": "<code>get_spike_indicator(key, time_range, sampling_rate=500.0)</code>", "text": "<p>For a given key, returns a dataframe with the spike indicator for each unit</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> required <code>time_range</code> <code>tuple[float, float]</code> <p>Start and end time of the spike indicator</p> required <code>sampling_rate</code> <code>float</code> <code>500.0</code> <p>Returns:</p> Name Type Description <code>spike_indicator</code> <code>(DataFrame, shape(n_time, n_units))</code> <p>A dataframe with the spike indicator for each unit</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>def get_spike_indicator(\n    key: dict, time_range: tuple[float, float], sampling_rate: float = 500.0\n) -&gt; pd.DataFrame:\n    \"\"\"For a given key, returns a dataframe with the spike indicator for each unit\n\n    Parameters\n    ----------\n    key : dict\n    time_range : tuple[float, float]\n        Start and end time of the spike indicator\n    sampling_rate : float, optional\n\n    Returns\n    -------\n    spike_indicator : pd.DataFrame, shape (n_time, n_units)\n        A dataframe with the spike indicator for each unit\n    \"\"\"\n    start_time, end_time = time_range\n    n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n    time = np.linspace(start_time, end_time, n_samples)\n\n    spike_indicator = dict()\n    spikes_nwb_table = CuratedSpikeSorting() &amp; key\n\n    for n_trode in spikes_nwb_table.fetch_nwb():\n        try:\n            for unit_id, unit_spike_times in n_trode[\"units\"][\n                \"spike_times\"\n            ].items():\n                unit_spike_times = unit_spike_times[\n                    (unit_spike_times &gt; time[0])\n                    &amp; (unit_spike_times &lt;= time[-1])\n                ]\n                unit_name = f'{n_trode[\"sort_group_id\"]:04d}_{unit_id:04d}'\n                spike_indicator[unit_name] = np.bincount(\n                    np.digitize(unit_spike_times, time[1:-1]),\n                    minlength=time.shape[0],\n                )\n        except KeyError:\n            pass\n\n    return pd.DataFrame(\n        spike_indicator,\n        index=pd.Index(time, name=\"time\"),\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.get_decoding_data_for_epoch", "title": "<code>get_decoding_data_for_epoch(nwb_file_name, interval_list_name, position_info_param_name='default', additional_spike_keys={})</code>", "text": "<p>Collects the data needed for decoding</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>interval_list_name</code> <code>str</code> required <code>position_info_param_name</code> <code>str</code> <code>'default'</code> <code>additional_spike_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>(DataFrame, shape(n_time, n_position_features))</code> <code>spikes</code> <code>(DataFrame, shape(n_time, n_units))</code> <code>valid_slices</code> <code>list[slice]</code> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>def get_decoding_data_for_epoch(\n    nwb_file_name: str,\n    interval_list_name: str,\n    position_info_param_name: str = \"default\",\n    additional_spike_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, list[slice]]:\n    \"\"\"Collects the data needed for decoding\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    interval_list_name : str\n    position_info_param_name : str, optional\n    additional_spike_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_position_features)\n    spikes : pd.DataFrame, shape (n_time, n_units)\n    valid_slices : list[slice]\n\n    \"\"\"\n    # valid slices\n    valid_ephys_position_times_by_epoch = (\n        get_valid_ephys_position_times_by_epoch(nwb_file_name)\n    )\n    valid_ephys_position_times = valid_ephys_position_times_by_epoch[\n        interval_list_name\n    ]\n    valid_slices = convert_valid_times_to_slice(valid_ephys_position_times)\n\n    # position interval\n    position_interval_name = (\n        convert_epoch_interval_name_to_position_interval_name(\n            {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": interval_list_name,\n            }\n        )\n    )\n\n    # spikes\n    valid_times = np.asarray(\n        [(times.start, times.stop) for times in valid_slices]\n    )\n\n    curated_spikes_key = {\n        \"nwb_file_name\": nwb_file_name,\n        **additional_spike_keys,\n    }\n    spikes = get_spike_indicator(\n        curated_spikes_key,\n        (valid_times.min(), valid_times.max()),\n        sampling_rate=500,\n    )\n    spikes = pd.concat([spikes.loc[times] for times in valid_slices])\n\n    # position\n    position_info = (\n        IntervalPositionInfo()\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": position_interval_name,\n            \"position_info_param_name\": position_info_param_name,\n        }\n    ).fetch1_dataframe()\n    new_time = spikes.index.to_numpy()\n    new_index = pd.Index(\n        np.unique(np.concatenate((position_info.index, new_time))), name=\"time\"\n    )\n    position_info = (\n        position_info.reindex(index=new_index)\n        .interpolate(method=\"linear\")\n        .reindex(index=new_time)\n    )\n\n    return position_info, spikes, valid_slices\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.get_data_for_multiple_epochs", "title": "<code>get_data_for_multiple_epochs(nwb_file_name, epoch_names, position_info_param_name='decoding', additional_spike_keys={})</code>", "text": "<p>Collects the data needed for decoding for multiple epochs</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>epoch_names</code> <code>list</code> required <code>position_info_param_name</code> <code>str</code> <code>'decoding'</code> <code>additional_spike_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>(DataFrame, shape(n_time, n_position_features))</code> <code>spikes</code> <code>(DataFrame, shape(n_time, n_units))</code> <code>valid_slices</code> <code>list[slice]</code> <code>environment_labels</code> <code>(ndarray, shape(n_time))</code> <p>The environment label for each time point</p> <code>sort_group_ids</code> <code>(ndarray, shape(n_units))</code> <p>The sort group of each unit</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>def get_data_for_multiple_epochs(\n    nwb_file_name: str,\n    epoch_names: list,\n    position_info_param_name: str = \"decoding\",\n    additional_spike_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, list[slice], np.ndarray, np.ndarray]:\n    \"\"\"Collects the data needed for decoding for multiple epochs\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    epoch_names : list\n    position_info_param_name : str, optional\n    additional_spike_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_position_features)\n    spikes : pd.DataFrame, shape (n_time, n_units)\n    valid_slices : list[slice]\n    environment_labels : np.ndarray, shape (n_time,)\n        The environment label for each time point\n    sort_group_ids : np.ndarray, shape (n_units,)\n        The sort group of each unit\n    \"\"\"\n    data = []\n    environment_labels = []\n\n    for epoch in epoch_names:\n        print(epoch)\n        data.append(\n            get_decoding_data_for_epoch(\n                nwb_file_name,\n                epoch,\n                position_info_param_name=position_info_param_name,\n                additional_spike_keys=additional_spike_keys,\n            )\n        )\n        n_time = data[-1][0].shape[0]\n        environment_labels.append([epoch] * n_time)\n\n    environment_labels = np.concatenate(environment_labels, axis=0)\n    position_info, spikes, valid_slices = list(zip(*data))\n    position_info = pd.concat(position_info, axis=0)\n    spikes = pd.concat(spikes, axis=0)\n    valid_slices = {\n        epoch: valid_slice\n        for epoch, valid_slice in zip(epoch_names, valid_slices)\n    }\n\n    assert position_info.shape[0] == spikes.shape[0]\n\n    sort_group_ids = np.asarray(\n        [int(col.split(\"_\")[0]) for col in spikes.columns]\n    )\n\n    return (\n        position_info,\n        spikes,\n        valid_slices,\n        environment_labels,\n        sort_group_ids,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/visualization/", "title": "visualization.py", "text": ""}, {"location": "api/src/spyglass/decoding/visualization_1D_view/", "title": "visualization_1D_view.py", "text": ""}, {"location": "api/src/spyglass/decoding/visualization_1D_view/#src.spyglass.decoding.visualization_1D_view.create_1D_decode_view", "title": "<code>create_1D_decode_view(posterior, linear_position=None, ref_time_sec=None)</code>", "text": "<p>Creates a view of an interactive heatmap of position vs. time.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>(DataArray, shape(n_time, n_position_bins))</code> required <code>linear_position</code> <code>(ndarray, shape(n_time))</code> <code>None</code> <code>ref_time_sec</code> <code>float64</code> <p>Reference time for the purpose of offsetting the start time</p> <code>None</code> <p>Returns:</p> Name Type Description <code>view</code> <code>DecodedLinearPositionData</code> Source code in <code>src/spyglass/decoding/visualization_1D_view.py</code> <pre><code>def create_1D_decode_view(\n    posterior: xr.DataArray,\n    linear_position: np.ndarray = None,\n    ref_time_sec: Union[np.float64, None] = None,\n) -&gt; vvf.DecodedLinearPositionData:\n    \"\"\"Creates a view of an interactive heatmap of position vs. time.\n\n    Parameters\n    ----------\n    posterior : xr.DataArray, shape (n_time, n_position_bins)\n    linear_position : np.ndarray, shape (n_time, ), optional\n    ref_time_sec : np.float64, optional\n        Reference time for the purpose of offsetting the start time\n\n    Returns\n    -------\n    view : vvf.DecodedLinearPositionData\n\n    \"\"\"\n    if linear_position is not None:\n        linear_position = np.asarray(linear_position).squeeze()\n\n    trimmed_posterior = discretize_and_trim(posterior)\n    observations_per_time = get_observations_per_time(\n        trimmed_posterior, posterior\n    )\n    sampling_freq = get_sampling_freq(posterior.time)\n    start_time_sec = posterior.time.values[0]\n    if ref_time_sec is not None:\n        start_time_sec = start_time_sec - ref_time_sec\n\n    trimmed_bin_center_index = get_trimmed_bin_center_index(\n        posterior.position.values, trimmed_posterior.position.values\n    )\n\n    return vvf.DecodedLinearPositionData(\n        values=trimmed_posterior.values,\n        positions=trimmed_bin_center_index,\n        frame_bounds=observations_per_time,\n        positions_key=posterior.position.values.astype(np.float32),\n        observed_positions=linear_position,\n        start_time_sec=start_time_sec,\n        sampling_frequency=sampling_freq,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/visualization_2D_view/", "title": "visualization_2D_view.py", "text": ""}, {"location": "api/src/spyglass/decoding/visualization_2D_view/#src.spyglass.decoding.visualization_2D_view.create_2D_decode_view", "title": "<code>create_2D_decode_view(position_time, position, interior_place_bin_centers, place_bin_size, posterior, head_dir=None)</code>", "text": "<p>Creates a 2D decoding movie view</p> <p>Parameters:</p> Name Type Description Default <code>position_time</code> <code>(ndarray, shape(n_time))</code> required <code>position</code> <code>(ndarray, shape(n_time, 2))</code> required <code>interior_place_bin_centers</code> <code>ndarray</code> required <code>place_bin_size</code> <code>(ndarray, shape(2, 1))</code> required <code>posterior</code> <code>(DataArray, shape(n_time, n_position_bins))</code> required <code>head_dir</code> <code>ndarray</code> <code>None</code> <p>Returns:</p> Name Type Description <code>view</code> <code>TrackPositionAnimationV1</code> Source code in <code>src/spyglass/decoding/visualization_2D_view.py</code> <pre><code>def create_2D_decode_view(\n    position_time: np.ndarray,\n    position: np.ndarray,\n    interior_place_bin_centers: np.ndarray,\n    place_bin_size: np.ndarray,\n    posterior: xr.DataArray,\n    head_dir: np.ndarray = None,\n) -&gt; vvf.TrackPositionAnimationV1:\n    \"\"\"Creates a 2D decoding movie view\n\n    Parameters\n    ----------\n    position_time : np.ndarray, shape (n_time,)\n    position : np.ndarray, shape (n_time, 2)\n    interior_place_bin_centers: np.ndarray, shape (n_track_bins, 2)\n    place_bin_size : np.ndarray, shape (2, 1)\n    posterior : xr.DataArray, shape (n_time, n_position_bins)\n    head_dir : np.ndarray, optional\n\n    Returns\n    -------\n    view : vvf.TrackPositionAnimationV1\n\n    \"\"\"\n    assert (\n        position_time.shape[0] == position.shape[0]\n    ), \"position_time and position must have the same length\"\n    assert (\n        posterior.shape[0] == position.shape[0]\n    ), \"posterior and position must have the same length\"\n\n    position_time = np.squeeze(np.asarray(position_time)).copy()\n    position = np.asarray(position)\n    if head_dir is not None:\n        head_dir = np.squeeze(np.asarray(head_dir))\n\n    track_bin_width = place_bin_size[0]\n    track_bin_height = place_bin_size[1]\n    # NOTE: We expect caller to have converted from fortran ordering already\n    # i.e. somewhere upstream, centers = env.place_bin_centers_[env.is_track_interior_.ravel(order=\"F\")]\n    upper_left_points = get_ul_corners(\n        track_bin_width, track_bin_height, interior_place_bin_centers\n    )\n\n    data = create_static_track_animation(\n        ul_corners=upper_left_points,\n        track_rect_height=track_bin_height,\n        track_rect_width=track_bin_width,\n        timestamps=position_time,\n        positions=position.T,\n        head_dir=head_dir,\n        compute_real_time_rate=True,\n    )\n    data[\"decodedData\"] = process_decoded_data(posterior)\n\n    return create_track_animation_object(static_track_animation=data)\n</code></pre>"}, {"location": "api/src/spyglass/figurl_views/SpikeSortingRecordingView/", "title": "SpikeSortingRecordingView.py", "text": ""}, {"location": "api/src/spyglass/figurl_views/SpikeSortingView/", "title": "SpikeSortingView.py", "text": ""}, {"location": "api/src/spyglass/figurl_views/prepare_spikesortingview_data/", "title": "prepare_spikesortingview_data.py", "text": ""}, {"location": "api/src/spyglass/lfp/lfp_electrode/", "title": "lfp_electrode.py", "text": ""}, {"location": "api/src/spyglass/lfp/lfp_electrode/#src.spyglass.lfp.lfp_electrode.LFPElectrodeGroup", "title": "<code>LFPElectrodeGroup</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/lfp/lfp_electrode.py</code> <pre><code>@schema\nclass LFPElectrodeGroup(dj.Manual):\n    definition = \"\"\"\n     -&gt; Session                             # the session to which this LFP belongs\n     lfp_electrode_group_name: varchar(200) # the name of this group of electrodes\n     \"\"\"\n\n    class LFPElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPElectrodeGroup # the group of electrodes to be filtered\n        -&gt; Electrode        # the electrode to be filtered\n        \"\"\"\n\n    @staticmethod\n    def create_lfp_electrode_group(\n        nwb_file_name: str, group_name: str, electrode_list: list[int]\n    ):\n        \"\"\"Adds an LFPElectrodeGroup and the individual electrodes\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the nwb file (e.g. the session)\n        group_name : str\n            The name of this group (&lt; 200 char)\n        electrode_list : list\n            A list of the electrode ids to include in this group.\n        \"\"\"\n        # remove the session and then recreate the session and Electrode list\n        # check to see if the user allowed the deletion\n        key = {\n            \"nwb_file_name\": nwb_file_name,\n            \"lfp_electrode_group_name\": group_name,\n        }\n        LFPElectrodeGroup().insert1(key, skip_duplicates=True)\n\n        # TODO: do this in a better way\n        all_electrodes = (Electrode() &amp; {\"nwb_file_name\": nwb_file_name}).fetch(\n            as_dict=True\n        )\n        primary_key = Electrode.primary_key\n        for e in all_electrodes:\n            # create a dictionary so we can insert the electrodes\n            if e[\"electrode_id\"] in electrode_list:\n                lfpelectdict = {k: v for k, v in e.items() if k in primary_key}\n                lfpelectdict[\"lfp_electrode_group_name\"] = group_name\n                LFPElectrodeGroup().LFPElectrode.insert1(\n                    lfpelectdict, skip_duplicates=True\n                )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/lfp_electrode/#src.spyglass.lfp.lfp_electrode.LFPElectrodeGroup.create_lfp_electrode_group", "title": "<code>create_lfp_electrode_group(nwb_file_name, group_name, electrode_list)</code>  <code>staticmethod</code>", "text": "<p>Adds an LFPElectrodeGroup and the individual electrodes</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the nwb file (e.g. the session)</p> required <code>group_name</code> <code>str</code> <p>The name of this group (&lt; 200 char)</p> required <code>electrode_list</code> <code>list</code> <p>A list of the electrode ids to include in this group.</p> required Source code in <code>src/spyglass/lfp/lfp_electrode.py</code> <pre><code>@staticmethod\ndef create_lfp_electrode_group(\n    nwb_file_name: str, group_name: str, electrode_list: list[int]\n):\n    \"\"\"Adds an LFPElectrodeGroup and the individual electrodes\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the nwb file (e.g. the session)\n    group_name : str\n        The name of this group (&lt; 200 char)\n    electrode_list : list\n        A list of the electrode ids to include in this group.\n    \"\"\"\n    # remove the session and then recreate the session and Electrode list\n    # check to see if the user allowed the deletion\n    key = {\n        \"nwb_file_name\": nwb_file_name,\n        \"lfp_electrode_group_name\": group_name,\n    }\n    LFPElectrodeGroup().insert1(key, skip_duplicates=True)\n\n    # TODO: do this in a better way\n    all_electrodes = (Electrode() &amp; {\"nwb_file_name\": nwb_file_name}).fetch(\n        as_dict=True\n    )\n    primary_key = Electrode.primary_key\n    for e in all_electrodes:\n        # create a dictionary so we can insert the electrodes\n        if e[\"electrode_id\"] in electrode_list:\n            lfpelectdict = {k: v for k, v in e.items() if k in primary_key}\n            lfpelectdict[\"lfp_electrode_group_name\"] = group_name\n            LFPElectrodeGroup().LFPElectrode.insert1(\n                lfpelectdict, skip_duplicates=True\n            )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/lfp_imported/", "title": "lfp_imported.py", "text": ""}, {"location": "api/src/spyglass/lfp/lfp_merge/", "title": "lfp_merge.py", "text": ""}, {"location": "api/src/spyglass/lfp/lfp_merge/#src.spyglass.lfp.lfp_merge.LFPOutput", "title": "<code>LFPOutput</code>", "text": "<p>             Bases: <code>_Merge</code></p> Source code in <code>src/spyglass/lfp/lfp_merge.py</code> <pre><code>@schema\nclass LFPOutput(_Merge):\n    definition = \"\"\"\n    merge_id: uuid\n    ---\n    source: varchar(32)\n    \"\"\"\n\n    class LFPV1(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; LFPV1\n        \"\"\"\n\n    class ImportedLFP(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; ImportedLFP\n        \"\"\"\n\n    class CommonLFP(dj.Part):\n        \"\"\"Table to pass-through legacy LFP\"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; CommonLFP\n        \"\"\"\n\n    def fetch1_dataframe(self, *attrs, **kwargs):\n        # Note: `proj` below facilitates operator syntax eg Table &amp; restrict\n        nwb_lfp = self.fetch_nwb(self.proj())[0]\n        return pd.DataFrame(\n            nwb_lfp[\"lfp\"].data,\n            index=pd.Index(nwb_lfp[\"lfp\"].timestamps, name=\"time\"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/lfp_merge/#src.spyglass.lfp.lfp_merge.LFPOutput.CommonLFP", "title": "<code>CommonLFP</code>", "text": "<p>             Bases: <code>Part</code></p> <p>Table to pass-through legacy LFP</p> Source code in <code>src/spyglass/lfp/lfp_merge.py</code> <pre><code>class CommonLFP(dj.Part):\n    \"\"\"Table to pass-through legacy LFP\"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    ---\n    -&gt; CommonLFP\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/", "title": "lfp_band.py", "text": ""}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/#src.spyglass.lfp.analysis.v1.lfp_band.LFPBandSelection", "title": "<code>LFPBandSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>The user's selection of LFP data to be filtered in a given frequency band.</p> Source code in <code>src/spyglass/lfp/analysis/v1/lfp_band.py</code> <pre><code>@schema\nclass LFPBandSelection(dj.Manual):\n    \"\"\"The user's selection of LFP data to be filtered in a given frequency band.\"\"\"\n\n    definition = \"\"\"\n    -&gt; LFPOutput.proj(lfp_merge_id='merge_id')                                # the LFP data to be filtered\n    -&gt; FirFilterParameters                                                # the filter to use for the data\n    -&gt; IntervalList.proj(target_interval_list_name='interval_list_name')  # the original set of times to be filtered\n    lfp_band_sampling_rate: int                                           # the sampling rate for this band\n    ---\n    min_interval_len = 1.0: float  # the minimum length of a valid interval to filter\n    \"\"\"\n\n    class LFPBandElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPBandSelection # the LFP band selection\n        -&gt; LFPElectrodeGroup.LFPElectrode  # the LFP electrode to be filtered\n        reference_elect_id = -1: int  # the reference electrode to use; -1 for no reference\n        ---\n        \"\"\"\n\n    def set_lfp_band_electrodes(\n        self,\n        nwb_file_name: str,\n        lfp_merge_id: int,\n        electrode_list: list[int],\n        filter_name: str,\n        interval_list_name: str,\n        reference_electrode_list: list[int],\n        lfp_band_sampling_rate: int,\n    ):\n        \"\"\"Sets the electrodes to be filtered for a given LFP\n\n        Parameters\n        ----------\n        nwb_file_name: str\n            The name of the NWB file containing the LFP data\n        lfp_merge_id: int\n            The uuid of the LFP data to be filtered\n        electrode_list: list\n            A list of the electrodes to be filtered\n        filter_name: str\n            The name of the filter to be used\n        interval_list_name: str\n            The name of the interval list to be used\n        reference_electrode_list: list\n            A list of the reference electrodes to be used\n        lfp_band_sampling_rate: int\n        \"\"\"\n        # Error checks on parameters\n        # electrode_list\n\n        lfp_key = {\"merge_id\": lfp_merge_id}\n        lfp_part_table = LFPOutput.merge_get_part(lfp_key)\n\n        query = LFPElectrodeGroup().LFPElectrode() &amp; lfp_key\n        available_electrodes = query.fetch(\"electrode_id\")\n        if not np.all(np.isin(electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in electrode_list must be valid electrode_ids in the LFPElectodeGroup table\"\n            )\n        # sampling rate\n        lfp_sampling_rate = LFPOutput.merge_get_parent(lfp_key).fetch1(\n            \"lfp_sampling_rate\"\n        )\n        decimation = lfp_sampling_rate // lfp_band_sampling_rate\n        if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n            raise ValueError(\n                f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n                f\"samping rate {lfp_sampling_rate}\"\n            )\n        # filter\n        filter_query = FirFilterParameters() &amp; {\n            \"filter_name\": filter_name,\n            \"filter_sampling_rate\": lfp_sampling_rate,\n        }\n        if not filter_query:\n            raise ValueError(\n                f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n            )\n        # interval_list\n        interval_query = IntervalList() &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_name\": interval_list_name,\n        }\n        if not interval_query:\n            raise ValueError(\n                f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n                \"added before this function is called\"\n            )\n        # reference_electrode_list\n        if len(reference_electrode_list) != 1 and len(\n            reference_electrode_list\n        ) != len(electrode_list):\n            raise ValueError(\n                \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n            )\n        # add a -1 element to the list to allow for the no reference option\n        available_electrodes = np.append(available_electrodes, [-1])\n        if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n                \"table\"\n            )\n\n        # make a list of all the references\n        ref_list = np.zeros((len(electrode_list),))\n        ref_list[:] = reference_electrode_list\n\n        key = dict(\n            nwb_file_name=nwb_file_name,\n            lfp_merge_id=lfp_merge_id,\n            filter_name=filter_name,\n            filter_sampling_rate=lfp_sampling_rate,\n            target_interval_list_name=interval_list_name,\n            lfp_band_sampling_rate=lfp_sampling_rate // decimation,\n        )\n        # insert an entry into the main LFPBandSelectionTable\n        self.insert1(key, skip_duplicates=True)\n\n        key[\"lfp_electrode_group_name\"] = lfp_part_table.fetch1(\n            \"lfp_electrode_group_name\"\n        )\n        # iterate through all of the new elements and add them\n        for e, r in zip(electrode_list, ref_list):\n            elect_key = (\n                LFPElectrodeGroup.LFPElectrode\n                &amp; {\n                    \"nwb_file_name\": nwb_file_name,\n                    \"lfp_electrode_group_name\": key[\"lfp_electrode_group_name\"],\n                    \"electrode_id\": e,\n                }\n            ).fetch1(\"KEY\")\n            for item in elect_key:\n                key[item] = elect_key[item]\n            query = Electrode &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"electrode_id\": e,\n            }\n            key[\"reference_elect_id\"] = r\n            self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/#src.spyglass.lfp.analysis.v1.lfp_band.LFPBandSelection.set_lfp_band_electrodes", "title": "<code>set_lfp_band_electrodes(nwb_file_name, lfp_merge_id, electrode_list, filter_name, interval_list_name, reference_electrode_list, lfp_band_sampling_rate)</code>", "text": "<p>Sets the electrodes to be filtered for a given LFP</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file containing the LFP data</p> required <code>lfp_merge_id</code> <code>int</code> <p>The uuid of the LFP data to be filtered</p> required <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to be filtered</p> required <code>filter_name</code> <code>str</code> <p>The name of the filter to be used</p> required <code>interval_list_name</code> <code>str</code> <p>The name of the interval list to be used</p> required <code>reference_electrode_list</code> <code>list[int]</code> <p>A list of the reference electrodes to be used</p> required <code>lfp_band_sampling_rate</code> <code>int</code> required Source code in <code>src/spyglass/lfp/analysis/v1/lfp_band.py</code> <pre><code>def set_lfp_band_electrodes(\n    self,\n    nwb_file_name: str,\n    lfp_merge_id: int,\n    electrode_list: list[int],\n    filter_name: str,\n    interval_list_name: str,\n    reference_electrode_list: list[int],\n    lfp_band_sampling_rate: int,\n):\n    \"\"\"Sets the electrodes to be filtered for a given LFP\n\n    Parameters\n    ----------\n    nwb_file_name: str\n        The name of the NWB file containing the LFP data\n    lfp_merge_id: int\n        The uuid of the LFP data to be filtered\n    electrode_list: list\n        A list of the electrodes to be filtered\n    filter_name: str\n        The name of the filter to be used\n    interval_list_name: str\n        The name of the interval list to be used\n    reference_electrode_list: list\n        A list of the reference electrodes to be used\n    lfp_band_sampling_rate: int\n    \"\"\"\n    # Error checks on parameters\n    # electrode_list\n\n    lfp_key = {\"merge_id\": lfp_merge_id}\n    lfp_part_table = LFPOutput.merge_get_part(lfp_key)\n\n    query = LFPElectrodeGroup().LFPElectrode() &amp; lfp_key\n    available_electrodes = query.fetch(\"electrode_id\")\n    if not np.all(np.isin(electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in electrode_list must be valid electrode_ids in the LFPElectodeGroup table\"\n        )\n    # sampling rate\n    lfp_sampling_rate = LFPOutput.merge_get_parent(lfp_key).fetch1(\n        \"lfp_sampling_rate\"\n    )\n    decimation = lfp_sampling_rate // lfp_band_sampling_rate\n    if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n        raise ValueError(\n            f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n            f\"samping rate {lfp_sampling_rate}\"\n        )\n    # filter\n    filter_query = FirFilterParameters() &amp; {\n        \"filter_name\": filter_name,\n        \"filter_sampling_rate\": lfp_sampling_rate,\n    }\n    if not filter_query:\n        raise ValueError(\n            f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n        )\n    # interval_list\n    interval_query = IntervalList() &amp; {\n        \"nwb_file_name\": nwb_file_name,\n        \"interval_name\": interval_list_name,\n    }\n    if not interval_query:\n        raise ValueError(\n            f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n            \"added before this function is called\"\n        )\n    # reference_electrode_list\n    if len(reference_electrode_list) != 1 and len(\n        reference_electrode_list\n    ) != len(electrode_list):\n        raise ValueError(\n            \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n        )\n    # add a -1 element to the list to allow for the no reference option\n    available_electrodes = np.append(available_electrodes, [-1])\n    if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n            \"table\"\n        )\n\n    # make a list of all the references\n    ref_list = np.zeros((len(electrode_list),))\n    ref_list[:] = reference_electrode_list\n\n    key = dict(\n        nwb_file_name=nwb_file_name,\n        lfp_merge_id=lfp_merge_id,\n        filter_name=filter_name,\n        filter_sampling_rate=lfp_sampling_rate,\n        target_interval_list_name=interval_list_name,\n        lfp_band_sampling_rate=lfp_sampling_rate // decimation,\n    )\n    # insert an entry into the main LFPBandSelectionTable\n    self.insert1(key, skip_duplicates=True)\n\n    key[\"lfp_electrode_group_name\"] = lfp_part_table.fetch1(\n        \"lfp_electrode_group_name\"\n    )\n    # iterate through all of the new elements and add them\n    for e, r in zip(electrode_list, ref_list):\n        elect_key = (\n            LFPElectrodeGroup.LFPElectrode\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"lfp_electrode_group_name\": key[\"lfp_electrode_group_name\"],\n                \"electrode_id\": e,\n            }\n        ).fetch1(\"KEY\")\n        for item in elect_key:\n            key[item] = elect_key[item]\n        query = Electrode &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"electrode_id\": e,\n        }\n        key[\"reference_elect_id\"] = r\n        self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/#src.spyglass.lfp.analysis.v1.lfp_band.LFPBandV1", "title": "<code>LFPBandV1</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/lfp/analysis/v1/lfp_band.py</code> <pre><code>@schema\nclass LFPBandV1(dj.Computed):\n    definition = \"\"\"\n    -&gt; LFPBandSelection              # the LFP band selection\n    ---\n    -&gt; AnalysisNwbfile               # the name of the nwb file with the lfp data\n    -&gt; IntervalList                  # the final interval list of valid times for the data\n    lfp_band_object_id: varchar(40)  # the NWB object ID for loading this object from the file\n    \"\"\"\n\n    def make(self, key):\n        # get the NWB object with the lfp data; FIX: change to fetch with additional infrastructure\n        lfp_key = {\"merge_id\": key[\"lfp_merge_id\"]}\n        lfp_object = LFPOutput.fetch_nwb(lfp_key)[0][\"lfp\"]\n\n        # get the electrodes to be filtered and their references\n        lfp_band_elect_id, lfp_band_ref_id = (\n            LFPBandSelection().LFPBandElectrode() &amp; key\n        ).fetch(\"electrode_id\", \"reference_elect_id\")\n\n        # sort the electrodes to make sure they are in ascending order\n        lfp_band_elect_id = np.asarray(lfp_band_elect_id)\n        lfp_band_ref_id = np.asarray(lfp_band_ref_id)\n        lfp_sort_order = np.argsort(lfp_band_elect_id)\n        lfp_band_elect_id = lfp_band_elect_id[lfp_sort_order]\n        lfp_band_ref_id = lfp_band_ref_id[lfp_sort_order]\n\n        lfp_sampling_rate, lfp_interval_list = LFPOutput.merge_get_parent(\n            lfp_key\n        ).fetch1(\"lfp_sampling_rate\", \"interval_list_name\")\n        interval_list_name, lfp_band_sampling_rate = (\n            LFPBandSelection() &amp; key\n        ).fetch1(\"target_interval_list_name\", \"lfp_band_sampling_rate\")\n        valid_times = (\n            IntervalList()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        # the valid_times for this interval may be slightly beyond the valid times for the lfp itself,\n        # so we have to intersect the two lists\n        lfp_valid_times = (\n            IntervalList()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": lfp_interval_list,\n            }\n        ).fetch1(\"valid_times\")\n        min_length = (LFPBandSelection &amp; key).fetch1(\"min_interval_len\")\n        lfp_band_valid_times = interval_list_intersect(\n            valid_times, lfp_valid_times, min_length=min_length\n        )\n\n        filter_name, filter_sampling_rate, lfp_band_sampling_rate = (\n            LFPBandSelection() &amp; key\n        ).fetch1(\n            \"filter_name\", \"filter_sampling_rate\", \"lfp_band_sampling_rate\"\n        )\n\n        decimation = int(lfp_sampling_rate) // lfp_band_sampling_rate\n\n        # load in the timestamps\n        timestamps = np.asarray(lfp_object.timestamps)\n        # get the indices of the first timestamp and the last timestamp that are within the valid times\n        included_indices = interval_list_contains_ind(\n            lfp_band_valid_times, timestamps\n        )\n        # pad the indices by 1 on each side to avoid message in filter_data\n        if included_indices[0] &gt; 0:\n            included_indices[0] -= 1\n        if included_indices[-1] != len(timestamps) - 1:\n            included_indices[-1] += 1\n\n        timestamps = timestamps[included_indices[0] : included_indices[-1]]\n\n        # load all the data to speed filtering\n        lfp_data = np.asarray(\n            lfp_object.data[included_indices[0] : included_indices[-1], :],\n            dtype=type(lfp_object.data[0][0]),\n        )\n\n        # get the indices of the electrodes to be filtered and the references\n        lfp_band_elect_index = get_electrode_indices(\n            lfp_object, lfp_band_elect_id\n        )\n        lfp_band_ref_index = get_electrode_indices(lfp_object, lfp_band_ref_id)\n\n        # subtract off the references for the selected channels\n        lfp_data_original = lfp_data.copy()\n        for index, elect_index in enumerate(lfp_band_elect_index):\n            if lfp_band_ref_id[index] != -1:\n                lfp_data[:, elect_index] = (\n                    lfp_data_original[:, elect_index]\n                    - lfp_data_original[:, lfp_band_ref_index[index]]\n                )\n\n        # get the LFP filter that matches the raw data\n        filter = (\n            FirFilterParameters()\n            &amp; {\"filter_name\": filter_name}\n            &amp; {\"filter_sampling_rate\": filter_sampling_rate}\n        ).fetch(as_dict=True)\n        if len(filter) == 0:\n            raise ValueError(\n                f\"Filter {filter_name} and sampling_rate {lfp_band_sampling_rate} does not exit in the \"\n                \"FirFilterParameters table\"\n            )\n\n        filter_coeff = filter[0][\"filter_coeff\"]\n        if len(filter_coeff) == 0:\n            print(\n                f\"Error in LFPBand: no filter found with data sampling rate of {lfp_band_sampling_rate}\"\n            )\n            return None\n\n        # create the analysis nwb file to store the results.\n        lfp_band_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n        lfp_band_file_abspath = AnalysisNwbfile().get_abs_path(\n            lfp_band_file_name\n        )\n        # filter the data and write to an the nwb file\n        filtered_data, new_timestamps = FirFilterParameters().filter_data(\n            timestamps,\n            lfp_data,\n            filter_coeff,\n            lfp_band_valid_times,\n            lfp_band_elect_index,\n            decimation,\n        )\n\n        # now that the LFP is filtered, we create an electrical series for it and add it to the file\n        with pynwb.NWBHDF5IO(\n            path=lfp_band_file_abspath, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the indices of the electrodes in the electrode table of the file to get the right values\n            elect_index = get_electrode_indices(nwbf, lfp_band_elect_id)\n            electrode_table_region = nwbf.create_electrode_table_region(\n                elect_index, \"filtered electrode table\"\n            )\n            eseries_name = \"filtered data\"\n            # TODO: use datatype of data\n            es = pynwb.ecephys.ElectricalSeries(\n                name=eseries_name,\n                data=filtered_data,\n                electrodes=electrode_table_region,\n                timestamps=new_timestamps,\n            )\n            lfp = pynwb.ecephys.LFP(electrical_series=es)\n            ecephys_module = nwbf.create_processing_module(\n                name=\"ecephys\",\n                description=f\"LFP data processed with {filter_name}\",\n            )\n            ecephys_module.add(lfp)\n            io.write(nwbf)\n            lfp_band_object_id = es.object_id\n        #\n        # add the file to the AnalysisNwbfile table\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], lfp_band_file_name)\n        key[\"analysis_file_name\"] = lfp_band_file_name\n        key[\"lfp_band_object_id\"] = lfp_band_object_id\n\n        # finally, we need to censor the valid times to account for the downsampling if this is the first time we've\n        # downsampled these data\n        key[\"interval_list_name\"] = (\n            interval_list_name\n            + \" lfp band \"\n            + str(lfp_band_sampling_rate)\n            + \"Hz\"\n        )\n        tmp_valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch(\"valid_times\")\n        if len(tmp_valid_times) == 0:\n            lfp_band_valid_times = interval_list_censor(\n                lfp_band_valid_times, new_timestamps\n            )\n            # add an interval list for the LFP valid times\n            IntervalList.insert1(\n                {\n                    \"nwb_file_name\": key[\"nwb_file_name\"],\n                    \"interval_list_name\": key[\"interval_list_name\"],\n                    \"valid_times\": lfp_band_valid_times,\n                }\n            )\n        else:\n            # check that the valid times are the same\n            assert np.isclose(\n                tmp_valid_times[0], lfp_band_valid_times\n            ).all(), (\n                \"previously saved lfp band times do not match current times\"\n            )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self, *attrs, **kwargs):\n        \"\"\"Fetches the filtered data as a dataframe\"\"\"\n        filtered_nwb = self.fetch_nwb()[0]\n        return pd.DataFrame(\n            filtered_nwb[\"lfp_band\"].data,\n            index=pd.Index(filtered_nwb[\"lfp_band\"].timestamps, name=\"time\"),\n        )\n\n    def compute_analytic_signal(self, electrode_list: list[int], **kwargs):\n        \"\"\"Computes the hilbert transform of a given LFPBand signal using scipy.signal.hilbert\n\n        Parameters\n        ----------\n        electrode_list: list[int]\n            A list of the electrodes to compute the hilbert transform of\n\n        Returns\n        -------\n        analytic_signal_df: pd.DataFrame\n            DataFrame containing hilbert transform of signal\n\n        Raises\n        ------\n        ValueError\n            If any electrodes passed to electrode_list are invalid for the dataset\n        \"\"\"\n\n        filtered_band = self.fetch_nwb()[0][\"lfp_band\"]\n        electrode_index = np.isin(\n            filtered_band.electrodes.data[:], electrode_list\n        )\n        if len(electrode_list) != np.sum(electrode_index):\n            raise ValueError(\n                \"Some of the electrodes specified in electrode_list are missing in the current LFPBand table.\"\n            )\n        analytic_signal_df = pd.DataFrame(\n            hilbert(filtered_band.data[:, electrode_index], axis=0),\n            index=pd.Index(filtered_band.timestamps, name=\"time\"),\n            columns=[f\"electrode {e}\" for e in electrode_list],\n        )\n        return analytic_signal_df\n\n    def compute_signal_phase(\n        self, electrode_list: list[int] = None, **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"Computes the phase of a given LFPBand signals using the hilbert transform\n\n        Parameters\n        ----------\n        electrode_list : list[int], optional\n            A list of the electrodes to compute the phase of, by default None\n\n        Returns\n        -------\n        signal_phase_df : pd.DataFrame\n            DataFrame containing the phase of the signals\n        \"\"\"\n        if electrode_list is None:\n            electrode_list = []\n\n        analytic_signal_df = self.compute_analytic_signal(\n            electrode_list, **kwargs\n        )\n\n        return pd.DataFrame(\n            np.angle(analytic_signal_df) + np.pi,\n            columns=analytic_signal_df.columns,\n            index=analytic_signal_df.index,\n        )\n\n    def compute_signal_power(\n        self, electrode_list: list[int] = None, **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"Computes the power of a given LFPBand signals using the hilbert transform\n\n        Parameters\n        ----------\n        electrode_list : list[int], optional\n            A list of the electrodes to compute the power of, by default None\n\n        Returns\n        -------\n        signal_power_df : pd.DataFrame\n            DataFrame containing the power of the signals\n        \"\"\"\n        if electrode_list is None:\n            electrode_list = []\n\n        analytic_signal_df = self.compute_analytic_signal(\n            electrode_list, **kwargs\n        )\n\n        return pd.DataFrame(\n            np.abs(analytic_signal_df) ** 2,\n            columns=analytic_signal_df.columns,\n            index=analytic_signal_df.index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/#src.spyglass.lfp.analysis.v1.lfp_band.LFPBandV1.fetch1_dataframe", "title": "<code>fetch1_dataframe(*attrs, **kwargs)</code>", "text": "<p>Fetches the filtered data as a dataframe</p> Source code in <code>src/spyglass/lfp/analysis/v1/lfp_band.py</code> <pre><code>def fetch1_dataframe(self, *attrs, **kwargs):\n    \"\"\"Fetches the filtered data as a dataframe\"\"\"\n    filtered_nwb = self.fetch_nwb()[0]\n    return pd.DataFrame(\n        filtered_nwb[\"lfp_band\"].data,\n        index=pd.Index(filtered_nwb[\"lfp_band\"].timestamps, name=\"time\"),\n    )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/#src.spyglass.lfp.analysis.v1.lfp_band.LFPBandV1.compute_analytic_signal", "title": "<code>compute_analytic_signal(electrode_list, **kwargs)</code>", "text": "<p>Computes the hilbert transform of a given LFPBand signal using scipy.signal.hilbert</p> <p>Parameters:</p> Name Type Description Default <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to compute the hilbert transform of</p> required <p>Returns:</p> Name Type Description <code>analytic_signal_df</code> <code>DataFrame</code> <p>DataFrame containing hilbert transform of signal</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any electrodes passed to electrode_list are invalid for the dataset</p> Source code in <code>src/spyglass/lfp/analysis/v1/lfp_band.py</code> <pre><code>def compute_analytic_signal(self, electrode_list: list[int], **kwargs):\n    \"\"\"Computes the hilbert transform of a given LFPBand signal using scipy.signal.hilbert\n\n    Parameters\n    ----------\n    electrode_list: list[int]\n        A list of the electrodes to compute the hilbert transform of\n\n    Returns\n    -------\n    analytic_signal_df: pd.DataFrame\n        DataFrame containing hilbert transform of signal\n\n    Raises\n    ------\n    ValueError\n        If any electrodes passed to electrode_list are invalid for the dataset\n    \"\"\"\n\n    filtered_band = self.fetch_nwb()[0][\"lfp_band\"]\n    electrode_index = np.isin(\n        filtered_band.electrodes.data[:], electrode_list\n    )\n    if len(electrode_list) != np.sum(electrode_index):\n        raise ValueError(\n            \"Some of the electrodes specified in electrode_list are missing in the current LFPBand table.\"\n        )\n    analytic_signal_df = pd.DataFrame(\n        hilbert(filtered_band.data[:, electrode_index], axis=0),\n        index=pd.Index(filtered_band.timestamps, name=\"time\"),\n        columns=[f\"electrode {e}\" for e in electrode_list],\n    )\n    return analytic_signal_df\n</code></pre>"}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/#src.spyglass.lfp.analysis.v1.lfp_band.LFPBandV1.compute_signal_phase", "title": "<code>compute_signal_phase(electrode_list=None, **kwargs)</code>", "text": "<p>Computes the phase of a given LFPBand signals using the hilbert transform</p> <p>Parameters:</p> Name Type Description Default <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to compute the phase of, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>signal_phase_df</code> <code>DataFrame</code> <p>DataFrame containing the phase of the signals</p> Source code in <code>src/spyglass/lfp/analysis/v1/lfp_band.py</code> <pre><code>def compute_signal_phase(\n    self, electrode_list: list[int] = None, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"Computes the phase of a given LFPBand signals using the hilbert transform\n\n    Parameters\n    ----------\n    electrode_list : list[int], optional\n        A list of the electrodes to compute the phase of, by default None\n\n    Returns\n    -------\n    signal_phase_df : pd.DataFrame\n        DataFrame containing the phase of the signals\n    \"\"\"\n    if electrode_list is None:\n        electrode_list = []\n\n    analytic_signal_df = self.compute_analytic_signal(\n        electrode_list, **kwargs\n    )\n\n    return pd.DataFrame(\n        np.angle(analytic_signal_df) + np.pi,\n        columns=analytic_signal_df.columns,\n        index=analytic_signal_df.index,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/analysis/v1/lfp_band/#src.spyglass.lfp.analysis.v1.lfp_band.LFPBandV1.compute_signal_power", "title": "<code>compute_signal_power(electrode_list=None, **kwargs)</code>", "text": "<p>Computes the power of a given LFPBand signals using the hilbert transform</p> <p>Parameters:</p> Name Type Description Default <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to compute the power of, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>signal_power_df</code> <code>DataFrame</code> <p>DataFrame containing the power of the signals</p> Source code in <code>src/spyglass/lfp/analysis/v1/lfp_band.py</code> <pre><code>def compute_signal_power(\n    self, electrode_list: list[int] = None, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"Computes the power of a given LFPBand signals using the hilbert transform\n\n    Parameters\n    ----------\n    electrode_list : list[int], optional\n        A list of the electrodes to compute the power of, by default None\n\n    Returns\n    -------\n    signal_power_df : pd.DataFrame\n        DataFrame containing the power of the signals\n    \"\"\"\n    if electrode_list is None:\n        electrode_list = []\n\n    analytic_signal_df = self.compute_analytic_signal(\n        electrode_list, **kwargs\n    )\n\n    return pd.DataFrame(\n        np.abs(analytic_signal_df) ** 2,\n        columns=analytic_signal_df.columns,\n        index=analytic_signal_df.index,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp/", "title": "lfp.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp/#src.spyglass.lfp.v1.lfp.LFPSelection", "title": "<code>LFPSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>The user's selection of LFP data to be filtered</p> <p>This table is used to select the LFP data to be filtered.  The user can select the LFP data by specifying the electrode group and the interval list to be used. The interval list is used to select the times from the raw data that will be filtered.  The user can also specify the filter to be used.</p> <p>The LFP data is filtered and downsampled to 1 KHz.  The filtered data is stored in the AnalysisNwbfile table.  The valid times for the filtered data are stored in the IntervalList table.</p> Source code in <code>src/spyglass/lfp/v1/lfp.py</code> <pre><code>@schema\nclass LFPSelection(dj.Manual):\n    \"\"\"The user's selection of LFP data to be filtered\n\n    This table is used to select the LFP data to be filtered.  The user can select\n    the LFP data by specifying the electrode group and the interval list to be used.\n    The interval list is used to select the times from the raw data that will be\n    filtered.  The user can also specify the filter to be used.\n\n    The LFP data is filtered and downsampled to 1 KHz.  The filtered data is stored\n    in the AnalysisNwbfile table.  The valid times for the filtered data are stored\n    in the IntervalList table.\n    \"\"\"\n\n    definition = \"\"\"\n     -&gt; LFPElectrodeGroup                                                  # the group of electrodes to be filtered\n     -&gt; IntervalList.proj(target_interval_list_name='interval_list_name')  # the original set of times to be filtered\n     -&gt; FirFilterParameters                                                # the filter to be used\n     \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp/#src.spyglass.lfp.v1.lfp.LFPV1", "title": "<code>LFPV1</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>The filtered LFP data</p> Source code in <code>src/spyglass/lfp/v1/lfp.py</code> <pre><code>@schema\nclass LFPV1(dj.Computed):\n    \"\"\"The filtered LFP data\"\"\"\n\n    definition = \"\"\"\n    -&gt; LFPSelection             # the user's selection of LFP data to be filtered\n    ---\n    -&gt; AnalysisNwbfile          # the name of the nwb file with the lfp data\n    -&gt; IntervalList             # the final interval list of valid times for the data\n    lfp_object_id: varchar(40)  # the NWB object ID for loading this object from the file\n    lfp_sampling_rate: float    # the sampling rate, in HZ\n    \"\"\"\n\n    def make(self, key):\n        DECIMATION_FACTOR = 1000\n        # get the NWB object with the data\n        nwbf_key = {\"nwb_file_name\": key[\"nwb_file_name\"]}\n        rawdata = (Raw &amp; nwbf_key).fetch_nwb()[0][\"raw\"]\n\n        # CBroz: assumes Raw sampling rate matches FirFilterParameters set?\n        #        if we just pull rate from Raw, why include in Param table?\n        sampling_rate, raw_interval_list_name = (Raw &amp; nwbf_key).fetch1(\n            \"sampling_rate\", \"interval_list_name\"\n        )\n        sampling_rate = int(np.round(sampling_rate))\n\n        # to get the list of valid times, we need to combine those from the user with those from the\n        # raw data\n        orig_key = copy.deepcopy(key)\n        orig_key[\"interval_list_name\"] = key[\"target_interval_list_name\"]\n        user_valid_times = (IntervalList() &amp; orig_key).fetch1(\"valid_times\")\n        # we remove the extra entry so we can insert this into the LFPOutput table.\n        del orig_key[\"interval_list_name\"]\n\n        raw_valid_times = (\n            IntervalList()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": raw_interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_times = interval_list_intersect(\n            user_valid_times,\n            raw_valid_times,\n            min_length=MIN_LFP_INTERVAL_DURATION,\n        )\n        print(\n            f\"LFP: found {len(valid_times)} intervals &gt; {MIN_LFP_INTERVAL_DURATION} sec long.\"\n        )\n\n        # target 1 KHz sampling rate\n        decimation = sampling_rate // DECIMATION_FACTOR\n\n        # get the LFP filter that matches the raw data\n        filter = (\n            FirFilterParameters()\n            &amp; {\n                \"filter_name\": key[\"filter_name\"],\n                \"filter_sampling_rate\": sampling_rate,\n            }  # not key['filter_sampling_rate']?\n        ).fetch(as_dict=True)[0]\n\n        # there should only be one filter that matches, so we take the first of the dictionaries\n        key[\"filter_name\"] = filter[\"filter_name\"]\n        key[\"filter_sampling_rate\"] = filter[\"filter_sampling_rate\"]\n\n        filter_coeff = filter[\"filter_coeff\"]\n        if len(filter_coeff) == 0:\n            print(\n                f\"Error in LFP: no filter found with data sampling rate of {sampling_rate}\"\n            )\n            return None\n        # get the list of selected LFP Channels from LFPElectrode\n        electrode_keys = (LFPElectrodeGroup.LFPElectrode &amp; key).fetch(\"KEY\")\n        electrode_id_list = list(k[\"electrode_id\"] for k in electrode_keys)\n        electrode_id_list.sort()\n\n        lfp_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n\n        lfp_file_abspath = AnalysisNwbfile().get_abs_path(lfp_file_name)\n        (\n            lfp_object_id,\n            timestamp_interval,\n        ) = FirFilterParameters().filter_data_nwb(\n            lfp_file_abspath,\n            rawdata,\n            filter_coeff,\n            valid_times,\n            electrode_id_list,\n            decimation,\n        )\n\n        # now that the LFP is filtered and in the file, add the file to the AnalysisNwbfile table\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], lfp_file_name)\n\n        key[\"analysis_file_name\"] = lfp_file_name\n        key[\"lfp_object_id\"] = lfp_object_id\n        key[\"lfp_sampling_rate\"] = sampling_rate // decimation\n\n        # finally, we need to censor the valid times to account for the downsampling\n        lfp_valid_times = interval_list_censor(valid_times, timestamp_interval)\n\n        # add an interval list for the LFP valid times, skipping duplicates\n        key[\"interval_list_name\"] = \"_\".join(\n            (\n                \"lfp\",\n                key[\"lfp_electrode_group_name\"],\n                key[\"target_interval_list_name\"],\n                \"valid times\",\n            )\n        )\n        IntervalList.insert1(\n            {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n                \"valid_times\": lfp_valid_times,\n            },\n            replace=True,\n        )\n        self.insert1(key)\n\n        # finally, we insert this into the LFP output table.\n        from spyglass.lfp.lfp_merge import LFPOutput\n\n        orig_key[\"analysis_file_name\"] = lfp_file_name\n        orig_key[\"lfp_object_id\"] = lfp_object_id\n        LFPOutput.insert1(orig_key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self, *attrs, **kwargs):\n        nwb_lfp = self.fetch_nwb()[0]\n        return pd.DataFrame(\n            nwb_lfp[\"lfp\"].data,\n            index=pd.Index(nwb_lfp[\"lfp\"].timestamps, name=\"time\"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact/", "title": "lfp_artifact.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact/#src.spyglass.lfp.v1.lfp_artifact.LFPArtifactDetectionParameters", "title": "<code>LFPArtifactDetectionParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact.py</code> <pre><code>@schema\nclass LFPArtifactDetectionParameters(dj.Manual):\n    definition = \"\"\"\n    # Parameters for detecting LFP artifact times within a LFP group.\n    artifact_params_name: varchar(200)\n    ---\n    artifact_params: blob  # dictionary of parameters\n    \"\"\"\n\n    def insert_default(self):\n        \"\"\"Insert the default artifact parameters.\"\"\"\n        diff_params = [\n            \"default_difference\",\n            {\n                \"artifact_detection_algorithm\": \"difference\",\n                \"artifact_detection_algorithm_params\": {\n                    \"amplitude_thresh_1st\": 500,  # must be None or &gt;= 0\n                    \"proportion_above_thresh_1st\": 0.1,\n                    \"amplitude_thresh_2nd\": 1000,  # must be None or &gt;= 0\n                    \"proportion_above_thresh_2nd\": 0.05,\n                    \"removal_window_ms\": 10.0,  # in milliseconds\n                    \"local_window_ms\": 40.0,  # in milliseconds\n                },\n            },\n        ]\n\n        diff_ref_params = [\n            \"default_difference_ref\",\n            {\n                \"artifact_detection_algorithm\": \"difference\",\n                \"artifact_detection_algorithm_params\": {\n                    \"amplitude_thresh_1st\": 500,  # must be None or &gt;= 0\n                    \"proportion_above_thresh_1st\": 0.1,\n                    \"amplitude_thresh_2nd\": 1000,  # must be None or &gt;= 0\n                    \"proportion_above_thresh_2nd\": 0.05,\n                    \"removal_window_ms\": 10.0,  # in milliseconds\n                    \"local_window_ms\": 40.0,  # in milliseconds\n                },\n                \"referencing\": {\n                    \"ref_on\": 1,\n                    \"reference_list\": [0, 0, 0, 0, 0],\n                    \"electrode_list\": [0, 0],\n                },\n            },\n        ]\n\n        no_params = [\n            \"none\",\n            {\n                \"artifact_detection_algorithm\": \"difference\",\n                \"artifact_detection_algorithm_params\": {\n                    \"amplitude_thresh_1st\": None,  # must be None or &gt;= 0\n                    \"proportion_above_thresh_1st\": None,\n                    \"amplitude_thresh_2nd\": None,  # must be None or &gt;= 0\n                    \"proportion_above_thresh_2nd\": None,\n                    \"removal_window_ms\": None,  # in milliseconds\n                    \"local_window_ms\": None,  # in milliseconds\n                },\n            },\n        ]\n\n        mad_params = [\n            \"default_mad\",\n            {\n                \"artifact_detection_algorithm\": \"mad\",\n                \"artifact_detection_algorithm_params\": {\n                    # akin to z-score std dev if the distribution is normal\n                    \"mad_thresh\": 6.0,\n                    \"proportion_above_thresh\": 0.1,\n                    \"removal_window_ms\": 10.0,  # in milliseconds\n                },\n            },\n        ]\n\n        self.insert(\n            [diff_params, diff_ref_params, no_params, mad_params],\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact/#src.spyglass.lfp.v1.lfp_artifact.LFPArtifactDetectionParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default artifact parameters.</p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact.py</code> <pre><code>def insert_default(self):\n    \"\"\"Insert the default artifact parameters.\"\"\"\n    diff_params = [\n        \"default_difference\",\n        {\n            \"artifact_detection_algorithm\": \"difference\",\n            \"artifact_detection_algorithm_params\": {\n                \"amplitude_thresh_1st\": 500,  # must be None or &gt;= 0\n                \"proportion_above_thresh_1st\": 0.1,\n                \"amplitude_thresh_2nd\": 1000,  # must be None or &gt;= 0\n                \"proportion_above_thresh_2nd\": 0.05,\n                \"removal_window_ms\": 10.0,  # in milliseconds\n                \"local_window_ms\": 40.0,  # in milliseconds\n            },\n        },\n    ]\n\n    diff_ref_params = [\n        \"default_difference_ref\",\n        {\n            \"artifact_detection_algorithm\": \"difference\",\n            \"artifact_detection_algorithm_params\": {\n                \"amplitude_thresh_1st\": 500,  # must be None or &gt;= 0\n                \"proportion_above_thresh_1st\": 0.1,\n                \"amplitude_thresh_2nd\": 1000,  # must be None or &gt;= 0\n                \"proportion_above_thresh_2nd\": 0.05,\n                \"removal_window_ms\": 10.0,  # in milliseconds\n                \"local_window_ms\": 40.0,  # in milliseconds\n            },\n            \"referencing\": {\n                \"ref_on\": 1,\n                \"reference_list\": [0, 0, 0, 0, 0],\n                \"electrode_list\": [0, 0],\n            },\n        },\n    ]\n\n    no_params = [\n        \"none\",\n        {\n            \"artifact_detection_algorithm\": \"difference\",\n            \"artifact_detection_algorithm_params\": {\n                \"amplitude_thresh_1st\": None,  # must be None or &gt;= 0\n                \"proportion_above_thresh_1st\": None,\n                \"amplitude_thresh_2nd\": None,  # must be None or &gt;= 0\n                \"proportion_above_thresh_2nd\": None,\n                \"removal_window_ms\": None,  # in milliseconds\n                \"local_window_ms\": None,  # in milliseconds\n            },\n        },\n    ]\n\n    mad_params = [\n        \"default_mad\",\n        {\n            \"artifact_detection_algorithm\": \"mad\",\n            \"artifact_detection_algorithm_params\": {\n                # akin to z-score std dev if the distribution is normal\n                \"mad_thresh\": 6.0,\n                \"proportion_above_thresh\": 0.1,\n                \"removal_window_ms\": 10.0,  # in milliseconds\n            },\n        },\n    ]\n\n    self.insert(\n        [diff_params, diff_ref_params, no_params, mad_params],\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_MAD_detection/", "title": "lfp_artifact_MAD_detection.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_MAD_detection/#src.spyglass.lfp.v1.lfp_artifact_MAD_detection.mad_artifact_detector", "title": "<code>mad_artifact_detector(recording, mad_thresh=6.0, proportion_above_thresh=0.1, removal_window_ms=10.0, sampling_frequency=1000.0, *args, **kwargs)</code>", "text": "<p>Detect LFP artifacts using the median absolute deviation method.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>RecordingExtractor</code> <p>The recording extractor object</p> required <code>mad_thresh</code> <code>float</code> <p>Threshold on the median absolute deviation scaled LFPs, defaults to 6.0</p> <code>6.0</code> <code>proportion_above_thresh</code> <code>float</code> <p>Proportion of electrodes that need to be above the threshold, defaults to 1.0</p> <code>0.1</code> <code>removal_window_ms</code> <code>float</code> <p>Width of the window in milliseconds to mask out per artifact (window/2 removed on each side of threshold crossing), defaults to 1 ms</p> <code>10.0</code> <code>sampling_frequency</code> <code>float</code> <p>Sampling frequency of the recording extractor, defaults to 1000.0</p> <code>1000.0</code> <p>Returns:</p> Name Type Description <code>artifact_removed_valid_times</code> <code>ndarray</code> <p>Intervals of valid times where artifacts were not detected, unit: seconds</p> <code>artifact_intervals</code> <code>ndarray</code> <p>Intervals in which artifacts are detected (including removal windows), unit: seconds</p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact_MAD_detection.py</code> <pre><code>def mad_artifact_detector(\n    recording: None,\n    mad_thresh: float = 6.0,\n    proportion_above_thresh: float = 0.1,\n    removal_window_ms: float = 10.0,\n    sampling_frequency: float = 1000.0,\n    *args,\n    **kwargs,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Detect LFP artifacts using the median absolute deviation method.\n\n    Parameters\n    ----------\n    recording : RecordingExtractor\n        The recording extractor object\n    mad_thresh : float, optional\n        Threshold on the median absolute deviation scaled LFPs, defaults to 6.0\n    proportion_above_thresh : float, optional\n        Proportion of electrodes that need to be above the threshold, defaults to 1.0\n    removal_window_ms : float, optional\n        Width of the window in milliseconds to mask out per artifact\n        (window/2 removed on each side of threshold crossing), defaults to 1 ms\n    sampling_frequency : float, optional\n        Sampling frequency of the recording extractor, defaults to 1000.0\n\n    Returns\n    -------\n    artifact_removed_valid_times : np.ndarray\n        Intervals of valid times where artifacts were not detected, unit: seconds\n    artifact_intervals : np.ndarray\n        Intervals in which artifacts are detected (including removal windows), unit: seconds\n\n    \"\"\"\n\n    timestamps = np.asarray(recording.timestamps)\n    lfps = np.asarray(recording.data)\n\n    mad = median_abs_deviation(lfps, axis=0, nan_policy=\"omit\", scale=\"normal\")\n    is_artifact = _is_above_proportion_thresh(\n        _mad_scale_lfps(lfps, mad), mad_thresh, proportion_above_thresh\n    )\n\n    MILLISECONDS_PER_SECOND = 1000.0\n    half_removal_window_s = (removal_window_ms / MILLISECONDS_PER_SECOND) * 0.5\n    half_removal_window_idx = int(half_removal_window_s * sampling_frequency)\n    is_artifact = _extend_array_by_window(is_artifact, half_removal_window_idx)\n\n    artifact_intervals_s = _get_time_intervals_from_bool_array(\n        is_artifact, timestamps\n    )\n\n    valid_times = _get_time_intervals_from_bool_array(~is_artifact, timestamps)\n\n    return valid_times, artifact_intervals_s\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_difference_detection/", "title": "lfp_artifact_difference_detection.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_difference_detection/#src.spyglass.lfp.v1.lfp_artifact_difference_detection.difference_artifact_detector", "title": "<code>difference_artifact_detector(recording, timestamps, amplitude_thresh_1st=None, amplitude_thresh_2nd=None, proportion_above_thresh_1st=1.0, proportion_above_thresh_2nd=1.0, removal_window_ms=1.0, local_window_ms=1.0, sampling_frequency=1000.0, referencing=0)</code>", "text": "<p>Detects times during which artifacts do and do not occur.</p> <p>Artifacts are defined as periods where the absolute value of the change in LFP exceeds amplitude change thresholds on the proportion of channels specified, with the period extended by the removal_window_ms/2 on each side. amplitude change threshold values of None are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>lfp eseries zscore_thresh : float</code> <p>Stdev threshold for exclusion, should be &gt;=0, defaults to None</p> required <code>amplitude_thresh_1st</code> <code>float</code> <p>Amplitude (ad units) threshold for exclusion, should be &gt;=0, defaults to None</p> <code>None</code> <code>amplitude_thresh_2nd</code> <code>float</code> <p>Amplitude (ad units) threshold for exclusion, should be &gt;=0, defaults to None</p> <code>None</code> <code>proportion_above_thresh_1st</code> <code>float, optional, should be&gt;0 and &lt;=1</code> <p>Proportion of electrodes that need to have threshold crossings, defaults to 1</p> <code>1.0</code> <code>proportion_above_thresh_2nd</code> <code>float, optional, should be&gt;0 and &lt;=1</code> <p>Proportion of electrodes that need to have threshold crossings, defaults to 1</p> <code>1.0</code> <code>removal_window_ms</code> <code>float</code> <p>Width of the window in milliseconds to mask out per artifact (window/2 removed on each side of threshold crossing), defaults to 1 ms</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>artifact_removed_valid_times</code> <code>ndarray</code> <p>Intervals of valid times where artifacts were not detected, unit: seconds</p> <code>artifact_intervals</code> <code>ndarray</code> <p>Intervals in which artifacts are detected (including removal windows), unit: seconds</p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact_difference_detection.py</code> <pre><code>def difference_artifact_detector(\n    recording: None,\n    timestamps: None,\n    amplitude_thresh_1st: Union[float, None] = None,\n    amplitude_thresh_2nd: Union[float, None] = None,\n    proportion_above_thresh_1st: float = 1.0,\n    proportion_above_thresh_2nd: float = 1.0,\n    removal_window_ms: float = 1.0,\n    local_window_ms: float = 1.0,\n    sampling_frequency: float = 1000.0,\n    referencing: int = 0,\n):\n    \"\"\"Detects times during which artifacts do and do not occur.\n\n    Artifacts are defined as periods where the absolute value of the change in\n    LFP exceeds amplitude change thresholds on the proportion of channels\n    specified, with the period extended by the removal_window_ms/2 on each side.\n    amplitude change threshold values of None are ignored.\n\n    Parameters\n    ----------\n    recording : lfp eseries zscore_thresh : float, optional\n        Stdev threshold for exclusion, should be &gt;=0, defaults to None\n    amplitude_thresh_1st : float, optional\n        Amplitude (ad units) threshold for exclusion, should be &gt;=0, defaults to\n        None\n    amplitude_thresh_2nd : float, optional\n        Amplitude (ad units) threshold for exclusion, should be &gt;=0, defaults to\n        None\n    proportion_above_thresh_1st : float, optional, should be&gt;0 and &lt;=1\n        Proportion of electrodes that need to have threshold crossings, defaults\n        to 1\n    proportion_above_thresh_2nd : float, optional, should be&gt;0 and &lt;=1\n        Proportion of electrodes that need to have threshold crossings, defaults\n        to 1\n    removal_window_ms : float, optional\n        Width of the window in milliseconds to mask out per artifact (window/2\n        removed on each side of threshold crossing), defaults to 1 ms\n\n    Returns\n    -------\n    artifact_removed_valid_times : np.ndarray\n        Intervals of valid times where artifacts were not detected, unit:\n        seconds\n    artifact_intervals : np.ndarray\n        Intervals in which artifacts are detected (including removal windows),\n        unit: seconds\n    \"\"\"\n\n    # NOTE: 7-17-23 updated to remove recording.data, since it will converted to\n    # numpy array before referencing check for referencing flag\n\n    if referencing == 1:\n        print(\"referencing activated. may be set to -1\")\n\n    # valid_timestamps = recording.timestamps\n    valid_timestamps = timestamps\n\n    local_window = int(local_window_ms / 2)\n\n    # if both thresholds are None, we skip artifact detection\n    if amplitude_thresh_1st is None:\n        recording_interval = np.asarray(\n            [valid_timestamps[0], valid_timestamps[-1]]\n        )\n        artifact_times_empty = np.asarray([])\n        print(\"Amplitude threshold is None, skipping artifact detection\")\n        return recording_interval, artifact_times_empty\n\n    # verify threshold parameters\n    (\n        amplitude_thresh_1st,\n        amplitude_thresh_2nd,\n        proportion_above_thresh_1st,\n        proportion_above_thresh_2nd,\n    ) = _check_artifact_thresholds(\n        amplitude_thresh_1st,\n        amplitude_thresh_2nd,\n        proportion_above_thresh_1st,\n        proportion_above_thresh_2nd,\n    )\n\n    # want to detect frames without parallel processing\n    # compute the number of electrodes that have to be above threshold\n    nelect_above_1st = np.ceil(proportion_above_thresh_1st * recording.shape[1])\n    nelect_above_2nd = np.ceil(proportion_above_thresh_2nd * recording.shape[1])\n    print(\"num tets 1\", nelect_above_1st, \"num tets 2\", nelect_above_2nd)\n    print(\"data shape\", recording.shape)\n\n    # find the artifact occurrences using one or both thresholds, across\n    # channels\n\n    if amplitude_thresh_1st is not None:\n        # first find times with large amp change: sum diff over several timebins\n\n        diff_array = np.diff(recording, axis=0)\n        window = np.ones((3, 1)) if referencing else np.ones((15, 1))\n\n        # sum differences over bins using convolution for speed\n        width = int((window.size - 1) / 2)\n        diff_array = np.pad(\n            diff_array,\n            pad_width=((width, width), (0, 0)),\n            mode=\"constant\",\n        )\n        diff_array_5 = scipy.signal.convolve(diff_array, window, mode=\"valid\")\n\n        artifact_times_all_5 = np.sum(\n            (np.abs(diff_array_5) &gt; amplitude_thresh_1st), axis=1\n        )\n\n        above_thresh_1st = np.where(artifact_times_all_5 &gt;= nelect_above_1st)[0]\n\n        # second, find artifacts with large baseline change\n        print(\"thresh\", amplitude_thresh_2nd, \"window\", local_window)\n\n        big_artifacts = np.zeros(\n            (recording.shape[1], above_thresh_1st.shape[0])\n        )\n        for art_count in np.arange(above_thresh_1st.shape[0]):\n            if above_thresh_1st[art_count] &lt;= local_window:\n                local_min = local_max = above_thresh_1st[art_count]\n            else:\n                local_max = np.max(\n                    recording[\n                        above_thresh_1st[art_count]\n                        - local_window : above_thresh_1st[art_count]\n                        + local_window,\n                        :,\n                    ],\n                    axis=0,\n                )\n                local_min = np.min(\n                    recording[\n                        above_thresh_1st[art_count]\n                        - local_window : above_thresh_1st[art_count]\n                        + local_window,\n                        :,\n                    ],\n                    axis=0,\n                )\n            big_artifacts[:, art_count] = (\n                np.abs(local_max - local_min) &gt; amplitude_thresh_2nd\n            )\n\n        # sum columns in big artficat, then compare to nelect_above_2nd\n        above_thresh = above_thresh_1st[\n            np.sum(big_artifacts, axis=0) &gt;= nelect_above_2nd\n        ]\n\n    artifact_frames = above_thresh.copy()\n    print(\"detected \", artifact_frames.shape[0], \" artifacts\")\n\n    # Convert to s to remove from either side of each detected artifact\n    half_removal_window_s = removal_window_ms / 1000 * 0.5\n\n    if len(artifact_frames) == 0:\n        recording_interval = np.asarray(\n            [[valid_timestamps[0], valid_timestamps[-1]]]\n        )\n        artifact_times_empty = np.asarray([])\n        print(\"No artifacts detected.\")\n        return recording_interval, artifact_times_empty\n\n    artifact_intervals = interval_from_inds(artifact_frames)\n\n    artifact_intervals_s = np.zeros(\n        (len(artifact_intervals), 2), dtype=np.float64\n    )\n\n    for interval_idx, interval in enumerate(artifact_intervals):\n        artifact_intervals_s[interval_idx] = [\n            valid_timestamps[interval[0]] - half_removal_window_s,\n            valid_timestamps[interval[1]] + half_removal_window_s,\n        ]\n    artifact_intervals_s = reduce(_union_concat, artifact_intervals_s)\n\n    valid_intervals = get_valid_intervals(\n        valid_timestamps, sampling_frequency, 1.5, 0.000001\n    )\n\n    # these are artifact times - need to subtract these from valid timestamps\n    artifact_valid_times = interval_list_intersect(\n        valid_intervals, artifact_intervals_s\n    )\n\n    # note: this is a slow step\n    list_triggers = []\n    for interval in artifact_valid_times:\n        list_triggers.append(\n            np.arange(\n                np.searchsorted(valid_timestamps, interval[0]),\n                np.searchsorted(valid_timestamps, interval[1]),\n            )\n        )\n\n    new_array = np.array(np.concatenate(list_triggers))\n\n    new_timestamps = np.delete(valid_timestamps, new_array)\n\n    artifact_removed_valid_times = get_valid_intervals(\n        new_timestamps, sampling_frequency, 1.5, 0.000001\n    )\n\n    return artifact_removed_valid_times, artifact_intervals_s\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/", "title": "file_lock.py", "text": ""}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.NwbfileLock", "title": "<code>NwbfileLock</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>@schema\nclass NwbfileLock(dj.Manual):\n    definition = \"\"\"\n    -&gt; Nwbfile\n    \"\"\"\n\n    def populate_from_lock_file(self):\n        \"\"\"\n        Reads from the NWB_LOCK_FILE (defined by an environment variable),\n        adds the entries to this schema, and then removes the file\n        \"\"\"\n        if os.path.exists(os.getenv(\"NWB_LOCK_FILE\")):\n            lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"r\")\n            for line in lock_file:\n                print(line)\n                key = {\"nwb_file_name\": line.strip()}\n                self.insert1(key, skip_duplicates=\"True\")\n            lock_file.close()\n            os.remove(os.getenv(\"NWB_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.NwbfileLock.populate_from_lock_file", "title": "<code>populate_from_lock_file()</code>", "text": "<p>Reads from the NWB_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and then removes the file</p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>def populate_from_lock_file(self):\n    \"\"\"\n    Reads from the NWB_LOCK_FILE (defined by an environment variable),\n    adds the entries to this schema, and then removes the file\n    \"\"\"\n    if os.path.exists(os.getenv(\"NWB_LOCK_FILE\")):\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"r\")\n        for line in lock_file:\n            print(line)\n            key = {\"nwb_file_name\": line.strip()}\n            self.insert1(key, skip_duplicates=\"True\")\n        lock_file.close()\n        os.remove(os.getenv(\"NWB_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.AnalysisNwbfileLock", "title": "<code>AnalysisNwbfileLock</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>@schema\nclass AnalysisNwbfileLock(dj.Manual):\n    definition = \"\"\"\n    -&gt; AnalysisNwbfile\n    \"\"\"\n\n    def populate_from_lock_file(self):\n        \"\"\"Reads from the ANALYSIS_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and\n        then removes the file\n        \"\"\"\n        if os.path.exists(os.getenv(\"ANALYSIS_LOCK_FILE\")):\n            lock_file = open(os.getenv(\"ANALYSIS_LOCK_FILE\"), \"r\")\n            for line in lock_file:\n                key = {\"analysis_file_name\": line.strip()}\n                self.insert1(key, skip_duplicates=\"True\")\n            os.remove(os.getenv(\"ANALYSIS_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.AnalysisNwbfileLock.populate_from_lock_file", "title": "<code>populate_from_lock_file()</code>", "text": "<p>Reads from the ANALYSIS_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and then removes the file</p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>def populate_from_lock_file(self):\n    \"\"\"Reads from the ANALYSIS_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and\n    then removes the file\n    \"\"\"\n    if os.path.exists(os.getenv(\"ANALYSIS_LOCK_FILE\")):\n        lock_file = open(os.getenv(\"ANALYSIS_LOCK_FILE\"), \"r\")\n        for line in lock_file:\n            key = {\"analysis_file_name\": line.strip()}\n            self.insert1(key, skip_duplicates=\"True\")\n        os.remove(os.getenv(\"ANALYSIS_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/", "title": "position_merge.py", "text": ""}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput", "title": "<code>PositionOutput</code>", "text": "<p>             Bases: <code>_Merge</code></p> <p>Table to identify source of Position Information from upstream options (e.g. DLC, Trodes, etc...) To add another upstream option, a new Part table should be added in the same syntax as DLCPos and TrodesPos.</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>@schema\nclass PositionOutput(_Merge):\n    \"\"\"\n    Table to identify source of Position Information from upstream options\n    (e.g. DLC, Trodes, etc...) To add another upstream option, a new Part table\n    should be added in the same syntax as DLCPos and TrodesPos.\n    \"\"\"\n\n    definition = \"\"\"\n    merge_id: uuid\n    ---\n    source: varchar(32)\n    \"\"\"\n\n    class DLCPosV1(dj.Part):\n        \"\"\"\n        Table to pass-through upstream DLC Pose Estimation information\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; PositionOutput\n        ---\n        -&gt; DLCPosV1\n        \"\"\"\n\n    class TrodesPosV1(dj.Part):\n        \"\"\"\n        Table to pass-through upstream Trodes Position Tracking information\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; PositionOutput\n        ---\n        -&gt; TrodesPosV1\n        \"\"\"\n\n    class CommonPos(dj.Part):\n        \"\"\"\n        Table to pass-through upstream Trodes Position Tracking information\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; PositionOutput\n        ---\n        -&gt; CommonPos\n        \"\"\"\n\n    def fetch1_dataframe(self):\n        # proj replaces operator restriction to enable\n        # (TableName &amp; restriction).fetch1_dataframe()\n        key = self.merge_restrict(self.proj())\n        query = (\n            source_class_dict[\n                to_camel_case(self.merge_get_parent(self.proj()).table_name)\n            ]\n            &amp; key\n        )\n        return query.fetch1_dataframe()\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput.DLCPosV1", "title": "<code>DLCPosV1</code>", "text": "<p>             Bases: <code>Part</code></p> <p>Table to pass-through upstream DLC Pose Estimation information</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>class DLCPosV1(dj.Part):\n    \"\"\"\n    Table to pass-through upstream DLC Pose Estimation information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionOutput\n    ---\n    -&gt; DLCPosV1\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput.TrodesPosV1", "title": "<code>TrodesPosV1</code>", "text": "<p>             Bases: <code>Part</code></p> <p>Table to pass-through upstream Trodes Position Tracking information</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>class TrodesPosV1(dj.Part):\n    \"\"\"\n    Table to pass-through upstream Trodes Position Tracking information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionOutput\n    ---\n    -&gt; TrodesPosV1\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput.CommonPos", "title": "<code>CommonPos</code>", "text": "<p>             Bases: <code>Part</code></p> <p>Table to pass-through upstream Trodes Position Tracking information</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>class CommonPos(dj.Part):\n    \"\"\"\n    Table to pass-through upstream Trodes Position Tracking information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionOutput\n    ---\n    -&gt; CommonPos\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionVideo", "title": "<code>PositionVideo</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>@schema\nclass PositionVideo(dj.Computed):\n    \"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionVideoSelection\n    ---\n    \"\"\"\n\n    def make(self, key):\n        raise NotImplementedError(\"work in progress -DPG\")\n\n        plot = key.get(\"plot\")\n        if plot not in [\"DLC\", \"Trodes\", \"Common\", \"All\"]:\n            raise ValueError(f\"Plot {key['plot']} not supported\")\n            # CBroz: I was told only tests should `assert`, code should `raise`\n\n        M_TO_CM = 100\n        output_dir = (PositionVideoSelection &amp; key).fetch1(\"output_dir\")\n\n        print(\"Loading position data...\")\n        # raw_position_df = (\n        #     RawPosition()\n        #     &amp; {\n        #         \"nwb_file_name\": key[\"nwb_file_name\"],\n        #         \"interval_list_name\": key[\"interval_list_name\"],\n        #     }\n        # ).fetch1_dataframe()\n\n        query = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": key[\"interval_list_name\"],\n        }\n        merge_entries = {\n            \"DLC\": PositionOutput.DLCPosV1 &amp; query,\n            \"Trodes\": PositionOutput.TrodesPosV1 &amp; query,\n            \"Common\": PositionOutput.CommonPos &amp; query,\n        }\n\n        position_mean_dict = {}\n        if plot == \"All\":\n            # Check which entries exist in PositionOutput\n            merge_dict = {}\n            for source, entries in merge_entries.items():\n                if entries:\n                    merge_dict[source] = entries.fetch1_dataframe().drop(\n                        columns=[\"velocity_x\", \"velocity_y\", \"speed\"]\n                    )\n\n            pos_df = ft.reduce(\n                lambda left, right,: pd.merge(\n                    left[1],\n                    right[1],\n                    left_index=True,\n                    right_index=True,\n                    suffixes=[f\"_{left[0]}\", f\"_{right[0]}\"],\n                ),\n                merge_dict.items(),\n            )\n            position_mean_dict = {\n                source: {\n                    \"position\": np.asarray(\n                        pos_df[[f\"position_x_{source}\", f\"position_y_{source}\"]]\n                    ),\n                    \"orientation\": np.asarray(\n                        pos_df[[f\"orientation_{source}\"]]\n                    ),\n                }\n                for source in merge_dict.keys()\n            }\n        else:\n            if plot == \"DLC\":\n                # CBroz - why is this extra step needed for DLC?\n                pos_df_key = merge_entries[plot].fetch1(as_dict=True)\n                pos_df = (PositionOutput &amp; pos_df_key).fetch1_dataframe()\n            elif plot in [\"Trodes\", \"Common\"]:\n                pos_df = merge_entries[plot].fetch1_dataframe()\n\n            position_mean_dict[plot][\"position\"] = np.asarray(\n                pos_df[[\"position_x\", \"position_y\"]]\n            )\n            position_mean_dict[plot][\"orientation\"] = np.asarray(\n                pos_df[[\"orientation\"]]\n            )\n\n        print(\"Loading video data...\")\n\n        (\n            video_path,\n            video_filename,\n            meters_per_pixel,\n            video_time,\n        ) = get_video_path(\n            {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"epoch\": int(\n                    \"\".join(filter(str.isdigit, key[\"interval_list_name\"]))\n                )\n                + 1,\n            }\n        )\n        video_dir = os.path.dirname(video_path) + \"/\"\n        video_frame_col_name = [\n            col for col in pos_df.columns if \"video_frame_ind\" in col\n        ][0]\n        video_frame_inds = pos_df[video_frame_col_name].astype(int).to_numpy()\n        if plot in [\"DLC\", \"All\"]:\n            video_path = (\n                DLCPoseEstimationSelection\n                &amp; (PositionOutput.DLCPosV1 &amp; key).fetch1(\"KEY\")\n            ).fetch1(\"video_path\")\n        else:\n            video_path = check_videofile(\n                video_dir, key[\"output_dir\"], video_filename\n            )[0]\n\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        output_video_filename = Path(\n            f\"{Path(output_dir).as_posix()}/{nwb_base_filename}{epoch:02d}_\"\n            f\"{key['plot']}_pos_overlay.mp4\"\n        ).as_posix()\n\n        # centroids = {'red': np.asarray(raw_position_df[['xloc', 'yloc']]),\n        #              'green':  np.asarray(raw_position_df[['xloc2', 'yloc2']])}\n\n        print(\"Making video...\")\n\n        make_video(\n            video_path,\n            video_frame_inds,\n            position_mean_dict,\n            video_time,\n            np.asarray(pos_df.index),\n            processor=\"opencv\",\n            output_video_filename=output_video_filename,\n            cm_to_pixels=meters_per_pixel * M_TO_CM,\n            disable_progressbar=False,\n        )\n        self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_decorators/", "title": "dlc_decorators.py", "text": ""}, {"location": "api/src/spyglass/position/v1/dlc_reader/", "title": "dlc_reader.py", "text": ""}, {"location": "api/src/spyglass/position/v1/dlc_reader/#src.spyglass.position.v1.dlc_reader.read_yaml", "title": "<code>read_yaml(fullpath, filename='*')</code>", "text": "<p>Return contents of yml in fullpath. If available, defer to DJ-saved version</p> <p>Parameters:</p> Name Type Description Default <code>fullpath</code> required <code>filename</code> <code>'*'</code> <p>Returns filepath and contents as dict</p> Source code in <code>src/spyglass/position/v1/dlc_reader.py</code> <pre><code>def read_yaml(fullpath, filename=\"*\"):\n    \"\"\"Return contents of yml in fullpath. If available, defer to DJ-saved version\n\n    Parameters\n    ----------\n    fullpath: String or pathlib path. Directory with yaml files\n    filename: String. Filename, no extension. Permits wildcards.\n\n    Returns filepath and contents as dict\n    \"\"\"\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    # Take the DJ-saved if there. If not, return list of available\n    yml_paths = list(Path(fullpath).glob(\"dj_dlc_config.yaml\")) or sorted(\n        list(Path(fullpath).glob(f\"{filename}.y*ml\"))\n    )\n\n    assert (  # If more than 1 and not DJ-saved,\n        len(yml_paths) == 1\n    ), f\"Found more yaml files than expected: {len(yml_paths)}\\n{fullpath}\"\n\n    return yml_paths[0], read_config(yml_paths[0])\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_reader/#src.spyglass.position.v1.dlc_reader.save_yaml", "title": "<code>save_yaml(output_dir, config_dict, filename='dj_dlc_config', mkdir=True)</code>", "text": "<p>Save config_dict to output_path as filename.yaml. By default, preserves original.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> required <code>config_dict</code> required <code>filename</code> <pre><code>  Set to 'config' to overwrite original file.\n  If extension is included, removed and replaced with \"yaml\".\n</code></pre> <code>'dj_dlc_config'</code> <code>mkdir</code> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>path of saved file as string - due to DLC func preference for strings</p> Source code in <code>src/spyglass/position/v1/dlc_reader.py</code> <pre><code>def save_yaml(output_dir, config_dict, filename=\"dj_dlc_config\", mkdir=True):\n    \"\"\"Save config_dict to output_path as filename.yaml. By default, preserves original.\n\n    Parameters\n    ----------\n    output_dir: where to save yaml file\n    config_dict: dict of config params or element-deeplabcut model.Model dict\n    filename: Optional, default 'dj_dlc_config' or preserve original 'config'\n              Set to 'config' to overwrite original file.\n              If extension is included, removed and replaced with \"yaml\".\n    mkdir (bool): Optional, True. Make new directory if output_dir not exist\n\n    Returns\n    -------\n    str\n        path of saved file as string - due to DLC func preference for strings\n    \"\"\"\n    from deeplabcut.utils.auxiliaryfunctions import write_config\n\n    if \"config_template\" in config_dict:  # if passed full model.Model dict\n        config_dict = config_dict[\"config_template\"]\n    if mkdir:\n        Path(output_dir).mkdir(exist_ok=True)\n    if \".\" in filename:  # if user provided extension, remove\n        filename = filename.split(\".\")[0]\n\n    output_filepath = Path(output_dir) / f\"{filename}.yaml\"\n    write_config(output_filepath, config_dict)\n    return str(output_filepath)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_reader/#src.spyglass.position.v1.dlc_reader.do_pose_estimation", "title": "<code>do_pose_estimation(video_filepaths, dlc_model, project_path, output_dir, videotype='', gputouse=None, save_as_csv=False, batchsize=None, cropping=None, TFGPUinference=True, dynamic=(False, 0.5, 10), robust_nframes=False, allow_growth=False, use_shelve=False)</code>", "text": "<p>Launch DLC's analyze_videos within element-deeplabcut</p> <p>Other optional parameters may be set other than those described below. See deeplabcut.analyze_videos parameters for descriptions/defaults.</p> <p>Parameters:</p> Name Type Description Default <code>video_filepaths</code> required <code>dlc_model</code> required <code>project_path</code> required <code>output_dir</code> required Source code in <code>src/spyglass/position/v1/dlc_reader.py</code> <pre><code>def do_pose_estimation(\n    video_filepaths,\n    dlc_model,\n    project_path,\n    output_dir,\n    videotype=\"\",\n    gputouse=None,\n    save_as_csv=False,\n    batchsize=None,\n    cropping=None,\n    TFGPUinference=True,\n    dynamic=(False, 0.5, 10),\n    robust_nframes=False,\n    allow_growth=False,\n    use_shelve=False,\n):\n    \"\"\"Launch DLC's analyze_videos within element-deeplabcut\n\n    Other optional parameters may be set other than those described below. See\n    deeplabcut.analyze_videos parameters for descriptions/defaults.\n\n    Parameters\n    ----------\n    video_filepaths: list of videos to analyze\n    dlc_model: element-deeplabcut dlc.Model dict\n    project_path: path to project config.yml\n    output_dir: where to save output\n    \"\"\"\n    from deeplabcut.pose_estimation_tensorflow import analyze_videos\n\n    # ---- Build and save DLC configuration (yaml) file ----\n    dlc_config = dlc_model[\"config_template\"]\n    dlc_project_path = Path(project_path)\n    dlc_config[\"project_path\"] = dlc_project_path.as_posix()\n\n    # ---- Write config files ----\n    # To output dir: Important for loading/parsing output in datajoint\n    _ = save_yaml(output_dir, dlc_config)\n    # To project dir: Required by DLC to run the analyze_videos\n    if dlc_project_path != output_dir:\n        config_filepath = save_yaml(dlc_project_path, dlc_config)\n    # ---- Trigger DLC prediction job ----\n    analyze_videos(\n        config=config_filepath,\n        videos=video_filepaths,\n        shuffle=dlc_model[\"shuffle\"],\n        trainingsetindex=dlc_model[\"trainingsetindex\"],\n        destfolder=output_dir,\n        modelprefix=dlc_model[\"model_prefix\"],\n        videotype=videotype,\n        gputouse=gputouse,\n        save_as_csv=save_as_csv,\n        batchsize=batchsize,\n        cropping=cropping,\n        TFGPUinference=TFGPUinference,\n        dynamic=dynamic,\n        robust_nframes=robust_nframes,\n        allow_growth=allow_growth,\n        use_shelve=use_shelve,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/", "title": "dlc_utils.py", "text": ""}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.OutputLogger", "title": "<code>OutputLogger</code>", "text": "<p>A class to wrap a logging.Logger object in order to provide context manager capabilities.</p> <p>This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus print statements to the log file instead of, or as well as the console.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>logger object</p> <code>name</code> <code>str</code> <p>name of logger</p> <code>level</code> <code>int</code> <p>level of logging that the logger is set to handle</p> <p>Methods:</p> Name Description <code>setup_logger</code> <p>initialize or get logger object with name_logfile that writes to path_logfile</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n...    print(\"this will print to logfile\")\n...    logger.logger.info(\"this will log to the logfile\")\n... print(\"this will print to the console\")\n... logger.logger.info(\"this will log to the logfile\")\n</code></pre> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>class OutputLogger:\n    \"\"\"\n    A class to wrap a logging.Logger object in order to provide context manager capabilities.\n\n    This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus\n    print statements to the log file instead of, or as well as the console.\n\n    Attributes\n    ----------\n    logger : logging.Logger\n        logger object\n    name : str\n        name of logger\n    level : int\n        level of logging that the logger is set to handle\n\n    Methods\n    -------\n    setup_logger(name_logfile, path_logfile, print_console=False)\n        initialize or get logger object with name_logfile\n        that writes to path_logfile\n\n    Examples\n    --------\n    &gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n    ...    print(\"this will print to logfile\")\n    ...    logger.logger.info(\"this will log to the logfile\")\n    ... print(\"this will print to the console\")\n    ... logger.logger.info(\"this will log to the logfile\")\n\n    \"\"\"\n\n    def __init__(self, name, path, level=\"INFO\", **kwargs):\n        self.logger = self.setup_logger(name, path, **kwargs)\n        self.name = self.logger.name\n        self.level = getattr(logging, level)\n\n    def setup_logger(\n        self, name_logfile, path_logfile, print_console=False\n    ) -&gt; logging.Logger:\n        \"\"\"\n        Sets up a logger for that outputs to a file, and optionally, the console\n\n        Parameters\n        ----------\n        name_logfile : str\n            name of the logfile to use\n        path_logfile : str\n            path to the file that should be used as the file handler\n        print_console : bool, default-False\n            if True, prints to console as well as log file.\n\n        Returns\n        -------\n        logger : logging.Logger\n            the logger object with specified handlers\n        \"\"\"\n\n        logger = logging.getLogger(name_logfile)\n        # check to see if handlers already exist for this logger\n        if logger.handlers:\n            for handler in logger.handlers:\n                # if it's a file handler\n                # type is used instead of isinstance,\n                # which doesn't work properly with logging.StreamHandler\n                if type(handler) == logging.FileHandler:\n                    # if paths don't match, change file handler path\n                    if not os.path.samefile(handler.baseFilename, path_logfile):\n                        handler.close()\n                        logger.removeHandler(handler)\n                        file_handler = self._get_file_handler(path_logfile)\n                        logger.addHandler(file_handler)\n                # if a stream handler exists and\n                # if print_console is False remove streamHandler\n                if type(handler) == logging.StreamHandler:\n                    if not print_console:\n                        handler.close()\n                        logger.removeHandler(handler)\n            if print_console and not any(\n                type(handler) == logging.StreamHandler\n                for handler in logger.handlers\n            ):\n                logger.addHandler(self._get_stream_handler())\n\n        else:\n            file_handler = self._get_file_handler(path_logfile)\n            logger.addHandler(file_handler)\n            if print_console:\n                logger.addHandler(self._get_stream_handler())\n        logger.setLevel(logging.INFO)\n        return logger\n\n    def _get_file_handler(self, path):\n        output_dir = pathlib.Path(os.path.dirname(path))\n        if not os.path.exists(output_dir):\n            output_dir.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(path, mode=\"a\")\n        file_handler.setFormatter(self._get_formatter())\n        return file_handler\n\n    def _get_stream_handler(self):\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(self._get_formatter())\n        return stream_handler\n\n    def _get_formatter(self):\n        return logging.Formatter(\n            \"[%(asctime)s] in %(pathname)s, line %(lineno)d: %(message)s\",\n            datefmt=\"%d-%b-%y %H:%M:%S\",\n        )\n\n    def write(self, msg):\n        if msg and not msg.isspace():\n            self.logger.log(self.level, msg)\n\n    def flush(self):\n        pass\n\n    def __enter__(self):\n        self._redirector = redirect_stdout(self)\n        self._redirector.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # let contextlib do any exception handling here\n        self._redirector.__exit__(exc_type, exc_value, traceback)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.OutputLogger.setup_logger", "title": "<code>setup_logger(name_logfile, path_logfile, print_console=False)</code>", "text": "<p>Sets up a logger for that outputs to a file, and optionally, the console</p> <p>Parameters:</p> Name Type Description Default <code>name_logfile</code> <code>str</code> <p>name of the logfile to use</p> required <code>path_logfile</code> <code>str</code> <p>path to the file that should be used as the file handler</p> required <code>print_console</code> <code>(bool, default - False)</code> <p>if True, prints to console as well as log file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>the logger object with specified handlers</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def setup_logger(\n    self, name_logfile, path_logfile, print_console=False\n) -&gt; logging.Logger:\n    \"\"\"\n    Sets up a logger for that outputs to a file, and optionally, the console\n\n    Parameters\n    ----------\n    name_logfile : str\n        name of the logfile to use\n    path_logfile : str\n        path to the file that should be used as the file handler\n    print_console : bool, default-False\n        if True, prints to console as well as log file.\n\n    Returns\n    -------\n    logger : logging.Logger\n        the logger object with specified handlers\n    \"\"\"\n\n    logger = logging.getLogger(name_logfile)\n    # check to see if handlers already exist for this logger\n    if logger.handlers:\n        for handler in logger.handlers:\n            # if it's a file handler\n            # type is used instead of isinstance,\n            # which doesn't work properly with logging.StreamHandler\n            if type(handler) == logging.FileHandler:\n                # if paths don't match, change file handler path\n                if not os.path.samefile(handler.baseFilename, path_logfile):\n                    handler.close()\n                    logger.removeHandler(handler)\n                    file_handler = self._get_file_handler(path_logfile)\n                    logger.addHandler(file_handler)\n            # if a stream handler exists and\n            # if print_console is False remove streamHandler\n            if type(handler) == logging.StreamHandler:\n                if not print_console:\n                    handler.close()\n                    logger.removeHandler(handler)\n        if print_console and not any(\n            type(handler) == logging.StreamHandler\n            for handler in logger.handlers\n        ):\n            logger.addHandler(self._get_stream_handler())\n\n    else:\n        file_handler = self._get_file_handler(path_logfile)\n        logger.addHandler(file_handler)\n        if print_console:\n            logger.addHandler(self._get_stream_handler())\n    logger.setLevel(logging.INFO)\n    return logger\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_dlc_processed_data_dir", "title": "<code>get_dlc_processed_data_dir()</code>", "text": "<p>Returns session_dir relative to custom 'dlc_output_dir' root</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_dlc_processed_data_dir() -&gt; str:\n    \"\"\"Returns session_dir relative to custom 'dlc_output_dir' root\"\"\"\n    if \"custom\" in dj.config:\n        if \"dlc_output_dir\" in dj.config[\"custom\"]:\n            dlc_output_dir = dj.config.get(\"custom\", {}).get(\"dlc_output_dir\")\n    if dlc_output_dir:\n        return pathlib.Path(dlc_output_dir)\n    else:\n        return pathlib.Path(\"/nimbus/deeplabcut/output/\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.find_full_path", "title": "<code>find_full_path(root_directories, relative_path)</code>", "text": "<p>from Datajoint Elements - unused Given a relative path, search and return the full-path  from provided potential root directories (in the given order)     :param root_directories: potential root directories     :param relative_path: the relative path to find the valid root directory     :return: full-path (pathlib.Path object)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def find_full_path(root_directories, relative_path):\n    \"\"\"\n    from Datajoint Elements - unused\n    Given a relative path, search and return the full-path\n     from provided potential root directories (in the given order)\n        :param root_directories: potential root directories\n        :param relative_path: the relative path to find the valid root directory\n        :return: full-path (pathlib.Path object)\n    \"\"\"\n    relative_path = _to_Path(relative_path)\n\n    if relative_path.exists():\n        return relative_path\n\n    # Turn to list if only a single root directory is provided\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [_to_Path(root_directories)]\n\n    for root_dir in root_directories:\n        if (_to_Path(root_dir) / relative_path).exists():\n            return _to_Path(root_dir) / relative_path\n\n    raise FileNotFoundError(\n        f\"No valid full-path found (from {root_directories})\"\n        f\" for {relative_path}\"\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.find_root_directory", "title": "<code>find_root_directory(root_directories, full_path)</code>", "text": "<p>From datajoint elements - unused Given multiple potential root directories and a full-path, search and return one directory that is the parent of the given path     :param root_directories: potential root directories     :param full_path: the full path to search the root directory     :return: root_directory (pathlib.Path object)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def find_root_directory(root_directories, full_path):\n    \"\"\"\n    From datajoint elements - unused\n    Given multiple potential root directories and a full-path,\n    search and return one directory that is the parent of the given path\n        :param root_directories: potential root directories\n        :param full_path: the full path to search the root directory\n        :return: root_directory (pathlib.Path object)\n    \"\"\"\n    full_path = _to_Path(full_path)\n\n    if not full_path.exists():\n        raise FileNotFoundError(f\"{full_path} does not exist!\")\n\n    # Turn to list if only a single root directory is provided\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [_to_Path(root_directories)]\n\n    try:\n        return next(\n            _to_Path(root_dir)\n            for root_dir in root_directories\n            if _to_Path(root_dir) in set(full_path.parents)\n        )\n\n    except StopIteration as exc:\n        raise FileNotFoundError(\n            f\"No valid root directory found (from {root_directories})\"\n            f\" for {full_path}\"\n        ) from exc\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.infer_output_dir", "title": "<code>infer_output_dir(key, makedir=True)</code>", "text": "<p>Return the expected pose_estimation_output_dir.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def infer_output_dir(key, makedir=True):\n    \"\"\"Return the expected pose_estimation_output_dir.\n\n    Parameters\n    ----------\n    key: DataJoint key specifying a pairing of VideoFile and Model.\n    \"\"\"\n    # TODO: add check to make sure interval_list_name refers to a single epoch\n    # Or make key include epoch in and of itself instead of interval_list_name\n    nwb_file_name = key[\"nwb_file_name\"].split(\"_.\")[0]\n    output_dir = pathlib.Path(os.getenv(\"DLC_OUTPUT_PATH\")) / pathlib.Path(\n        f\"{nwb_file_name}/{nwb_file_name}_{key['epoch']:02}\"\n        f\"_model_\" + key[\"dlc_model_name\"].replace(\" \", \"-\")\n    )\n    if makedir is True:\n        if not os.path.exists(output_dir):\n            output_dir.mkdir(parents=True, exist_ok=True)\n    return output_dir\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_video_path", "title": "<code>get_video_path(key)</code>", "text": "<p>Given nwb_file_name and interval_list_name returns specified video file filename and path</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dictionary containing nwb_file_name and interval_list_name as keys</p> required <p>Returns:</p> Name Type Description <code>video_filepath</code> <code>str</code> <p>path to the video file, including video filename</p> <code>video_filename</code> <code>str</code> <p>filename of the video</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_video_path(key):\n    \"\"\"\n    Given nwb_file_name and interval_list_name returns specified\n    video file filename and path\n\n    Parameters\n    ----------\n    key : dict\n        Dictionary containing nwb_file_name and interval_list_name as keys\n\n    Returns\n    -------\n    video_filepath : str\n        path to the video file, including video filename\n    video_filename : str\n        filename of the video\n    \"\"\"\n    import pynwb\n\n    from ...common.common_behav import VideoFile\n\n    vf_key = {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n    VideoFile()._no_transaction_make(vf_key, verbose=False)\n    video_query = VideoFile &amp; vf_key\n\n    if len(video_query) != 1:\n        print(f\"Found {len(video_query)} videos for {vf_key}\")\n        return None, None, None, None\n\n    video_info = video_query.fetch1()\n    nwb_path = f\"{raw_dir}/{video_info['nwb_file_name']}\"\n\n    with pynwb.NWBHDF5IO(path=nwb_path, mode=\"r\") as in_out:\n        nwb_file = in_out.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filepath = VideoFile.get_abs_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n        )\n        video_dir = os.path.dirname(video_filepath) + \"/\"\n        video_filename = video_filepath.split(video_dir)[-1]\n        meters_per_pixel = nwb_video.device.meters_per_pixel\n        timestamps = np.asarray(nwb_video.timestamps)\n\n    return video_dir, video_filename, meters_per_pixel, timestamps\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.check_videofile", "title": "<code>check_videofile(video_path, output_path=os.getenv('DLC_VIDEO_PATH'), video_filename=None, video_filetype='h264')</code>", "text": "<p>Checks the file extension of a video file to make sure it is .mp4 for DeepLabCut processes. Converts to MP4 if not already.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str or PosixPath object</code> <p>path to directory of the existing video file without filename</p> required <code>output_path</code> <code>str or PosixPath object</code> <p>path to directory where converted video will be saved</p> <code>getenv('DLC_VIDEO_PATH')</code> <code>video_filename</code> <code>(str, Optional)</code> <p>filename of the video to convert, if not provided, video_filetype must be and all video files of video_filetype in the directory will be converted</p> <code>None</code> <code>video_filetype</code> <code>str or List, Default 'h264', Optional</code> <p>If video_filename is not provided, all videos of this filetype will be converted to .mp4</p> <code>'h264'</code> <p>Returns:</p> Name Type Description <code>output_files</code> <code>List of PosixPath objects</code> <p>paths to converted video file(s)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def check_videofile(\n    video_path: Union[str, pathlib.PosixPath],\n    output_path: Union[str, pathlib.PosixPath] = os.getenv(\"DLC_VIDEO_PATH\"),\n    video_filename: str = None,\n    video_filetype: str = \"h264\",\n):\n    \"\"\"\n    Checks the file extension of a video file to make sure it is .mp4 for\n    DeepLabCut processes. Converts to MP4 if not already.\n\n    Parameters\n    ----------\n    video_path : str or PosixPath object\n        path to directory of the existing video file without filename\n    output_path : str or PosixPath object\n        path to directory where converted video will be saved\n    video_filename : str, Optional\n        filename of the video to convert, if not provided, video_filetype must be\n        and all video files of video_filetype in the directory will be converted\n    video_filetype : str or List, Default 'h264', Optional\n        If video_filename is not provided,\n        all videos of this filetype will be converted to .mp4\n\n    Returns\n    -------\n    output_files : List of PosixPath objects\n        paths to converted video file(s)\n    \"\"\"\n\n    if not video_filename:\n        video_files = pathlib.Path(video_path).glob(f\"*.{video_filetype}\")\n    else:\n        video_files = [pathlib.Path(f\"{video_path}/{video_filename}\")]\n    output_files = []\n    for video_filepath in video_files:\n        if video_filepath.exists():\n            if video_filepath.suffix == \".mp4\":\n                output_files.append(video_filepath)\n                continue\n        video_file = (\n            video_filepath.as_posix()\n            .rsplit(video_filepath.parent.as_posix(), maxsplit=1)[-1]\n            .split(\"/\")[-1]\n        )\n        output_files.append(\n            _convert_mp4(video_file, video_path, output_path, videotype=\"mp4\")\n        )\n    return output_files\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_gpu_memory", "title": "<code>get_gpu_memory()</code>", "text": "<p>Queries the gpu cluster and returns the memory use for each core. This is used to evaluate which GPU cores are available to run jobs on (i.e. pose estimation, DLC model training)</p> <p>Returns:</p> Name Type Description <code>memory_use_values</code> <code>dict</code> <p>dictionary with core number as key and memory in use as value.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if subproccess command errors.</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_gpu_memory():\n    \"\"\"Queries the gpu cluster and returns the memory use for each core.\n    This is used to evaluate which GPU cores are available to run jobs on\n    (i.e. pose estimation, DLC model training)\n\n    Returns\n    -------\n    memory_use_values : dict\n        dictionary with core number as key and memory in use as value.\n\n    Raises\n    ------\n    RuntimeError\n        if subproccess command errors.\n    \"\"\"\n\n    def output_to_list(x):\n        return x.decode(\"ascii\").split(\"\\n\")[:-1]\n\n    query_cmd = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n    try:\n        memory_use_info = output_to_list(\n            subprocess.check_output(query_cmd.split(), stderr=subprocess.STDOUT)\n        )[1:]\n    except subprocess.CalledProcessError as err:\n        raise RuntimeError(\n            f\"command {err.cmd} return with error (code {err.returncode}): \"\n            + f\"{err.output}\"\n        ) from err\n    memory_use_values = {\n        i: int(x.split()[0]) for i, x in enumerate(memory_use_info)\n    }\n    return memory_use_values\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_span_start_stop", "title": "<code>get_span_start_stop(indices)</code>", "text": "<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_span_start_stop(indices):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    indices : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    span_inds = []\n    # Get start and stop index of spans of consecutive indices\n    for k, g in groupby(enumerate(indices), lambda x: x[1] - x[0]):\n        group = list(map(itemgetter(1), g))\n        span_inds.append((group[0], group[-1]))\n    return span_inds\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.convert_to_pixels", "title": "<code>convert_to_pixels(data, frame_size, cm_to_pixels=1.0)</code>", "text": "<p>Converts from cm to pixels and flips the y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>(ndarray, shape(n_time, 2))</code> required <code>frame_size</code> <code>(array_like, shape(2))</code> required <code>cm_to_pixels</code> <code>float</code> <code>1.0</code> <p>Returns:</p> Name Type Description <code>converted_data</code> <code>(ndarray, shape(n_time, 2))</code> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n    \"\"\"Converts from cm to pixels and flips the y-axis.\n    Parameters\n    ----------\n    data : ndarray, shape (n_time, 2)\n    frame_size : array_like, shape (2,)\n    cm_to_pixels : float\n\n    Returns\n    -------\n    converted_data : ndarray, shape (n_time, 2)\n    \"\"\"\n    return data / cm_to_pixels\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/", "title": "position_dlc_centroid.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidParams", "title": "<code>DLCCentroidParams</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Parameters for calculating the centroid</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@schema\nclass DLCCentroidParams(dj.Manual):\n    \"\"\"\n    Parameters for calculating the centroid\n    \"\"\"\n\n    # TODO: whether to keep all params in a params dict\n    # or break out into individual secondary keys\n    definition = \"\"\"\n    dlc_centroid_params_name: varchar(80) # name for this set of parameters\n    ---\n    params: longblob\n    \"\"\"\n\n    _available_centroid_methods = [\n        \"four_led_centroid\",\n        \"two_pt_centroid\",\n        \"one_pt_centroid\",\n    ]\n    _four_led_labels = [\"greenLED\", \"redLED_L\", \"redLED_C\", \"redLED_R\"]\n    _two_pt_labels = [\"point1\", \"point2\"]\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        \"\"\"\n        Inserts default centroid parameters. Assumes 2 LEDs tracked\n        \"\"\"\n        params = {\n            \"centroid_method\": \"two_pt_centroid\",\n            \"points\": {\n                \"point1\": \"greenLED\",\n                \"point2\": \"redLED_C\",\n            },\n            \"interpolate\": True,\n            \"interp_params\": {\"max_cm_to_interp\": 15},\n            \"smooth\": True,\n            \"smoothing_params\": {\n                \"smoothing_duration\": 0.05,\n                \"smooth_method\": \"moving_avg\",\n            },\n            \"max_LED_separation\": 12,\n            \"speed_smoothing_std_dev\": 0.100,\n        }\n        cls.insert1(\n            {\"dlc_centroid_params_name\": \"default\", \"params\": params}, **kwargs\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_centroid_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (cls &amp; {\"dlc_centroid_params_name\": \"default\"}).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n\n    def insert1(self, key, **kwargs):\n        \"\"\"\n        Check provided parameter dictionary to make sure\n        it contains all necessary items\n        \"\"\"\n        params = key[\"params\"]\n        if \"centroid_method\" in params:\n            if params[\"centroid_method\"] in self._available_centroid_methods:\n                if params[\"centroid_method\"] == \"four_led_centroid\":\n                    if any(\n                        x not in self._four_led_labels for x in params[\"points\"]\n                    ):\n                        raise KeyError(\n                            f\"Please make sure to specify all necessary labels: \"\n                            f\"{self._four_led_labels} \"\n                            f\"if using the 'four_led_centroid' method\"\n                        )\n                elif params[\"centroid_method\"] == \"two_pt_centroid\":\n                    if any(\n                        x not in self._two_pt_labels for x in params[\"points\"]\n                    ):\n                        raise KeyError(\n                            f\"Please make sure to specify all necessary labels: \"\n                            f\"{self._two_pt_labels} \"\n                            f\"if using the 'two_pt_centroid' method\"\n                        )\n                elif params[\"centroid_method\"] == \"one_pt_centroid\":\n                    if \"point1\" not in params[\"points\"]:\n                        raise KeyError(\n                            \"Please make sure to specify the necessary label: \"\n                            \"'point1' \"\n                            \"if using the 'one_pt_centroid' method\"\n                        )\n                else:\n                    raise Exception(\"This shouldn't happen lol oops\")\n            else:\n                raise ValueError(\n                    f\"The given 'centroid_method': {params['centroid_method']} \"\n                    f\"is not in the available methods: \"\n                    f\"{self._available_centroid_methods}\"\n                )\n        else:\n            raise KeyError(\n                \"'centroid_method' needs to be provided as a parameter\"\n            )\n\n        if \"max_LED_separation\" in params:\n            if not isinstance(params[\"max_LED_separation\"], (int, float)):\n                raise TypeError(\n                    f\"parameter 'max_LED_separation' is type: \"\n                    f\"{type(params['max_LED_separation'])}, \"\n                    f\"it should be one of type (float, int)\"\n                )\n        if \"smooth\" in params:\n            if params[\"smooth\"]:\n                if \"smoothing_params\" in params:\n                    if \"smooth_method\" in params[\"smoothing_params\"]:\n                        smooth_method = params[\"smoothing_params\"][\n                            \"smooth_method\"\n                        ]\n                        if smooth_method not in _key_to_smooth_func_dict:\n                            raise KeyError(\n                                f\"smooth_method: {smooth_method} not an available method.\"\n                            )\n                    if not \"smoothing_duration\" in params[\"smoothing_params\"]:\n                        raise KeyError(\n                            \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                        )\n                    else:\n                        assert isinstance(\n                            params[\"smoothing_params\"][\"smoothing_duration\"],\n                            (float, int),\n                        ), \"smoothing_duration must be a float or int\"\n                else:\n                    raise ValueError(\"smoothing_params not in key['params']\")\n\n        super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidParams.insert_default", "title": "<code>insert_default(**kwargs)</code>  <code>classmethod</code>", "text": "<p>Inserts default centroid parameters. Assumes 2 LEDs tracked</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@classmethod\ndef insert_default(cls, **kwargs):\n    \"\"\"\n    Inserts default centroid parameters. Assumes 2 LEDs tracked\n    \"\"\"\n    params = {\n        \"centroid_method\": \"two_pt_centroid\",\n        \"points\": {\n            \"point1\": \"greenLED\",\n            \"point2\": \"redLED_C\",\n        },\n        \"interpolate\": True,\n        \"interp_params\": {\"max_cm_to_interp\": 15},\n        \"smooth\": True,\n        \"smoothing_params\": {\n            \"smoothing_duration\": 0.05,\n            \"smooth_method\": \"moving_avg\",\n        },\n        \"max_LED_separation\": 12,\n        \"speed_smoothing_std_dev\": 0.100,\n    }\n    cls.insert1(\n        {\"dlc_centroid_params_name\": \"default\", \"params\": params}, **kwargs\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidParams.insert1", "title": "<code>insert1(key, **kwargs)</code>", "text": "<p>Check provided parameter dictionary to make sure it contains all necessary items</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def insert1(self, key, **kwargs):\n    \"\"\"\n    Check provided parameter dictionary to make sure\n    it contains all necessary items\n    \"\"\"\n    params = key[\"params\"]\n    if \"centroid_method\" in params:\n        if params[\"centroid_method\"] in self._available_centroid_methods:\n            if params[\"centroid_method\"] == \"four_led_centroid\":\n                if any(\n                    x not in self._four_led_labels for x in params[\"points\"]\n                ):\n                    raise KeyError(\n                        f\"Please make sure to specify all necessary labels: \"\n                        f\"{self._four_led_labels} \"\n                        f\"if using the 'four_led_centroid' method\"\n                    )\n            elif params[\"centroid_method\"] == \"two_pt_centroid\":\n                if any(\n                    x not in self._two_pt_labels for x in params[\"points\"]\n                ):\n                    raise KeyError(\n                        f\"Please make sure to specify all necessary labels: \"\n                        f\"{self._two_pt_labels} \"\n                        f\"if using the 'two_pt_centroid' method\"\n                    )\n            elif params[\"centroid_method\"] == \"one_pt_centroid\":\n                if \"point1\" not in params[\"points\"]:\n                    raise KeyError(\n                        \"Please make sure to specify the necessary label: \"\n                        \"'point1' \"\n                        \"if using the 'one_pt_centroid' method\"\n                    )\n            else:\n                raise Exception(\"This shouldn't happen lol oops\")\n        else:\n            raise ValueError(\n                f\"The given 'centroid_method': {params['centroid_method']} \"\n                f\"is not in the available methods: \"\n                f\"{self._available_centroid_methods}\"\n            )\n    else:\n        raise KeyError(\n            \"'centroid_method' needs to be provided as a parameter\"\n        )\n\n    if \"max_LED_separation\" in params:\n        if not isinstance(params[\"max_LED_separation\"], (int, float)):\n            raise TypeError(\n                f\"parameter 'max_LED_separation' is type: \"\n                f\"{type(params['max_LED_separation'])}, \"\n                f\"it should be one of type (float, int)\"\n            )\n    if \"smooth\" in params:\n        if params[\"smooth\"]:\n            if \"smoothing_params\" in params:\n                if \"smooth_method\" in params[\"smoothing_params\"]:\n                    smooth_method = params[\"smoothing_params\"][\n                        \"smooth_method\"\n                    ]\n                    if smooth_method not in _key_to_smooth_func_dict:\n                        raise KeyError(\n                            f\"smooth_method: {smooth_method} not an available method.\"\n                        )\n                if not \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    raise KeyError(\n                        \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                    )\n                else:\n                    assert isinstance(\n                        params[\"smoothing_params\"][\"smoothing_duration\"],\n                        (float, int),\n                    ), \"smoothing_duration must be a float or int\"\n            else:\n                raise ValueError(\"smoothing_params not in key['params']\")\n\n    super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidSelection", "title": "<code>DLCCentroidSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Table to pair a cohort of bodypart entries with the parameters for calculating their centroid</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@schema\nclass DLCCentroidSelection(dj.Manual):\n    \"\"\"\n    Table to pair a cohort of bodypart entries with\n    the parameters for calculating their centroid\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohort\n    -&gt; DLCCentroidParams\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroid", "title": "<code>DLCCentroid</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Table to calculate the centroid of a group of bodyparts</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@schema\nclass DLCCentroid(dj.Computed):\n    \"\"\"\n    Table to calculate the centroid of a group of bodyparts\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCCentroidSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_position_object_id : varchar(80)\n    dlc_velocity_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        idx = pd.IndexSlice\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Centroid Calculation\")\n            # Get labels to smooth from Parameters table\n            cohort_entries = DLCSmoothInterpCohort.BodyPart &amp; key\n            params = (DLCCentroidParams() &amp; key).fetch1(\"params\")\n            centroid_method = params.pop(\"centroid_method\")\n            bodyparts_avail = cohort_entries.fetch(\"bodypart\")\n            speed_smoothing_std_dev = params.pop(\"speed_smoothing_std_dev\")\n            # TODO, generalize key naming\n            if centroid_method == \"four_led_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"greenLED\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"greenLED\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"greenLED\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A green led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_L\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_L\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_L\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A left red led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_C\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_C\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_C\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A center red led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_R\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_R\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_R\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A right red led needs to be specified for the 4 led centroid method\"\n                    )\n                bodyparts_to_use = [\n                    params[\"points\"][\"greenLED\"],\n                    params[\"points\"][\"redLED_L\"],\n                    params[\"points\"][\"redLED_C\"],\n                    params[\"points\"][\"redLED_R\"],\n                ]\n\n            elif centroid_method == \"two_pt_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"point1\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point1\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point1\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point1 needs to be specified for the 2 pt centroid method\"\n                    )\n                if \"point2\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point2\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point2\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point2 needs to be specified for the 2 pt centroid method\"\n                    )\n                bodyparts_to_use = [\n                    params[\"points\"][\"point1\"],\n                    params[\"points\"][\"point2\"],\n                ]\n\n            elif centroid_method == \"one_pt_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"point1\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point1\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point1\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point1 needs to be specified for the 1 pt centroid method\"\n                    )\n                bodyparts_to_use = [params[\"points\"][\"point1\"]]\n\n            else:\n                raise ValueError(\"Please specify a centroid method to use.\")\n            pos_df = pd.concat(\n                {\n                    bodypart: (\n                        DLCSmoothInterpCohort.BodyPart\n                        &amp; {**key, **{\"bodypart\": bodypart}}\n                    ).fetch1_dataframe()\n                    for bodypart in bodyparts_to_use\n                },\n                axis=1,\n            )\n            dt = np.median(np.diff(pos_df.index.to_numpy()))\n            sampling_rate = 1 / dt\n            logger.logger.info(\n                \"Calculating centroid with %s\", str(centroid_method)\n            )\n            centroid = centroid_func(pos_df, **params)\n            centroid_df = pd.DataFrame(\n                centroid,\n                columns=[\"x\", \"y\"],\n                index=pos_df.index.to_numpy(),\n            )\n            if params[\"interpolate\"]:\n                if np.any(np.isnan(centroid)):\n                    logger.logger.info(\"interpolating over NaNs\")\n                    nan_inds = (\n                        pd.isnull(centroid_df.loc[:, idx[(\"x\", \"y\")]])\n                        .any(axis=1)\n                        .to_numpy()\n                        .nonzero()[0]\n                    )\n                    nan_spans = get_span_start_stop(nan_inds)\n                    interp_df = interp_pos(\n                        centroid_df.copy(), nan_spans, **params[\"interp_params\"]\n                    )\n                else:\n                    logger.logger.info(\"no NaNs to interpolate over\")\n                    interp_df = centroid_df.copy()\n            else:\n                interp_df = centroid_df.copy()\n            if params[\"smooth\"]:\n                if \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    smoothing_duration = params[\"smoothing_params\"].pop(\n                        \"smoothing_duration\"\n                    )\n                    dt = np.median(np.diff(pos_df.index.to_numpy()))\n                    sampling_rate = 1 / dt\n                    logger.logger.info(\"smoothing position\")\n                    smooth_func = _key_to_smooth_func_dict[\n                        params[\"smoothing_params\"][\"smooth_method\"]\n                    ]\n                    logger.logger.info(\n                        \"Smoothing using method: %s\",\n                        str(params[\"smoothing_params\"][\"smooth_method\"]),\n                    )\n                    final_df = smooth_func(\n                        interp_df,\n                        smoothing_duration=smoothing_duration,\n                        sampling_rate=sampling_rate,\n                        **params[\"smoothing_params\"],\n                    )\n                else:\n                    raise KeyError(\n                        \"smoothing_duration needs to be passed within smoothing_params\"\n                    )\n            else:\n                final_df = interp_df.copy()\n            logger.logger.info(\"getting velocity\")\n            velocity = get_velocity(\n                final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                time=pos_df.index.to_numpy(),\n                sigma=speed_smoothing_std_dev,\n                sampling_frequency=sampling_rate,\n            )  # cm/s\n            speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n            # Create dataframe\n            velocity_df = pd.DataFrame(\n                np.concatenate((velocity, speed[:, np.newaxis]), axis=1),\n                columns=[\"velocity_x\", \"velocity_y\", \"speed\"],\n                index=pos_df.index.to_numpy(),\n            )\n            total_nan = np.sum(\n                final_df.loc[:, idx[(\"x\", \"y\")]].isna().any(axis=1)\n            )\n            pretrack_nan = np.sum(\n                final_df.iloc[:1000].loc[:, idx[(\"x\", \"y\")]].isna().any(axis=1)\n            )\n            logger.logger.info(\"total NaNs in centroid dataset: %d\", total_nan)\n            logger.logger.info(\n                \"NaNs in centroid dataset before ind 1000: %d\", pretrack_nan\n            )\n            position = pynwb.behavior.Position()\n            velocity = pynwb.behavior.BehavioralTimeSeries()\n            spatial_series = (RawPosition() &amp; key).fetch_nwb()[0][\n                \"raw_position\"\n            ]\n            METERS_PER_CM = 0.01\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=final_df.index.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            velocity.create_timeseries(\n                name=\"velocity\",\n                timestamps=velocity_df.index.to_numpy(),\n                conversion=METERS_PER_CM,\n                unit=\"m/s\",\n                data=velocity_df.loc[\n                    :, idx[(\"velocity_x\", \"velocity_y\", \"speed\")]\n                ].to_numpy(),\n                comments=spatial_series.comments,\n                description=\"x_velocity, y_velocity, speed\",\n            )\n            velocity.create_timeseries(\n                name=\"video_frame_ind\",\n                unit=\"index\",\n                timestamps=final_df.index.to_numpy(),\n                data=pos_df[\n                    pos_df.columns.levels[0][0]\n                ].video_frame_ind.to_numpy(),\n                description=\"video_frame_ind\",\n                comments=\"no comments\",\n            )\n            # Add to Analysis NWB file\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\"dlc_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n                key[\"analysis_file_name\"], position\n            )\n            key[\"dlc_velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n                key[\"analysis_file_name\"], velocity\n            )\n\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.insert1(key)\n            logger.logger.info(\"inserted entry into DLCCentroid\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_position\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"dlc_velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"dlc_position\"].get_spatial_series().data\n                    ),\n                    np.asarray(\n                        nwb_data[\"dlc_velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.four_led_centroid", "title": "<code>four_led_centroid(pos_df, **params)</code>", "text": "<p>Determines the centroid of 4 LEDS on an implant LED ring. Assumed to be the Green LED, and 3 red LEDs called: redLED_C, redLED_L, redLED_R By default, uses (greenled + redLED_C) / 2 to calculate centroid If Green LED is NaN, but red center LED is not,     then the red center LED is called the centroid If green and red center LEDs are NaN, but red left and red right LEDs are not,     then the centroid is (redLED_L + redLED_R) / 2 If red center LED is NaN, but the other 3 LEDS are not,     then the centroid is (greenled + (redLED_L + redLED_R) / 2) / 2 If red center and left LEDs are NaN, but green and red right LEDs are not,     then the centroid is (greenled + redLED_R) / 2 If red center and right LEDs are NaN, but green and red left LEDs are not,     then the centroid is (greenled + redLED_L) / 2 If all red LEDs are NaN, but green LED is not,     then the green LED is called the centroid If all LEDs are NaN, then the centroid is NaN</p> <p>Parameters:</p> Name Type Description Default <code>pos_df</code> <code>DataFrame</code> <p>dataframe containing x and y position for each LED of interest, index is timestamps. Column names specified by params</p> required <code>**params</code> <code>dict</code> <p>contains 'greenLED' and 'redLED_C', 'redLED_R', 'redLED_L' keys, whose values specify the column names in <code>pos_df</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>centroid</code> <code>ndarray</code> <p>numpy array with shape (n_time, 2) centroid[0] is the x coord and centroid[1] is the y coord</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def four_led_centroid(pos_df: pd.DataFrame, **params):\n    \"\"\"\n    Determines the centroid of 4 LEDS on an implant LED ring.\n    Assumed to be the Green LED, and 3 red LEDs called: redLED_C, redLED_L, redLED_R\n    By default, uses (greenled + redLED_C) / 2 to calculate centroid\n    If Green LED is NaN, but red center LED is not,\n        then the red center LED is called the centroid\n    If green and red center LEDs are NaN, but red left and red right LEDs are not,\n        then the centroid is (redLED_L + redLED_R) / 2\n    If red center LED is NaN, but the other 3 LEDS are not,\n        then the centroid is (greenled + (redLED_L + redLED_R) / 2) / 2\n    If red center and left LEDs are NaN, but green and red right LEDs are not,\n        then the centroid is (greenled + redLED_R) / 2\n    If red center and right LEDs are NaN, but green and red left LEDs are not,\n        then the centroid is (greenled + redLED_L) / 2\n    If all red LEDs are NaN, but green LED is not,\n        then the green LED is called the centroid\n    If all LEDs are NaN, then the centroid is NaN\n\n    Parameters\n    ----------\n    pos_df : pd.DataFrame\n        dataframe containing x and y position for each LED of interest,\n        index is timestamps. Column names specified by params\n    **params : dict\n        contains 'greenLED' and 'redLED_C', 'redLED_R', 'redLED_L' keys,\n        whose values specify the column names in `pos_df`\n\n    Returns\n    -------\n    centroid : np.ndarray\n        numpy array with shape (n_time, 2)\n        centroid[0] is the x coord and centroid[1] is the y coord\n    \"\"\"\n    centroid = np.zeros(shape=(len(pos_df), 2))\n    idx = pd.IndexSlice\n    # TODO: this feels messy, clean-up\n    green_led = params[\"points\"].pop(\"greenLED\", None)\n    red_led_C = params[\"points\"].pop(\"redLED_C\", None)\n    red_led_L = params[\"points\"].pop(\"redLED_L\", None)\n    red_led_R = params[\"points\"].pop(\"redLED_R\", None)\n    green_nans = pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].isna().any(axis=1)\n    red_C_nans = pos_df.loc[:, idx[red_led_C, (\"x\", \"y\")]].isna().any(axis=1)\n    red_L_nans = pos_df.loc[:, idx[red_led_L, (\"x\", \"y\")]].isna().any(axis=1)\n    red_R_nans = pos_df.loc[:, idx[red_led_R, (\"x\", \"y\")]].isna().any(axis=1)\n    # TODO: implement checks to make sure not rewriting previously set index in centroid\n    # If all given LEDs are not NaN\n    dist_between_green_red = get_distance(\n        pos_df.loc[:, idx[red_led_C, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].to_numpy(),\n    )\n    g_c_is_too_separated = (\n        dist_between_green_red &gt;= params[\"max_LED_separation\"]\n    )\n    all_good_mask = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            ~red_C_nans,\n            ~red_L_nans,\n            ~red_R_nans,\n            ~g_c_is_too_separated,\n        ),\n    )\n    centroid[all_good_mask] = [\n        *zip(\n            (\n                pos_df.loc[idx[all_good_mask], idx[red_led_C, \"x\"]]\n                + pos_df.loc[idx[all_good_mask], idx[green_led, \"x\"]]\n            )\n            / 2,\n            (\n                pos_df.loc[idx[all_good_mask], idx[red_led_C, \"y\"]]\n                + pos_df.loc[idx[all_good_mask], idx[green_led, \"y\"]]\n            )\n            / 2,\n        )\n    ]\n    # If green LED and red center LED are both not NaN\n    green_red_C = np.logical_and(\n        ~green_nans, ~red_C_nans, ~g_c_is_too_separated\n    )\n    if np.sum(green_red_C) &gt; 0:\n        centroid[green_red_C] = [\n            *zip(\n                (\n                    pos_df.loc[idx[green_red_C], idx[red_led_C, \"x\"]]\n                    + pos_df.loc[idx[green_red_C], idx[green_led, \"x\"]]\n                )\n                / 2,\n                (\n                    pos_df.loc[idx[green_red_C], idx[red_led_C, \"y\"]]\n                    + pos_df.loc[idx[green_red_C], idx[green_led, \"y\"]]\n                )\n                / 2,\n            )\n        ]\n    # If all given LEDs are NaN\n    all_bad_mask = reduce(\n        np.logical_and, (green_nans, red_C_nans, red_L_nans, red_R_nans)\n    )\n    centroid[all_bad_mask, :] = np.nan\n    # If green LED is NaN, but red center LED is not\n    no_green_red_C = np.logical_and(green_nans, ~red_C_nans)\n    if np.sum(no_green_red_C) &gt; 0:\n        centroid[no_green_red_C] = [\n            *zip(\n                pos_df.loc[idx[no_green_red_C], idx[red_led_C, \"x\"]],\n                pos_df.loc[idx[no_green_red_C], idx[red_led_C, \"y\"]],\n            )\n        ]\n    # If green and red center LEDs are NaN, but red left and red right LEDs are not\n    dist_between_left_right = get_distance(\n        pos_df.loc[:, idx[red_led_L, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[red_led_R, (\"x\", \"y\")]].to_numpy(),\n    )\n    l_r_is_too_separated = (\n        dist_between_left_right &gt;= params[\"max_LED_separation\"]\n    )\n    no_green_no_red_C_red_L_red_R = reduce(\n        np.logical_and,\n        (\n            green_nans,\n            red_C_nans,\n            ~red_L_nans,\n            ~red_R_nans,\n            ~l_r_is_too_separated,\n        ),\n    )\n    if np.sum(no_green_no_red_C_red_L_red_R) &gt; 0:\n        centroid[no_green_no_red_C_red_L_red_R] = [\n            *zip(\n                (\n                    pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_L, \"x\"]\n                    ]\n                    + pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_R, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_L, \"y\"]\n                    ]\n                    + pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_R, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If red center LED is NaN, but green, red left, and right LEDs are not\n    dist_between_left_green = get_distance(\n        pos_df.loc[:, idx[red_led_L, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].to_numpy(),\n    )\n    dist_between_right_green = get_distance(\n        pos_df.loc[:, idx[red_led_R, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].to_numpy(),\n    )\n    l_g_is_too_separated = (\n        dist_between_left_green &gt;= params[\"max_LED_separation\"]\n    )\n    r_g_is_too_separated = (\n        dist_between_right_green &gt;= params[\"max_LED_separation\"]\n    )\n    green_red_L_red_R_no_red_C = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            red_C_nans,\n            ~red_L_nans,\n            ~red_R_nans,\n            ~l_r_is_too_separated,\n            ~l_g_is_too_separated,\n            ~r_g_is_too_separated,\n        ),\n    )\n    if np.sum(green_red_L_red_R_no_red_C) &gt; 0:\n        midpoint = (\n            (\n                pos_df.loc[idx[green_red_L_red_R_no_red_C], idx[red_led_L, \"x\"]]\n                + pos_df.loc[\n                    idx[green_red_L_red_R_no_red_C], idx[red_led_R, \"x\"]\n                ]\n            )\n            / 2,\n            (\n                pos_df.loc[idx[green_red_L_red_R_no_red_C], idx[red_led_L, \"y\"]]\n                + pos_df.loc[\n                    idx[green_red_L_red_R_no_red_C], idx[red_led_R, \"y\"]\n                ]\n            )\n            / 2,\n        )\n        centroid[green_red_L_red_R_no_red_C] = [\n            *zip(\n                (\n                    midpoint[0]\n                    + pos_df.loc[\n                        idx[green_red_L_red_R_no_red_C], idx[green_led, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    midpoint[1]\n                    + pos_df.loc[\n                        idx[green_red_L_red_R_no_red_C], idx[green_led, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If red center and left LED is NaN, but green and red right LED are not\n    green_red_R_no_red_C_no_red_L = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            red_C_nans,\n            red_L_nans,\n            ~red_R_nans,\n            ~r_g_is_too_separated,\n        ),\n    )\n    if np.sum(green_red_R_no_red_C_no_red_L) &gt; 0:\n        centroid[green_red_R_no_red_C_no_red_L] = [\n            *zip(\n                (\n                    pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[red_led_R, \"x\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[green_led, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[red_led_R, \"y\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[green_led, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If red center and right LED is NaN, but green and red left LED are not\n    green_red_L_no_red_C_no_red_R = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            red_C_nans,\n            ~red_L_nans,\n            red_R_nans,\n            ~l_g_is_too_separated,\n        ),\n    )\n    if np.sum(green_red_L_no_red_C_no_red_R) &gt; 0:\n        centroid[green_red_L_no_red_C_no_red_R] = [\n            *zip(\n                (\n                    pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[red_led_L, \"x\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[green_led, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[red_led_L, \"y\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[green_led, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If all LEDS are NaN except red left LED\n    red_L_no_green_no_red_C_no_red_R = reduce(\n        np.logical_and, (green_nans, red_C_nans, ~red_L_nans, red_R_nans)\n    )\n    if np.sum(red_L_no_green_no_red_C_no_red_R) &gt; 0:\n        centroid[red_L_no_green_no_red_C_no_red_R] = [\n            *zip(\n                pos_df.loc[\n                    idx[red_L_no_green_no_red_C_no_red_R], idx[red_led_L, \"x\"]\n                ],\n                pos_df.loc[\n                    idx[red_L_no_green_no_red_C_no_red_R], idx[red_led_L, \"y\"]\n                ],\n            )\n        ]\n    # If all LEDS are NaN except red right LED\n    red_R_no_green_no_red_C_no_red_L = reduce(\n        np.logical_and, (green_nans, red_C_nans, red_L_nans, ~red_R_nans)\n    )\n    if np.sum(red_R_no_green_no_red_C_no_red_L) &gt; 0:\n        centroid[red_R_no_green_no_red_C_no_red_L] = [\n            *zip(\n                pos_df.loc[\n                    idx[red_R_no_green_no_red_C_no_red_L], idx[red_led_R, \"x\"]\n                ],\n                pos_df.loc[\n                    idx[red_R_no_green_no_red_C_no_red_L], idx[red_led_R, \"y\"]\n                ],\n            )\n        ]\n    # If all red LEDs are NaN, but green LED is not\n    green_no_red = reduce(\n        np.logical_and, (~green_nans, red_C_nans, red_L_nans, red_R_nans)\n    )\n    if np.sum(green_no_red) &gt; 0:\n        centroid[green_no_red] = [\n            *zip(\n                pos_df.loc[idx[green_no_red], idx[green_led, \"x\"]],\n                pos_df.loc[idx[green_no_red], idx[green_led, \"y\"]],\n            )\n        ]\n    too_separated_inds = reduce(\n        np.logical_or,\n        (\n            g_c_is_too_separated,\n            l_r_is_too_separated,\n            l_g_is_too_separated,\n            r_g_is_too_separated,\n        ),\n    )\n    if np.sum(too_separated_inds) &gt; 0:\n        centroid[too_separated_inds, :] = np.nan\n    return centroid\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.two_pt_centroid", "title": "<code>two_pt_centroid(pos_df, **params)</code>", "text": "<p>Determines the centroid of 2 points using (point1 + point2) / 2 For a given timestamp, if one point is NaN, then the other point is assigned as the centroid. If both are NaN, the centroid is NaN</p> <p>Parameters:</p> Name Type Description Default <code>pos_df</code> <code>DataFrame</code> <p>dataframe containing x and y position for each point of interest, index is timestamps. Column names specified by params</p> required <code>**params</code> <code>dict</code> <p>contains 'point1' and 'point2' keys, whose values specify the column names in <code>pos_df</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>centroid</code> <code>ndarray</code> <p>numpy array with shape (n_time, 2) centroid[0] is the x coord and centroid[1] is the y coord</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def two_pt_centroid(pos_df: pd.DataFrame, **params):\n    \"\"\"\n    Determines the centroid of 2 points using (point1 + point2) / 2\n    For a given timestamp, if one point is NaN,\n    then the other point is assigned as the centroid.\n    If both are NaN, the centroid is NaN\n\n    Parameters\n    ----------\n    pos_df : pd.DataFrame\n        dataframe containing x and y position for each point of interest,\n        index is timestamps. Column names specified by params\n    **params : dict\n        contains 'point1' and 'point2' keys,\n        whose values specify the column names in `pos_df`\n\n    Returns\n    -------\n    centroid : np.ndarray\n        numpy array with shape (n_time, 2)\n        centroid[0] is the x coord and centroid[1] is the y coord\n    \"\"\"\n\n    idx = pd.IndexSlice\n    centroid = np.zeros(shape=(len(pos_df), 2))\n    PT1 = params[\"points\"].pop(\"point1\", None)\n    PT2 = params[\"points\"].pop(\"point2\", None)\n    pt1_nans = pos_df.loc[:, idx[PT1, (\"x\", \"y\")]].isna().any(axis=1)\n    pt2_nans = pos_df.loc[:, idx[PT2, (\"x\", \"y\")]].isna().any(axis=1)\n    dist_between_points = get_distance(\n        pos_df.loc[:, idx[PT1, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[PT2, (\"x\", \"y\")]].to_numpy(),\n    )\n    is_too_separated = dist_between_points &gt;= params[\"max_LED_separation\"]\n    all_good_mask = np.logical_and(~pt1_nans, ~pt2_nans, ~is_too_separated)\n    centroid[all_good_mask] = [\n        *zip(\n            (\n                pos_df.loc[idx[all_good_mask], idx[PT1, \"x\"]]\n                + pos_df.loc[idx[all_good_mask], idx[PT2, \"x\"]]\n            )\n            / 2,\n            (\n                pos_df.loc[idx[all_good_mask], idx[PT1, \"y\"]]\n                + pos_df.loc[idx[all_good_mask], idx[PT2, \"y\"]]\n            )\n            / 2,\n        )\n    ]\n    # If only point1 is good\n    pt1_mask = np.logical_and(~pt1_nans, pt2_nans)\n    if np.sum(pt1_mask) &gt; 0:\n        centroid[pt1_mask] = [\n            *zip(\n                pos_df.loc[idx[pt1_mask], idx[PT1, \"x\"]],\n                pos_df.loc[idx[pt1_mask], idx[PT1, \"y\"]],\n            )\n        ]\n    # If only point2 is good\n    pt2_mask = np.logical_and(pt1_nans, ~pt2_nans)\n    if np.sum(pt2_mask) &gt; 0:\n        centroid[pt2_mask] = [\n            *zip(\n                pos_df.loc[idx[pt2_mask], idx[PT2, \"x\"]],\n                pos_df.loc[idx[pt2_mask], idx[PT2, \"y\"]],\n            )\n        ]\n    # If neither point is not NaN\n    all_bad_mask = np.logical_and(pt1_nans, pt2_nans)\n    centroid[all_bad_mask, :] = np.nan\n    # If LEDs are too far apart\n    centroid[is_too_separated, :] = np.nan\n\n    return centroid\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.one_pt_centroid", "title": "<code>one_pt_centroid(pos_df, **params)</code>", "text": "<p>Passes through the provided point as the centroid For a given timestamp, if the point is NaN, then the centroid is NaN</p> <p>Parameters:</p> Name Type Description Default <code>pos_df</code> <code>DataFrame</code> <p>dataframe containing x and y position for the point of interest, index is timestamps. Column name specified by params</p> required <code>**params</code> <code>dict</code> <p>contains a 'point1' key, whose value specifies the column name in <code>pos_df</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>centroid</code> <code>ndarray</code> <p>numpy array with shape (n_time, 2) centroid[0] is the x coord and centroid[1] is the y coord</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def one_pt_centroid(pos_df: pd.DataFrame, **params):\n    \"\"\"\n    Passes through the provided point as the centroid\n    For a given timestamp, if the point is NaN,\n    then the centroid is NaN\n\n    Parameters\n    ----------\n    pos_df : pd.DataFrame\n        dataframe containing x and y position for the point of interest,\n        index is timestamps. Column name specified by params\n    **params : dict\n        contains a 'point1' key,\n        whose value specifies the column name in `pos_df`\n\n    Returns\n    -------\n    centroid : np.ndarray\n        numpy array with shape (n_time, 2)\n        centroid[0] is the x coord and centroid[1] is the y coord\n    \"\"\"\n    idx = pd.IndexSlice\n    PT1 = params[\"points\"].pop(\"point1\", None)\n    centroid = pos_df.loc[:, idx[PT1, (\"x\", \"y\")]].to_numpy()\n    return centroid\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/", "title": "position_dlc_cohort.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.DLCSmoothInterpCohortSelection", "title": "<code>DLCSmoothInterpCohortSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Table to specify which combination of bodyparts from DLCSmoothInterp get combined into a cohort</p> Source code in <code>src/spyglass/position/v1/position_dlc_cohort.py</code> <pre><code>@schema\nclass DLCSmoothInterpCohortSelection(dj.Manual):\n    \"\"\"\n    Table to specify which combination of bodyparts from DLCSmoothInterp\n    get combined into a cohort\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_si_cohort_selection_name : varchar(120)\n    -&gt; DLCPoseEstimation\n    ---\n    bodyparts_params_dict   : blob      # Dict with bodypart as key and desired dlc_si_params_name as value\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.DLCSmoothInterpCohort", "title": "<code>DLCSmoothInterpCohort</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Table to combine multiple bodyparts from DLCSmoothInterp to enable centroid/orientation calculations</p> Source code in <code>src/spyglass/position/v1/position_dlc_cohort.py</code> <pre><code>@schema\nclass DLCSmoothInterpCohort(dj.Computed):\n    \"\"\"\n    Table to combine multiple bodyparts from DLCSmoothInterp\n    to enable centroid/orientation calculations\n    \"\"\"\n\n    # Need to ensure that nwb_file_name/epoch/interval list name endure as primary keys\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohortSelection\n    ---\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCSmoothInterpCohort\n        -&gt; DLCSmoothInterp\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_smooth_interp_position_object_id : varchar(80)\n        dlc_smooth_interp_info_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_smooth_interp_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_info\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Bodypart Cohort\")\n            # from Jen Guidera\n            self.insert1(key)\n            cohort_selection = (DLCSmoothInterpCohortSelection &amp; key).fetch1()\n            table_entries = []\n            bodyparts_params_dict = cohort_selection.pop(\n                \"bodyparts_params_dict\"\n            )\n            temp_key = cohort_selection.copy()\n            for bodypart, params in bodyparts_params_dict.items():\n                temp_key[\"bodypart\"] = bodypart\n                temp_key[\"dlc_si_params_name\"] = params\n                table_entries.append((DLCSmoothInterp &amp; temp_key).fetch())\n            assert len(table_entries) == len(\n                bodyparts_params_dict\n            ), \"more entries found in DLCSmoothInterp than specified in bodyparts_params_dict\"\n            table_column_names = list(table_entries[0].dtype.fields.keys())\n            for table_entry in table_entries:\n                entry_key = {\n                    **{\n                        k: v for k, v in zip(table_column_names, table_entry[0])\n                    },\n                    **key,\n                }\n                DLCSmoothInterpCohort.BodyPart.insert1(\n                    entry_key, skip_duplicates=True\n                )\n        logger.logger.info(\"Inserted entry into DLCSmoothInterpCohort\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/", "title": "position_dlc_model.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelInput", "title": "<code>DLCModelInput</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Table to hold model path if model is being input from local disk instead of Spyglass</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>@schema\nclass DLCModelInput(dj.Manual):\n    \"\"\"Table to hold model path if model is being input\n    from local disk instead of Spyglass\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_model_name : varchar(64)  # Different than dlc_model_name in DLCModelSource... not great\n    -&gt; DLCProject\n    ---\n    project_path         : varchar(255) # Path to project directory\n    \"\"\"\n\n    def insert1(self, key, **kwargs):\n        # expects key from DLCProject with config_path\n        project_path = Path(key[\"config_path\"]).parent\n        assert project_path.exists(), \"project path does not exist\"\n        key[\"dlc_model_name\"] = f'{project_path.name.split(\"model\")[0]}model'\n        key[\"project_path\"] = project_path.as_posix()\n        del key[\"config_path\"]\n        super().insert1(key, **kwargs)\n        DLCModelSource.insert_entry(\n            dlc_model_name=key[\"dlc_model_name\"],\n            project_name=key[\"project_name\"],\n            source=\"FromImport\",\n            key=key,\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelSource", "title": "<code>DLCModelSource</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Table to determine whether model originates from upstream DLCModelTraining table, or from local directory</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>@schema\nclass DLCModelSource(dj.Manual):\n    \"\"\"Table to determine whether model originates from\n    upstream DLCModelTraining table, or from local directory\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCProject\n    dlc_model_name : varchar(64)    # User-friendly model name\n    ---\n    source         : enum ('FromUpstream', 'FromImport')\n    \"\"\"\n\n    class FromImport(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCModelSource\n        -&gt; DLCModelInput\n        ---\n        project_path : varchar(255)\n        \"\"\"\n\n    class FromUpstream(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCModelSource\n        -&gt; DLCModelTraining\n        ---\n        project_path : varchar(255)\n        \"\"\"\n\n    @classmethod\n    @accepts(None, None, (\"FromUpstream\", \"FromImport\"), None)\n    def insert_entry(\n        cls,\n        dlc_model_name: str,\n        project_name: str,\n        source: str = \"FromUpstream\",\n        key: dict = None,\n        **kwargs,\n    ):\n        cls.insert1(\n            {\n                \"dlc_model_name\": dlc_model_name,\n                \"project_name\": project_name,\n                \"source\": source,\n            },\n            **kwargs,\n        )\n        part_table = getattr(cls, source)\n        table_query = dj.FreeTable(\n            dj.conn(), full_table_name=part_table.parents()[-1]\n        ) &amp; {\"project_name\": project_name}\n        project_path = table_query.fetch1(\"project_path\")\n        part_table.insert1(\n            {\n                \"dlc_model_name\": dlc_model_name,\n                \"project_name\": project_name,\n                \"project_path\": project_path,\n                **key,\n            },\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelEvaluation", "title": "<code>DLCModelEvaluation</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>@schema\nclass DLCModelEvaluation(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCModel\n    ---\n    train_iterations   : int   # Training iterations\n    train_error=null   : float # Train error (px)\n    test_error=null    : float # Test error (px)\n    p_cutoff=null      : float # p-cutoff used\n    train_error_p=null : float # Train error with p-cutoff\n    test_error_p=null  : float # Test error with p-cutoff\n    \"\"\"\n\n    def make(self, key):\n        \"\"\".populate() method will launch evaluation for each unique entry in Model.\"\"\"\n        import csv\n\n        from deeplabcut import evaluate_network\n        from deeplabcut.utils.auxiliaryfunctions import get_evaluation_folder\n\n        dlc_config, project_path, model_prefix, shuffle, trainingsetindex = (\n            DLCModel &amp; key\n        ).fetch1(\n            \"config_template\",\n            \"project_path\",\n            \"model_prefix\",\n            \"shuffle\",\n            \"trainingsetindex\",\n        )\n\n        yml_path, _ = dlc_reader.read_yaml(project_path)\n\n        evaluate_network(\n            yml_path,\n            Shuffles=[shuffle],  # this needs to be a list\n            trainingsetindex=trainingsetindex,\n            comparisonbodyparts=\"all\",\n        )\n\n        eval_folder = get_evaluation_folder(\n            trainFraction=dlc_config[\"TrainingFraction\"][trainingsetindex],\n            shuffle=shuffle,\n            cfg=dlc_config,\n            modelprefix=model_prefix,\n        )\n        eval_path = project_path / eval_folder\n        assert (\n            eval_path.exists()\n        ), f\"Couldn't find evaluation folder:\\n{eval_path}\"\n\n        eval_csvs = list(eval_path.glob(\"*csv\"))\n        max_modified_time = 0\n        for eval_csv in eval_csvs:\n            modified_time = os.path.getmtime(eval_csv)\n            if modified_time &gt; max_modified_time:\n                eval_csv_latest = eval_csv\n        with open(eval_csv_latest, newline=\"\") as f:\n            results = list(csv.DictReader(f, delimiter=\",\"))[0]\n        # in testing, test_error_p returned empty string\n        self.insert1(\n            dict(\n                key,\n                train_iterations=results[\"Training iterations:\"],\n                train_error=results[\" Train error(px)\"],\n                test_error=results[\" Test error(px)\"],\n                p_cutoff=results[\"p-cutoff used\"],\n                train_error_p=results[\"Train error with p-cutoff\"],\n                test_error_p=results[\"Test error with p-cutoff\"],\n            )\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelEvaluation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch evaluation for each unique entry in Model.</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>def make(self, key):\n    \"\"\".populate() method will launch evaluation for each unique entry in Model.\"\"\"\n    import csv\n\n    from deeplabcut import evaluate_network\n    from deeplabcut.utils.auxiliaryfunctions import get_evaluation_folder\n\n    dlc_config, project_path, model_prefix, shuffle, trainingsetindex = (\n        DLCModel &amp; key\n    ).fetch1(\n        \"config_template\",\n        \"project_path\",\n        \"model_prefix\",\n        \"shuffle\",\n        \"trainingsetindex\",\n    )\n\n    yml_path, _ = dlc_reader.read_yaml(project_path)\n\n    evaluate_network(\n        yml_path,\n        Shuffles=[shuffle],  # this needs to be a list\n        trainingsetindex=trainingsetindex,\n        comparisonbodyparts=\"all\",\n    )\n\n    eval_folder = get_evaluation_folder(\n        trainFraction=dlc_config[\"TrainingFraction\"][trainingsetindex],\n        shuffle=shuffle,\n        cfg=dlc_config,\n        modelprefix=model_prefix,\n    )\n    eval_path = project_path / eval_folder\n    assert (\n        eval_path.exists()\n    ), f\"Couldn't find evaluation folder:\\n{eval_path}\"\n\n    eval_csvs = list(eval_path.glob(\"*csv\"))\n    max_modified_time = 0\n    for eval_csv in eval_csvs:\n        modified_time = os.path.getmtime(eval_csv)\n        if modified_time &gt; max_modified_time:\n            eval_csv_latest = eval_csv\n    with open(eval_csv_latest, newline=\"\") as f:\n        results = list(csv.DictReader(f, delimiter=\",\"))[0]\n    # in testing, test_error_p returned empty string\n    self.insert1(\n        dict(\n            key,\n            train_iterations=results[\"Training iterations:\"],\n            train_error=results[\" Train error(px)\"],\n            test_error=results[\" Test error(px)\"],\n            p_cutoff=results[\"p-cutoff used\"],\n            train_error_p=results[\"Train error with p-cutoff\"],\n            test_error_p=results[\"Test error with p-cutoff\"],\n        )\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.str_to_bool", "title": "<code>str_to_bool(value)</code>", "text": "<p>Return whether the provided string represents true. Otherwise false.</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>def str_to_bool(value) -&gt; bool:\n    \"\"\"Return whether the provided string represents true. Otherwise false.\"\"\"\n    # Due to distutils equivalent depreciation in 3.10\n    # Adopted from github.com/PostHog/posthog/blob/master/posthog/utils.py\n    if not value:\n        return False\n    return str(value).lower() in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/", "title": "position_dlc_orient.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.DLCOrientationParams", "title": "<code>DLCOrientationParams</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Parameters for determining and smoothing the orientation of a set of BodyParts</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>@schema\nclass DLCOrientationParams(dj.Manual):\n    \"\"\"\n    Parameters for determining and smoothing the orientation of a set of BodyParts\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_orientation_params_name: varchar(80) # name for this set of parameters\n    ---\n    params: longblob\n    \"\"\"\n\n    @classmethod\n    def insert_params(cls, params_name: str, params: dict, **kwargs):\n        cls.insert1(\n            {\"dlc_orientation_params_name\": params_name, \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        params = {\n            \"orient_method\": \"red_green_orientation\",\n            \"bodypart1\": \"greenLED\",\n            \"bodypart2\": \"redLED_C\",\n            \"orientation_smoothing_std_dev\": 0.001,\n        }\n        cls.insert1(\n            {\"dlc_orientation_params_name\": \"default\", \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_orientation_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (\n                cls &amp; {\"dlc_orientation_params_name\": \"default\"}\n            ).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.DLCOrientationSelection", "title": "<code>DLCOrientationSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>@schema\nclass DLCOrientationSelection(dj.Manual):\n    \"\"\" \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohort\n    -&gt; DLCOrientationParams\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.DLCOrientation", "title": "<code>DLCOrientation</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Determines and smooths orientation of a set of bodyparts given a specified method</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>@schema\nclass DLCOrientation(dj.Computed):\n    \"\"\"\n    Determines and smooths orientation of a set of bodyparts given a specified method\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCOrientationSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_orientation_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        # Get labels to smooth from Parameters table\n        cohort_entries = DLCSmoothInterpCohort.BodyPart &amp; key\n        pos_df = pd.concat(\n            {\n                bodypart: (\n                    DLCSmoothInterpCohort.BodyPart\n                    &amp; {**key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in cohort_entries.fetch(\"bodypart\")\n            },\n            axis=1,\n        )\n        params = (DLCOrientationParams() &amp; key).fetch1(\"params\")\n        orientation_smoothing_std_dev = params.pop(\n            \"orientation_smoothing_std_dev\", None\n        )\n        dt = np.median(np.diff(pos_df.index.to_numpy()))\n        sampling_rate = 1 / dt\n        orient_func = _key_to_func_dict[params[\"orient_method\"]]\n        orientation = orient_func(pos_df, **params)\n        if not params[\"orient_method\"] == \"none\":\n            # Smooth orientation\n            is_nan = np.isnan(orientation)\n            unwrap_orientation = orientation.copy()\n            # Only unwrap non nan values, while keeping nans in dataset for interpolation\n            unwrap_orientation[~is_nan] = np.unwrap(orientation[~is_nan])\n            unwrap_df = pd.DataFrame(\n                unwrap_orientation, columns=[\"orientation\"], index=pos_df.index\n            )\n            nan_spans = get_span_start_stop(np.where(is_nan)[0])\n            orient_df = interp_orientation(\n                unwrap_df,\n                nan_spans,\n            )\n            orientation = gaussian_smooth(\n                orient_df[\"orientation\"].to_numpy(),\n                orientation_smoothing_std_dev,\n                sampling_rate,\n                axis=0,\n                truncate=8,\n            )\n            # convert back to between -pi and pi\n            orientation = np.angle(np.exp(1j * orientation))\n        final_df = pd.DataFrame(\n            orientation, columns=[\"orientation\"], index=pos_df.index\n        )\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        spatial_series = (RawPosition() &amp; key).fetch_nwb()[0][\"raw_position\"]\n        orientation = pynwb.behavior.CompassDirection()\n        orientation.create_spatial_series(\n            name=\"orientation\",\n            timestamps=final_df.index.to_numpy(),\n            conversion=1.0,\n            data=final_df[\"orientation\"].to_numpy(),\n            reference_frame=spatial_series.reference_frame,\n            comments=spatial_series.comments,\n            description=\"orientation\",\n        )\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"dlc_orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_orientation\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"orientation\",\n        ]\n        return pd.DataFrame(\n            np.asarray(nwb_data[\"dlc_orientation\"].get_spatial_series().data)[\n                :, np.newaxis\n            ],\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.two_pt_head_orientation", "title": "<code>two_pt_head_orientation(pos_df, **params)</code>", "text": "<p>Determines orientation based on vector between two points</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>def two_pt_head_orientation(pos_df: pd.DataFrame, **params):\n    \"\"\"Determines orientation based on vector between two points\"\"\"\n    BP1 = params.pop(\"bodypart1\", None)\n    BP2 = params.pop(\"bodypart2\", None)\n    orientation = np.arctan2(\n        (pos_df[BP1][\"y\"] - pos_df[BP2][\"y\"]),\n        (pos_df[BP1][\"x\"] - pos_df[BP2][\"x\"]),\n    )\n    return orientation\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.red_led_bisector_orientation", "title": "<code>red_led_bisector_orientation(pos_df, **params)</code>", "text": "<p>Determines orientation based on 2 equally-spaced identifiers that are assumed to be perpendicular to the orientation direction. A third object is needed to determine forward/backward</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>def red_led_bisector_orientation(pos_df: pd.DataFrame, **params):\n    \"\"\"Determines orientation based on 2 equally-spaced identifiers\n    that are assumed to be perpendicular to the orientation direction.\n    A third object is needed to determine forward/backward\n    \"\"\"\n    LED1 = params.pop(\"led1\", None)\n    LED2 = params.pop(\"led2\", None)\n    LED3 = params.pop(\"led3\", None)\n    orientation = []\n    for index, row in pos_df.iterrows():\n        x_vec = row[LED1][\"x\"] - row[LED2][\"x\"]\n        y_vec = row[LED1][\"y\"] - row[LED2][\"y\"]\n        if y_vec == 0:\n            if (row[LED3][\"y\"] &gt; row[LED1][\"y\"]) &amp; (\n                row[LED3][\"y\"] &gt; row[LED2][\"y\"]\n            ):\n                orientation.append(np.pi / 2)\n            elif (row[LED3][\"y\"] &lt; row[LED1][\"y\"]) &amp; (\n                row[LED3][\"y\"] &lt; row[LED2][\"y\"]\n            ):\n                orientation.append(-(np.pi / 2))\n            else:\n                raise Exception(\"Cannot determine head direction from bisector\")\n        else:\n            length = np.sqrt(y_vec * y_vec + x_vec * x_vec)\n            norm = np.array([-y_vec / length, x_vec / length])\n            orientation.append(np.arctan2(norm[1], norm[0]))\n        if index + 1 == len(pos_df):\n            break\n    return np.array(orientation)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/", "title": "position_dlc_pose_estimation.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection", "title": "<code>DLCPoseEstimationSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimationSelection(dj.Manual):\n    definition = \"\"\"\n    -&gt; VideoFile                           # Session -&gt; Recording + File part table\n    -&gt; DLCModel                                    # Must specify a DLC project_path\n    ---\n    task_mode='load' : enum('load', 'trigger')  # load results or trigger computation\n    video_path : varchar(120)                   # path to video file\n    pose_estimation_output_dir='': varchar(255) # output dir relative to the root dir\n    pose_estimation_params=null  : longblob     # analyze_videos params, if not default\n    \"\"\"\n\n    @classmethod\n    def get_video_crop(cls, video_path):\n        \"\"\"\n        Queries the user to determine the cropping parameters for a given video\n\n        Parameters\n        ----------\n        video_path : str\n            path to the video file\n\n        Returns\n        -------\n        crop_ints : list\n            list of 4 integers [x min, x max, y min, y max]\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        _, frame = cap.read()\n        fig, ax = plt.subplots(figsize=(20, 10))\n        ax.imshow(frame)\n        xlims = ax.get_xlim()\n        ylims = ax.get_ylim()\n        ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n        ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n        ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n        display(fig)\n        crop_input = input(\n            \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n        )\n        plt.close()\n        if crop_input.lower() == \"none\":\n            return None\n        crop_ints = [int(val) for val in crop_input.split(\",\")]\n        assert all(isinstance(val, int) for val in crop_ints)\n        return crop_ints\n\n    @classmethod\n    def insert_estimation_task(\n        cls,\n        key,\n        task_mode=\"trigger\",\n        params: dict = None,\n        check_crop=None,\n        skip_duplicates=True,\n    ):\n        \"\"\"\n        Insert PoseEstimationTask in inferred output dir.\n        From Datajoint Elements\n\n        Parameters\n        ----------\n        key: DataJoint key specifying a pairing of VideoRecording and Model.\n        task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n        params (dict): Optional. Parameters passed to DLC's analyze_videos:\n            videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n            dynamic, robust_nframes, allow_growth, use_shelve\n        \"\"\"\n        from .dlc_utils import check_videofile, get_video_path\n\n        video_path, video_filename, _, _ = get_video_path(key)\n        output_dir = infer_output_dir(key)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"Pose Estimation Selection\")\n            video_dir = os.path.dirname(video_path) + \"/\"\n            logger.logger.info(\"video_dir: %s\", video_dir)\n            video_path = check_videofile(\n                video_path=video_dir, video_filename=video_filename\n            )[0]\n            if check_crop is not None:\n                params[\"cropping\"] = cls.get_video_crop(\n                    video_path=video_path.as_posix()\n                )\n            cls.insert1(\n                {\n                    **key,\n                    \"task_mode\": task_mode,\n                    \"pose_estimation_params\": params,\n                    \"video_path\": video_path,\n                    \"pose_estimation_output_dir\": output_dir,\n                },\n                skip_duplicates=skip_duplicates,\n            )\n        logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n        return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.get_video_crop", "title": "<code>get_video_crop(video_path)</code>  <code>classmethod</code>", "text": "<p>Queries the user to determine the cropping parameters for a given video</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>path to the video file</p> required <p>Returns:</p> Name Type Description <code>crop_ints</code> <code>list</code> <p>list of 4 integers [x min, x max, y min, y max]</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef get_video_crop(cls, video_path):\n    \"\"\"\n    Queries the user to determine the cropping parameters for a given video\n\n    Parameters\n    ----------\n    video_path : str\n        path to the video file\n\n    Returns\n    -------\n    crop_ints : list\n        list of 4 integers [x min, x max, y min, y max]\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    _, frame = cap.read()\n    fig, ax = plt.subplots(figsize=(20, 10))\n    ax.imshow(frame)\n    xlims = ax.get_xlim()\n    ylims = ax.get_ylim()\n    ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n    ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n    ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n    display(fig)\n    crop_input = input(\n        \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n    )\n    plt.close()\n    if crop_input.lower() == \"none\":\n        return None\n    crop_ints = [int(val) for val in crop_input.split(\",\")]\n    assert all(isinstance(val, int) for val in crop_ints)\n    return crop_ints\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.insert_estimation_task", "title": "<code>insert_estimation_task(key, task_mode='trigger', params=None, check_crop=None, skip_duplicates=True)</code>  <code>classmethod</code>", "text": "<p>Insert PoseEstimationTask in inferred output dir. From Datajoint Elements</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required <code>task_mode</code> <code>'trigger'</code> <code>params</code> <code>dict</code> <p>videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef insert_estimation_task(\n    cls,\n    key,\n    task_mode=\"trigger\",\n    params: dict = None,\n    check_crop=None,\n    skip_duplicates=True,\n):\n    \"\"\"\n    Insert PoseEstimationTask in inferred output dir.\n    From Datajoint Elements\n\n    Parameters\n    ----------\n    key: DataJoint key specifying a pairing of VideoRecording and Model.\n    task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n    params (dict): Optional. Parameters passed to DLC's analyze_videos:\n        videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n        dynamic, robust_nframes, allow_growth, use_shelve\n    \"\"\"\n    from .dlc_utils import check_videofile, get_video_path\n\n    video_path, video_filename, _, _ = get_video_path(key)\n    output_dir = infer_output_dir(key)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"Pose Estimation Selection\")\n        video_dir = os.path.dirname(video_path) + \"/\"\n        logger.logger.info(\"video_dir: %s\", video_dir)\n        video_path = check_videofile(\n            video_path=video_dir, video_filename=video_filename\n        )[0]\n        if check_crop is not None:\n            params[\"cropping\"] = cls.get_video_crop(\n                video_path=video_path.as_posix()\n            )\n        cls.insert1(\n            {\n                **key,\n                \"task_mode\": task_mode,\n                \"pose_estimation_params\": params,\n                \"video_path\": video_path,\n                \"pose_estimation_output_dir\": output_dir,\n            },\n            skip_duplicates=skip_duplicates,\n        )\n    logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n    return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimation", "title": "<code>DLCPoseEstimation</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimation(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCPoseEstimationSelection\n    ---\n    pose_estimation_time: datetime  # time of generation of this set of DLC results\n    meters_per_pixel : double       # conversion of meters per pixel for analyzed video\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\" # uses DeepLabCut h5 output for body part position\n        -&gt; DLCPoseEstimation\n        -&gt; DLCModel.BodyPart\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_pose_estimation_position_object_id : varchar(80)\n        dlc_pose_estimation_likelihood_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_pose_estimation_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n                \"likelihood\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"likelihood\"]\n                            .data\n                        )[:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n        \"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n        from . import dlc_reader\n        from .dlc_utils import get_video_path\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"----------------------\")\n            logger.logger.info(\"Pose Estimation\")\n            # ID model and directories\n            dlc_model = (DLCModel &amp; key).fetch1()\n            bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n            task_mode, analyze_video_params, video_path, output_dir = (\n                DLCPoseEstimationSelection &amp; key\n            ).fetch1(\n                \"task_mode\",\n                \"pose_estimation_params\",\n                \"video_path\",\n                \"pose_estimation_output_dir\",\n            )\n            analyze_video_params = analyze_video_params or {}\n\n            project_path = dlc_model[\"project_path\"]\n\n            # Trigger PoseEstimation\n            if task_mode == \"trigger\":\n                dlc_reader.do_pose_estimation(\n                    video_path,\n                    dlc_model,\n                    project_path,\n                    output_dir,\n                    **analyze_video_params,\n                )\n            dlc_result = dlc_reader.PoseEstimation(output_dir)\n            creation_time = datetime.fromtimestamp(\n                dlc_result.creation_time\n            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            logger.logger.info(\"getting raw position\")\n            interval_list_name = (\n                convert_epoch_interval_name_to_position_interval_name(\n                    {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"epoch\": key[\"epoch\"],\n                    },\n                    populate_missing=False,\n                )\n            )\n            spatial_series = (\n                RawPosition()\n                &amp; {**key, \"interval_list_name\": interval_list_name}\n            ).fetch_nwb()[0][\"raw_position\"]\n            _, _, _, video_time = get_video_path(key)\n            pos_time = spatial_series.timestamps\n            # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n            # which also has timestamps\n            key[\"meters_per_pixel\"] = spatial_series.conversion\n\n            # Insert entry into DLCPoseEstimation\n            logger.logger.info(\n                \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n                key[\"nwb_file_name\"],\n                key[\"epoch\"],\n            )\n            self.insert1({**key, \"pose_estimation_time\": creation_time})\n            meters_per_pixel = key[\"meters_per_pixel\"]\n            del key[\"meters_per_pixel\"]\n            body_parts = dlc_result.df.columns.levels[0]\n            body_parts_df = {}\n            # Insert dlc pose estimation into analysis NWB file for each body part.\n            for body_part in bodyparts:\n                if body_part in body_parts:\n                    body_parts_df[body_part] = pd.DataFrame.from_dict(\n                        {\n                            c: dlc_result.df.get(body_part).get(c).values\n                            for c in dlc_result.df.get(body_part).columns\n                        }\n                    )\n            idx = pd.IndexSlice\n            for body_part, part_df in body_parts_df.items():\n                logger.logger.info(\"converting to cm\")\n                part_df = convert_to_cm(part_df, meters_per_pixel)\n                logger.logger.info(\"adding timestamps to DataFrame\")\n                part_df = add_timestamps(\n                    part_df, pos_time=pos_time, video_time=video_time\n                )\n                key[\"bodypart\"] = body_part\n                key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                    key[\"nwb_file_name\"]\n                )\n                position = pynwb.behavior.Position()\n                likelihood = pynwb.behavior.BehavioralTimeSeries()\n                position.create_spatial_series(\n                    name=\"position\",\n                    timestamps=part_df.time.to_numpy(),\n                    conversion=METERS_PER_CM,\n                    data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                    reference_frame=spatial_series.reference_frame,\n                    comments=spatial_series.comments,\n                    description=\"x_position, y_position\",\n                )\n                likelihood.create_timeseries(\n                    name=\"likelihood\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                    unit=\"likelihood\",\n                    comments=\"no comments\",\n                    description=\"likelihood\",\n                )\n                likelihood.create_timeseries(\n                    name=\"video_frame_ind\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                    unit=\"index\",\n                    comments=\"no comments\",\n                    description=\"video_frame_ind\",\n                )\n                nwb_analysis_file = AnalysisNwbfile()\n                key[\n                    \"dlc_pose_estimation_position_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=position,\n                )\n                key[\n                    \"dlc_pose_estimation_likelihood_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=likelihood,\n                )\n                nwb_analysis_file.add(\n                    nwb_file_name=key[\"nwb_file_name\"],\n                    analysis_file_name=key[\"analysis_file_name\"],\n                )\n                self.BodyPart.insert1(key)\n\n    def fetch_dataframe(self, *attrs, **kwargs):\n        entries = (self.BodyPart &amp; self).fetch(\"KEY\")\n        nwb_data_dict = {\n            entry[\"bodypart\"]: (self.BodyPart() &amp; entry).fetch_nwb()[0]\n            for entry in entries\n        }\n        index = pd.Index(\n            np.asarray(\n                nwb_data_dict[entries[0][\"bodypart\"]][\n                    \"dlc_pose_estimation_position\"\n                ]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n            \"likelihood\",\n        ]\n        return pd.concat(\n            {\n                entry[\"bodypart\"]: pd.DataFrame(\n                    np.concatenate(\n                        (\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"video_frame_ind\"]\n                                .data,\n                                dtype=int,\n                            )[:, np.newaxis],\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_position\"\n                                ]\n                                .get_spatial_series()\n                                .data\n                            ),\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"likelihood\"]\n                                .data\n                            )[:, np.newaxis],\n                        ),\n                        axis=1,\n                    ),\n                    columns=COLUMNS,\n                    index=index,\n                )\n                for entry in entries\n            },\n            axis=1,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch training for each PoseEstimationTask</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>def make(self, key):\n    \"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n    from . import dlc_reader\n    from .dlc_utils import get_video_path\n\n    METERS_PER_CM = 0.01\n\n    output_dir = infer_output_dir(key=key, makedir=False)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"----------------------\")\n        logger.logger.info(\"Pose Estimation\")\n        # ID model and directories\n        dlc_model = (DLCModel &amp; key).fetch1()\n        bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n        task_mode, analyze_video_params, video_path, output_dir = (\n            DLCPoseEstimationSelection &amp; key\n        ).fetch1(\n            \"task_mode\",\n            \"pose_estimation_params\",\n            \"video_path\",\n            \"pose_estimation_output_dir\",\n        )\n        analyze_video_params = analyze_video_params or {}\n\n        project_path = dlc_model[\"project_path\"]\n\n        # Trigger PoseEstimation\n        if task_mode == \"trigger\":\n            dlc_reader.do_pose_estimation(\n                video_path,\n                dlc_model,\n                project_path,\n                output_dir,\n                **analyze_video_params,\n            )\n        dlc_result = dlc_reader.PoseEstimation(output_dir)\n        creation_time = datetime.fromtimestamp(\n            dlc_result.creation_time\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        logger.logger.info(\"getting raw position\")\n        interval_list_name = (\n            convert_epoch_interval_name_to_position_interval_name(\n                {\n                    \"nwb_file_name\": key[\"nwb_file_name\"],\n                    \"epoch\": key[\"epoch\"],\n                },\n                populate_missing=False,\n            )\n        )\n        spatial_series = (\n            RawPosition()\n            &amp; {**key, \"interval_list_name\": interval_list_name}\n        ).fetch_nwb()[0][\"raw_position\"]\n        _, _, _, video_time = get_video_path(key)\n        pos_time = spatial_series.timestamps\n        # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n        # which also has timestamps\n        key[\"meters_per_pixel\"] = spatial_series.conversion\n\n        # Insert entry into DLCPoseEstimation\n        logger.logger.info(\n            \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n            key[\"nwb_file_name\"],\n            key[\"epoch\"],\n        )\n        self.insert1({**key, \"pose_estimation_time\": creation_time})\n        meters_per_pixel = key[\"meters_per_pixel\"]\n        del key[\"meters_per_pixel\"]\n        body_parts = dlc_result.df.columns.levels[0]\n        body_parts_df = {}\n        # Insert dlc pose estimation into analysis NWB file for each body part.\n        for body_part in bodyparts:\n            if body_part in body_parts:\n                body_parts_df[body_part] = pd.DataFrame.from_dict(\n                    {\n                        c: dlc_result.df.get(body_part).get(c).values\n                        for c in dlc_result.df.get(body_part).columns\n                    }\n                )\n        idx = pd.IndexSlice\n        for body_part, part_df in body_parts_df.items():\n            logger.logger.info(\"converting to cm\")\n            part_df = convert_to_cm(part_df, meters_per_pixel)\n            logger.logger.info(\"adding timestamps to DataFrame\")\n            part_df = add_timestamps(\n                part_df, pos_time=pos_time, video_time=video_time\n            )\n            key[\"bodypart\"] = body_part\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            position = pynwb.behavior.Position()\n            likelihood = pynwb.behavior.BehavioralTimeSeries()\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=part_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            likelihood.create_timeseries(\n                name=\"likelihood\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                unit=\"likelihood\",\n                comments=\"no comments\",\n                description=\"likelihood\",\n            )\n            likelihood.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\n                \"dlc_pose_estimation_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_pose_estimation_likelihood_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=likelihood,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.BodyPart.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.add_timestamps", "title": "<code>add_timestamps(df, pos_time, video_time)</code>", "text": "<p>Takes timestamps from raw_pos_df and adds to df, which is returned with timestamps and their matching video frame index</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>pose estimation dataframe to add timestamps</p> required <code>pos_time</code> <code>ndarray</code> <p>numpy array containing timestamps from the raw position object</p> required <code>video_time</code> <code>ndarray</code> <p>numpy array containing timestamps from the video file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>original df with timestamps and video_frame_ind as new columns</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>def add_timestamps(\n    df: pd.DataFrame, pos_time: np.ndarray, video_time: np.ndarray\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Takes timestamps from raw_pos_df and adds to df,\n    which is returned with timestamps and their matching video frame index\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        pose estimation dataframe to add timestamps\n    pos_time : np.ndarray\n        numpy array containing timestamps from the raw position object\n    video_time: np.ndarray\n        numpy array containing timestamps from the video file\n\n    Returns\n    -------\n    pd.DataFrame\n        original df with timestamps and video_frame_ind as new columns\n    \"\"\"\n    first_video_frame = np.searchsorted(video_time, pos_time[0])\n    video_frame_ind = np.arange(first_video_frame, len(video_time))\n    time_df = pd.DataFrame(\n        index=video_frame_ind,\n        data=video_time[first_video_frame:],\n        columns=[\"time\"],\n    )\n    df = df.join(time_df)\n    # Drop indices where time is NaN\n    df = df.dropna(subset=[\"time\"])\n    # Add video_frame_ind as column\n    df = df.rename_axis(\"video_frame_ind\").reset_index()\n    return df\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/", "title": "position_dlc_position.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.DLCSmoothInterpParams", "title": "<code>DLCSmoothInterpParams</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Parameters for extracting the smoothed head position.</p> <p>Attributes:</p> Name Type Description <code>interpolate</code> <code>bool, default True</code> <p>whether to interpolate over NaN spans</p> <code>smooth</code> <code>bool, default True</code> <p>whether to smooth the dataset</p> <code>smoothing_params</code> <code>dict</code> <p>smoothing_duration : float, default 0.05     number of frames to smooth over: sampling_rate*smoothing_duration = num_frames</p> <code>interp_params</code> <code>dict</code> <p>max_cm_to_interp : int, default 20     maximum distance between high likelihood points on either side of a NaN span     to interpolate over</p> <code>likelihood_thresh</code> <code>float, default 0.95</code> <p>likelihood below which to NaN and interpolate over</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>@schema\nclass DLCSmoothInterpParams(dj.Manual):\n    \"\"\"\n    Parameters for extracting the smoothed head position.\n\n    Attributes\n    ----------\n    interpolate : bool, default True\n        whether to interpolate over NaN spans\n    smooth : bool, default True\n        whether to smooth the dataset\n    smoothing_params : dict\n        smoothing_duration : float, default 0.05\n            number of frames to smooth over: sampling_rate*smoothing_duration = num_frames\n    interp_params : dict\n        max_cm_to_interp : int, default 20\n            maximum distance between high likelihood points on either side of a NaN span\n            to interpolate over\n    likelihood_thresh : float, default 0.95\n        likelihood below which to NaN and interpolate over\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_si_params_name : varchar(80) # name for this set of parameters\n    ---\n    params: longblob # dictionary of parameters\n    \"\"\"\n\n    @classmethod\n    def insert_params(cls, params_name: str, params: dict, **kwargs):\n        cls.insert1(\n            {\"dlc_si_params_name\": params_name, \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        default_params = {\n            \"smooth\": True,\n            \"smoothing_params\": {\n                \"smoothing_duration\": 0.05,\n                \"smooth_method\": \"moving_avg\",\n            },\n            \"interpolate\": True,\n            \"likelihood_thresh\": 0.95,\n            \"interp_params\": {\"max_cm_to_interp\": 15},\n            \"max_cm_between_pts\": 20,\n            # This is for use when finding \"good spans\" and is how many indices to bridge in between good spans\n            # see inds_to_span in get_good_spans\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"default\", \"params\": default_params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_nan_params(cls, **kwargs):\n        nan_params = {\n            \"smooth\": False,\n            \"interpolate\": False,\n            \"likelihood_thresh\": 0.95,\n            \"max_cm_between_pts\": 20,\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"just_nan\", \"params\": nan_params}, **kwargs\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (cls &amp; {\"dlc_si_params_name\": \"default\"}).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n\n    @classmethod\n    def get_nan_params(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"just_nan\"}\n        if not len(query) &gt; 0:\n            cls().insert_nan_params(skip_duplicates=True)\n            nan_params = (cls &amp; {\"dlc_si_params_name\": \"just_nan\"}).fetch1()\n        else:\n            nan_params = query.fetch1()\n        return nan_params\n\n    @staticmethod\n    def get_available_methods():\n        return _key_to_smooth_func_dict.keys()\n\n    def insert1(self, key, **kwargs):\n        if \"params\" in key:\n            if not \"max_cm_between_pts\" in key[\"params\"]:\n                raise KeyError(\"max_cm_between_pts is a required parameter\")\n            if \"smooth\" in key[\"params\"]:\n                if key[\"params\"][\"smooth\"]:\n                    if \"smoothing_params\" in key[\"params\"]:\n                        if \"smooth_method\" in key[\"params\"][\"smoothing_params\"]:\n                            smooth_method = key[\"params\"][\"smoothing_params\"][\n                                \"smooth_method\"\n                            ]\n                            if smooth_method not in _key_to_smooth_func_dict:\n                                raise KeyError(\n                                    f\"smooth_method: {smooth_method} not an available method.\"\n                                )\n                        if (\n                            not \"smoothing_duration\"\n                            in key[\"params\"][\"smoothing_params\"]\n                        ):\n                            raise KeyError(\n                                \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                            )\n                        else:\n                            assert isinstance(\n                                key[\"params\"][\"smoothing_params\"][\n                                    \"smoothing_duration\"\n                                ],\n                                (float, int),\n                            ), \"smoothing_duration must be a float or int\"\n                    else:\n                        raise ValueError(\n                            \"smoothing_params not in key['params']\"\n                        )\n            if \"likelihood_thresh\" in key[\"params\"]:\n                assert isinstance(\n                    key[\"params\"][\"likelihood_thresh\"],\n                    float,\n                ), \"likelihood_thresh must be a float\"\n                assert (\n                    0 &lt; key[\"params\"][\"likelihood_thresh\"] &lt; 1\n                ), \"likelihood_thresh must be between 0 and 1\"\n            else:\n                raise ValueError(\n                    \"likelihood_thresh must be passed within key['params']\"\n                )\n        else:\n            raise KeyError(\"'params' must be in key\")\n        super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.DLCSmoothInterp", "title": "<code>DLCSmoothInterp</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Interpolates across low likelihood periods and smooths the position Can take a few minutes.</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>@schema\nclass DLCSmoothInterp(dj.Computed):\n    \"\"\"\n    Interpolates across low likelihood periods and smooths the position\n    Can take a few minutes.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_smooth_interp_position_object_id : varchar(80)\n    dlc_smooth_interp_info_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            idx = pd.IndexSlice\n            # Get labels to smooth from Parameters table\n            params = (DLCSmoothInterpParams() &amp; key).fetch1(\"params\")\n            # Get DLC output dataframe\n            logger.logger.info(\"fetching Pose Estimation Dataframe\")\n            dlc_df = (DLCPoseEstimation.BodyPart() &amp; key).fetch1_dataframe()\n            dt = np.median(np.diff(dlc_df.index.to_numpy()))\n            sampling_rate = 1 / dt\n            logger.logger.info(\"Identifying indices to NaN\")\n            df_w_nans, bad_inds = nan_inds(\n                dlc_df.copy(),\n                params[\"max_cm_between_pts\"],\n                likelihood_thresh=params.pop(\"likelihood_thresh\"),\n                inds_to_span=params[\"num_inds_to_span\"],\n            )\n\n            nan_spans = get_span_start_stop(np.where(bad_inds)[0])\n            if params[\"interpolate\"]:\n                logger.logger.info(\"interpolating across low likelihood times\")\n                interp_df = interp_pos(\n                    df_w_nans.copy(), nan_spans, **params[\"interp_params\"]\n                )\n            else:\n                interp_df = df_w_nans.copy()\n                logger.logger.info(\"skipping interpolation\")\n            if params[\"smooth\"]:\n                if \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    smoothing_duration = params[\"smoothing_params\"].pop(\n                        \"smoothing_duration\"\n                    )\n                dt = np.median(np.diff(dlc_df.index.to_numpy()))\n                sampling_rate = 1 / dt\n                logger.logger.info(\"smoothing position\")\n                smooth_func = _key_to_smooth_func_dict[\n                    params[\"smoothing_params\"][\"smooth_method\"]\n                ]\n                logger.logger.info(\n                    \"Smoothing using method: %s\",\n                    str(params[\"smoothing_params\"][\"smooth_method\"]),\n                )\n                smooth_df = smooth_func(\n                    interp_df,\n                    smoothing_duration=smoothing_duration,\n                    sampling_rate=sampling_rate,\n                    **params[\"smoothing_params\"],\n                )\n            else:\n                smooth_df = interp_df.copy()\n                logger.logger.info(\"skipping smoothing\")\n            final_df = smooth_df.drop([\"likelihood\"], axis=1)\n            final_df = final_df.rename_axis(\"time\").reset_index()\n            position_nwb_data = (\n                (DLCPoseEstimation.BodyPart() &amp; key)\n                .fetch_nwb()[0][\"dlc_pose_estimation_position\"]\n                .get_spatial_series()\n            )\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            # Add dataframe to AnalysisNwbfile\n            nwb_analysis_file = AnalysisNwbfile()\n            position = pynwb.behavior.Position()\n            video_frame_ind = pynwb.behavior.BehavioralTimeSeries()\n            logger.logger.info(\"Creating NWB objects\")\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=final_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=position_nwb_data.reference_frame,\n                comments=position_nwb_data.comments,\n                description=\"x_position, y_position\",\n            )\n            video_frame_ind.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=final_df.time.to_numpy(),\n                data=final_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            key[\n                \"dlc_smooth_interp_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_smooth_interp_info_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=video_frame_ind,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.insert1(key)\n            logger.logger.info(\"inserted entry into DLCSmoothInterp\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_smooth_interp_position\"]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"dlc_smooth_interp_info\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"dlc_smooth_interp_position\"]\n                        .get_spatial_series()\n                        .data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.get_good_spans", "title": "<code>get_good_spans(bad_inds_mask, inds_to_span=50)</code>", "text": "<p>This function takes in a boolean mask of good and bad indices and determines spans of consecutive good indices. It combines two neighboring spans with a separation of less than inds_to_span and treats them as a single good span.</p> <p>Parameters:</p> Name Type Description Default <code>bad_inds_mask</code> <code>boolean mask</code> <p>A boolean mask where True is a bad index and False is a good index.</p> required <code>inds_to_span</code> <code>int</code> <p>This indicates how many indices between two good spans should be bridged to form a single good span. For instance if span A is (1500, 2350) and span B is (2370, 3700), then span A and span B would be combined into span A (1500, 3700) since one would want to identify potential jumps in the space in between the original A and B.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>good_spans</code> <code>list</code> <p>List of spans of good indices, unmodified.</p> <code>modified_spans</code> <code>list</code> <p>spans that are amended to bridge up to inds_to_span consecutive bad indices</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>def get_good_spans(bad_inds_mask, inds_to_span: int = 50):\n    \"\"\"\n    This function takes in a boolean mask of good and bad indices and\n    determines spans of consecutive good indices. It combines two neighboring spans\n    with a separation of less than inds_to_span and treats them as a single good span.\n\n    Parameters\n    ----------\n    bad_inds_mask : boolean mask\n        A boolean mask where True is a bad index and False is a good index.\n    inds_to_span : int, default 50\n        This indicates how many indices between two good spans should\n        be bridged to form a single good span.\n        For instance if span A is (1500, 2350) and span B is (2370, 3700),\n        then span A and span B would be combined into span A (1500, 3700)\n        since one would want to identify potential jumps in the space in between the original A and B.\n\n    Returns\n    -------\n    good_spans : list\n        List of spans of good indices, unmodified.\n    modified_spans : list\n        spans that are amended to bridge up to inds_to_span consecutive bad indices\n    \"\"\"\n    good_spans = get_span_start_stop(\n        np.arange(len(bad_inds_mask))[~bad_inds_mask]\n    )\n    if len(good_spans) &gt; 1:\n        modified_spans = []\n        for (start1, stop1), (start2, stop2) in zip(\n            good_spans[:-1], good_spans[1:]\n        ):\n            check_existing = [\n                entry\n                for entry in modified_spans\n                if start1\n                in range(entry[0] - inds_to_span, entry[1] + inds_to_span)\n            ]\n            if len(check_existing) &gt; 0:\n                modify_ind = modified_spans.index(check_existing[0])\n                if (start2 - stop1) &lt;= inds_to_span:\n                    modified_spans[modify_ind] = (check_existing[0][0], stop2)\n                else:\n                    modified_spans[modify_ind] = (check_existing[0][0], stop1)\n                    modified_spans.append((start2, stop2))\n                continue\n            if (start2 - stop1) &lt;= inds_to_span:\n                modified_spans.append((start1, stop2))\n            else:\n                modified_spans.append((start1, stop1))\n                modified_spans.append((start2, stop2))\n        return good_spans, modified_spans\n    else:\n        return None, good_spans\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/", "title": "position_dlc_project.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Holds bodyparts for use in DeepLabCut models</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@schema\nclass BodyPart(dj.Manual):\n    \"\"\"Holds bodyparts for use in DeepLabCut models\"\"\"\n\n    definition = \"\"\"\n    bodypart                : varchar(32)\n    ---\n    bodypart_description='' : varchar(80)\n    \"\"\"\n\n    @classmethod\n    def add_from_config(cls, bodyparts: List, descriptions: List = None):\n        \"\"\"Given a list of bodyparts from the config and\n        an optional list of descriptions, inserts into BodyPart table.\n\n        Parameters\n        ----------\n        bodyparts : List\n            list of bodyparts from config\n        descriptions : List, default None\n            optional list of descriptions for bodyparts.\n            If None, description is set to bodypart name\n        \"\"\"\n        if descriptions is not None:\n            bodyparts_dict = [\n                {\"bodypart\": bp, \"bodypart_description\": desc}\n                for (bp, desc) in zip(bodyparts, descriptions)\n            ]\n        else:\n            bodyparts_dict = [\n                {\"bodypart\": bp, \"bodypart_description\": bp} for bp in bodyparts\n            ]\n        cls.insert(bodyparts_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.BodyPart.add_from_config", "title": "<code>add_from_config(bodyparts, descriptions=None)</code>  <code>classmethod</code>", "text": "<p>Given a list of bodyparts from the config and an optional list of descriptions, inserts into BodyPart table.</p> <p>Parameters:</p> Name Type Description Default <code>bodyparts</code> <code>List</code> <p>list of bodyparts from config</p> required <code>descriptions</code> <code>List</code> <p>optional list of descriptions for bodyparts. If None, description is set to bodypart name</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef add_from_config(cls, bodyparts: List, descriptions: List = None):\n    \"\"\"Given a list of bodyparts from the config and\n    an optional list of descriptions, inserts into BodyPart table.\n\n    Parameters\n    ----------\n    bodyparts : List\n        list of bodyparts from config\n    descriptions : List, default None\n        optional list of descriptions for bodyparts.\n        If None, description is set to bodypart name\n    \"\"\"\n    if descriptions is not None:\n        bodyparts_dict = [\n            {\"bodypart\": bp, \"bodypart_description\": desc}\n            for (bp, desc) in zip(bodyparts, descriptions)\n        ]\n    else:\n        bodyparts_dict = [\n            {\"bodypart\": bp, \"bodypart_description\": bp} for bp in bodyparts\n        ]\n    cls.insert(bodyparts_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject", "title": "<code>DLCProject</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Table to facilitate creation of a new DeepLabCut model. With ability to edit config, extract frames, label frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@schema\nclass DLCProject(dj.Manual):\n    \"\"\"Table to facilitate creation of a new DeepLabCut model.\n    With ability to edit config, extract frames, label frames\n    \"\"\"\n\n    # Add more parameters as secondary keys...\n    # TODO: collapse params into blob dict\n    definition = \"\"\"\n    project_name     : varchar(100) # name of DLC project\n    ---\n    -&gt; LabTeam\n    bodyparts        : blob         # list of bodyparts to label\n    frames_per_video : int          # number of frames to extract from each video\n    config_path      : varchar(120) # path to config.yaml for model\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        \"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n        definition = \"\"\"\n        -&gt; DLCProject\n        -&gt; BodyPart\n        \"\"\"\n\n    class File(dj.Part):\n        definition = \"\"\"\n        # Paths of training files (e.g., labeled pngs, CSV or video)\n        -&gt; DLCProject\n        file_name: varchar(200) # Concise name to describe file\n        file_ext : enum(\"mp4\", \"csv\", \"h5\") # extension of file\n        ---\n        file_path: varchar(255)\n        \"\"\"\n\n    def insert1(self, key, **kwargs):\n        assert isinstance(\n            key[\"project_name\"], str\n        ), \"project_name must be a string\"\n        assert isinstance(\n            key[\"frames_per_video\"], int\n        ), \"frames_per_video must be of type `int`\"\n        super().insert1(key, **kwargs)\n\n    @classmethod\n    def insert_existing_project(\n        cls,\n        project_name: str,\n        lab_team: str,\n        config_path: str,\n        bodyparts: List = None,\n        frames_per_video: int = None,\n        add_to_files: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        insert an existing project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        config_path : str\n            path to project directory\n        bodyparts : list\n            optional list of bodyparts to label that\n            are not already in existing config\n        \"\"\"\n\n        # Read config\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        cfg = read_config(config_path)\n        if bodyparts:\n            bodyparts_to_add = [\n                bodypart\n                for bodypart in bodyparts\n                if bodypart not in cfg[\"bodyparts\"]\n            ]\n            all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n        else:\n            all_bodyparts = cfg[\"bodyparts\"]\n        BodyPart.add_from_config(cfg[\"bodyparts\"])\n        for bodypart in all_bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        # check bodyparts are in config, if not add\n        if len(bodyparts_to_add) &gt; 0:\n            add_to_config(config_path, bodyparts=bodyparts_to_add)\n        # Get frames per video from config. If passed as arg, check match\n        if frames_per_video:\n            if frames_per_video != cfg[\"numframes2pick\"]:\n                add_to_config(\n                    config_path, **{\"numframes2pick\": frames_per_video}\n                )\n        config_path = Path(config_path)\n        project_path = config_path.parent\n        dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n        if dlc_project_path not in project_path.as_posix():\n            project_dirname = project_path.name\n            dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n            if dest_folder.exists():\n                new_proj_dir = dest_folder.as_posix()\n            else:\n                new_proj_dir = shutil.copytree(\n                    src=project_path,\n                    dst=f\"{dlc_project_path}/{project_dirname}/\",\n                )\n            new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n            assert (\n                new_config_path.exists()\n            ), \"config.yaml does not exist in new project directory\"\n            config_path = new_config_path\n            add_to_config(config_path, **{\"project_path\": new_proj_dir})\n        # TODO still need to copy videos over to video dir\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path.as_posix(),\n            \"frames_per_video\": frames_per_video,\n        }\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in all_bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Check for training files to add\n            cls.add_training_files(key, **kwargs)\n        return {\n            \"project_name\": project_name,\n            \"config_path\": config_path.as_posix(),\n        }\n\n    @classmethod\n    def insert_new_project(\n        cls,\n        project_name: str,\n        bodyparts: List,\n        lab_team: str,\n        frames_per_video: int,\n        video_list: List,\n        project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n        output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n        set_permissions=False,\n        **kwargs,\n    ):\n        \"\"\"\n        insert a new project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        bodyparts : list\n            list of bodyparts to label. Should match bodyparts in BodyPart table\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        project_directory : str\n            directory where to create project.\n            (Default is '/cumulus/deeplabcut/')\n        frames_per_video : int\n            number of frames to extract from each video\n        video_list : list\n            list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n            to query VideoFile table for videos to train on.\n            Can also be list of absolute paths to import videos from\n        output_path : str\n            target path to output converted videos\n            (Default is '/nimbus/deeplabcut/videos/')\n        set_permissions : bool\n            if True, will set permissions for user and group to be read+write\n            (Default is False)\n        \"\"\"\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n\n        add_to_files = kwargs.pop(\"add_to_files\", True)\n        if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n            raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n        skeleton_node = None\n        # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n        # and pass to get_video_path to reference VideoFile table for path\n\n        if all(isinstance(n, Dict) for n in video_list):\n            videos_to_convert = [\n                get_video_path(video_key) for video_key in video_list\n            ]\n            videos = [\n                check_videofile(\n                    video_path=video[0],\n                    output_path=output_path,\n                    video_filename=video[1],\n                )[0].as_posix()\n                for video in videos_to_convert\n            ]\n        # If not dict, assume list of video file paths that may or may not need to be converted\n        else:\n            videos = []\n            if not all([Path(video).exists() for video in video_list]):\n                raise OSError(\"at least one file in video_list does not exist\")\n            for video in video_list:\n                video_path = Path(video).parent\n                video_filename = video.rsplit(\n                    video_path.as_posix(), maxsplit=1\n                )[-1].split(\"/\")[-1]\n                videos.extend(\n                    [\n                        check_videofile(\n                            video_path=video_path,\n                            output_path=output_path,\n                            video_filename=video_filename,\n                        )[0].as_posix()\n                    ]\n                )\n            if len(videos) &lt; 1:\n                raise ValueError(f\"no .mp4 videos found in{video_path}\")\n        from deeplabcut import create_new_project\n\n        config_path = create_new_project(\n            project_name,\n            lab_team,\n            videos,\n            working_directory=project_directory,\n            copy_videos=True,\n            multianimal=False,\n        )\n        for bodypart in bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        kwargs_copy = copy.deepcopy(kwargs)\n        kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n        add_to_config(\n            config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n        )\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path,\n            \"frames_per_video\": frames_per_video,\n        }\n        # TODO: make permissions setting more flexible.\n        if set_permissions:\n            permissions = (\n                stat.S_IRUSR\n                | stat.S_IWUSR\n                | stat.S_IRGRP\n                | stat.S_IWGRP\n                | stat.S_IROTH\n            )\n            username = getpass.getuser()\n            if not groupname:\n                groupname = username\n            _set_permissions(\n                directory=project_directory,\n                mode=permissions,\n                username=username,\n                groupname=groupname,\n            )\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Add videos to training files\n            cls.add_training_files(key, **kwargs)\n        if isinstance(config_path, PosixPath):\n            config_path = config_path.as_posix()\n        return {\"project_name\": project_name, \"config_path\": config_path}\n\n    @classmethod\n    def add_training_files(cls, key, **kwargs):\n        \"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n        config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n            \"config_path\"\n        )\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        if \"config_path\" in key:\n            del key[\"config_path\"]\n        cfg = read_config(config_path)\n        video_names = list(cfg[\"video_sets\"].keys())\n        training_files = []\n        for video in video_names:\n            video_name = os.path.splitext(\n                video.split(os.path.dirname(video) + \"/\")[-1]\n            )[0]\n            training_files.extend(\n                glob.glob(\n                    f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n                )\n            )\n        for video in video_names:\n            key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n            key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n                \".\"\n            )[-1]\n            key[\"file_path\"] = video\n            cls.File.insert1(key, **kwargs)\n        if len(training_files) &gt; 0:\n            for file in training_files:\n                video_name = os.path.dirname(file).split(\"/\")[-1]\n                file_type = os.path.splitext(\n                    file.split(os.path.dirname(file) + \"/\")[-1]\n                )[-1].split(\".\")[-1]\n                key[\"file_name\"] = f\"{video_name}_labeled_data\"\n                key[\"file_ext\"] = file_type\n                key[\"file_path\"] = file\n                cls.File.insert1(key, **kwargs)\n        else:\n            Warning(\"No training files to add\")\n\n    @classmethod\n    def run_extract_frames(cls, key, **kwargs):\n        \"\"\"Convenience function to launch DLC GUI for extracting frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import extract_frames\n\n        extract_frames(config_path, **kwargs)\n\n    @classmethod\n    def run_label_frames(cls, key):\n        \"\"\"Convenience function to launch DLC GUI for labeling frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import label_frames\n\n        label_frames(config_path)\n\n    @classmethod\n    def check_labels(cls, key, **kwargs):\n        \"\"\"Convenience function to check labels on\n        previously extracted and labeled frames\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import check_labels\n\n        check_labels(config_path, **kwargs)\n\n    @classmethod\n    def import_labeled_frames(\n        cls,\n        key: Dict,\n        import_project_path: Union[str, PosixPath],\n        video_filenames: Union[str, List],\n        **kwargs,\n    ):\n        \"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n        Parameters\n        ----------\n        key : Dict\n            key to specify entry in DLCProject table to add labeled frames to\n        import_project_path : str\n            absolute path to project directory containing labeled frames to import\n        video_filenames : str or List\n            filename or list of filenames of video(s) from which to import frames.\n            without file extension\n        \"\"\"\n        project_entry = (cls &amp; key).fetch1()\n        team_name = project_entry[\"team_name\"]\n        current_project_path = Path(project_entry[\"config_path\"]).parent\n        current_labeled_data_path = Path(\n            f\"{current_project_path.as_posix()}/labeled-data\"\n        )\n        if isinstance(import_project_path, PosixPath):\n            assert (\n                import_project_path.exists()\n            ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n            import_labeled_data_path = Path(\n                f\"{import_project_path.as_posix()}/labeled-data\"\n            )\n        else:\n            assert Path(\n                import_project_path\n            ).exists(), (\n                f\"import_project_path: {import_project_path} does not exist\"\n            )\n            import_labeled_data_path = Path(\n                f\"{import_project_path}/labeled-data\"\n            )\n        assert (\n            import_labeled_data_path.exists()\n        ), \"import_project has no directory 'labeled-data'\"\n        if not isinstance(video_filenames, List):\n            video_filenames = [video_filenames]\n        for video_file in video_filenames:\n            h5_file = glob.glob(\n                f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n            )[0]\n            dlc_df = pd.read_hdf(h5_file)\n            dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n            dlc_df.to_hdf(\n                Path(\n                    f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n                ).as_posix(),\n                \"df_with_missing\",\n            )\n        cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>             Bases: <code>Part</code></p> <p>Part table to hold bodyparts used in each project.</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>class BodyPart(dj.Part):\n    \"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCProject\n    -&gt; BodyPart\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_existing_project", "title": "<code>insert_existing_project(project_name, lab_team, config_path, bodyparts=None, frames_per_video=None, add_to_files=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert an existing project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>config_path</code> <code>str</code> <p>path to project directory</p> required <code>bodyparts</code> <code>list</code> <p>optional list of bodyparts to label that are not already in existing config</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_existing_project(\n    cls,\n    project_name: str,\n    lab_team: str,\n    config_path: str,\n    bodyparts: List = None,\n    frames_per_video: int = None,\n    add_to_files: bool = True,\n    **kwargs,\n):\n    \"\"\"\n    insert an existing project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    config_path : str\n        path to project directory\n    bodyparts : list\n        optional list of bodyparts to label that\n        are not already in existing config\n    \"\"\"\n\n    # Read config\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    cfg = read_config(config_path)\n    if bodyparts:\n        bodyparts_to_add = [\n            bodypart\n            for bodypart in bodyparts\n            if bodypart not in cfg[\"bodyparts\"]\n        ]\n        all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n    else:\n        all_bodyparts = cfg[\"bodyparts\"]\n    BodyPart.add_from_config(cfg[\"bodyparts\"])\n    for bodypart in all_bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    # check bodyparts are in config, if not add\n    if len(bodyparts_to_add) &gt; 0:\n        add_to_config(config_path, bodyparts=bodyparts_to_add)\n    # Get frames per video from config. If passed as arg, check match\n    if frames_per_video:\n        if frames_per_video != cfg[\"numframes2pick\"]:\n            add_to_config(\n                config_path, **{\"numframes2pick\": frames_per_video}\n            )\n    config_path = Path(config_path)\n    project_path = config_path.parent\n    dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n    if dlc_project_path not in project_path.as_posix():\n        project_dirname = project_path.name\n        dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n        if dest_folder.exists():\n            new_proj_dir = dest_folder.as_posix()\n        else:\n            new_proj_dir = shutil.copytree(\n                src=project_path,\n                dst=f\"{dlc_project_path}/{project_dirname}/\",\n            )\n        new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n        assert (\n            new_config_path.exists()\n        ), \"config.yaml does not exist in new project directory\"\n        config_path = new_config_path\n        add_to_config(config_path, **{\"project_path\": new_proj_dir})\n    # TODO still need to copy videos over to video dir\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path.as_posix(),\n        \"frames_per_video\": frames_per_video,\n    }\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in all_bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Check for training files to add\n        cls.add_training_files(key, **kwargs)\n    return {\n        \"project_name\": project_name,\n        \"config_path\": config_path.as_posix(),\n    }\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_new_project", "title": "<code>insert_new_project(project_name, bodyparts, lab_team, frames_per_video, video_list, project_directory=os.getenv('DLC_PROJECT_PATH'), output_path=os.getenv('DLC_VIDEO_PATH'), set_permissions=False, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert a new project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>bodyparts</code> <code>list</code> <p>list of bodyparts to label. Should match bodyparts in BodyPart table</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>project_directory</code> <code>str</code> <p>directory where to create project. (Default is '/cumulus/deeplabcut/')</p> <code>getenv('DLC_PROJECT_PATH')</code> <code>frames_per_video</code> <code>int</code> <p>number of frames to extract from each video</p> required <code>video_list</code> <code>list</code> <p>list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...] to query VideoFile table for videos to train on. Can also be list of absolute paths to import videos from</p> required <code>output_path</code> <code>str</code> <p>target path to output converted videos (Default is '/nimbus/deeplabcut/videos/')</p> <code>getenv('DLC_VIDEO_PATH')</code> <code>set_permissions</code> <code>bool</code> <p>if True, will set permissions for user and group to be read+write (Default is False)</p> <code>False</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_new_project(\n    cls,\n    project_name: str,\n    bodyparts: List,\n    lab_team: str,\n    frames_per_video: int,\n    video_list: List,\n    project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n    output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n    set_permissions=False,\n    **kwargs,\n):\n    \"\"\"\n    insert a new project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    bodyparts : list\n        list of bodyparts to label. Should match bodyparts in BodyPart table\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    project_directory : str\n        directory where to create project.\n        (Default is '/cumulus/deeplabcut/')\n    frames_per_video : int\n        number of frames to extract from each video\n    video_list : list\n        list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n        to query VideoFile table for videos to train on.\n        Can also be list of absolute paths to import videos from\n    output_path : str\n        target path to output converted videos\n        (Default is '/nimbus/deeplabcut/videos/')\n    set_permissions : bool\n        if True, will set permissions for user and group to be read+write\n        (Default is False)\n    \"\"\"\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n\n    add_to_files = kwargs.pop(\"add_to_files\", True)\n    if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n        raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n    skeleton_node = None\n    # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n    # and pass to get_video_path to reference VideoFile table for path\n\n    if all(isinstance(n, Dict) for n in video_list):\n        videos_to_convert = [\n            get_video_path(video_key) for video_key in video_list\n        ]\n        videos = [\n            check_videofile(\n                video_path=video[0],\n                output_path=output_path,\n                video_filename=video[1],\n            )[0].as_posix()\n            for video in videos_to_convert\n        ]\n    # If not dict, assume list of video file paths that may or may not need to be converted\n    else:\n        videos = []\n        if not all([Path(video).exists() for video in video_list]):\n            raise OSError(\"at least one file in video_list does not exist\")\n        for video in video_list:\n            video_path = Path(video).parent\n            video_filename = video.rsplit(\n                video_path.as_posix(), maxsplit=1\n            )[-1].split(\"/\")[-1]\n            videos.extend(\n                [\n                    check_videofile(\n                        video_path=video_path,\n                        output_path=output_path,\n                        video_filename=video_filename,\n                    )[0].as_posix()\n                ]\n            )\n        if len(videos) &lt; 1:\n            raise ValueError(f\"no .mp4 videos found in{video_path}\")\n    from deeplabcut import create_new_project\n\n    config_path = create_new_project(\n        project_name,\n        lab_team,\n        videos,\n        working_directory=project_directory,\n        copy_videos=True,\n        multianimal=False,\n    )\n    for bodypart in bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    kwargs_copy = copy.deepcopy(kwargs)\n    kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n    add_to_config(\n        config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n    )\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path,\n        \"frames_per_video\": frames_per_video,\n    }\n    # TODO: make permissions setting more flexible.\n    if set_permissions:\n        permissions = (\n            stat.S_IRUSR\n            | stat.S_IWUSR\n            | stat.S_IRGRP\n            | stat.S_IWGRP\n            | stat.S_IROTH\n        )\n        username = getpass.getuser()\n        if not groupname:\n            groupname = username\n        _set_permissions(\n            directory=project_directory,\n            mode=permissions,\n            username=username,\n            groupname=groupname,\n        )\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Add videos to training files\n        cls.add_training_files(key, **kwargs)\n    if isinstance(config_path, PosixPath):\n        config_path = config_path.as_posix()\n    return {\"project_name\": project_name, \"config_path\": config_path}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.add_training_files", "title": "<code>add_training_files(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Add training videos and labeled frames .h5 and .csv to DLCProject.File</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef add_training_files(cls, key, **kwargs):\n    \"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n    config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n        \"config_path\"\n    )\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    if \"config_path\" in key:\n        del key[\"config_path\"]\n    cfg = read_config(config_path)\n    video_names = list(cfg[\"video_sets\"].keys())\n    training_files = []\n    for video in video_names:\n        video_name = os.path.splitext(\n            video.split(os.path.dirname(video) + \"/\")[-1]\n        )[0]\n        training_files.extend(\n            glob.glob(\n                f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n            )\n        )\n    for video in video_names:\n        key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n        key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n            \".\"\n        )[-1]\n        key[\"file_path\"] = video\n        cls.File.insert1(key, **kwargs)\n    if len(training_files) &gt; 0:\n        for file in training_files:\n            video_name = os.path.dirname(file).split(\"/\")[-1]\n            file_type = os.path.splitext(\n                file.split(os.path.dirname(file) + \"/\")[-1]\n            )[-1].split(\".\")[-1]\n            key[\"file_name\"] = f\"{video_name}_labeled_data\"\n            key[\"file_ext\"] = file_type\n            key[\"file_path\"] = file\n            cls.File.insert1(key, **kwargs)\n    else:\n        Warning(\"No training files to add\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_extract_frames", "title": "<code>run_extract_frames(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for extracting frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_extract_frames(cls, key, **kwargs):\n    \"\"\"Convenience function to launch DLC GUI for extracting frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import extract_frames\n\n    extract_frames(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_label_frames", "title": "<code>run_label_frames(key)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for labeling frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_label_frames(cls, key):\n    \"\"\"Convenience function to launch DLC GUI for labeling frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import label_frames\n\n    label_frames(config_path)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.check_labels", "title": "<code>check_labels(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to check labels on previously extracted and labeled frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef check_labels(cls, key, **kwargs):\n    \"\"\"Convenience function to check labels on\n    previously extracted and labeled frames\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import check_labels\n\n    check_labels(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.import_labeled_frames", "title": "<code>import_labeled_frames(key, import_project_path, video_filenames, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Function to import pre-labeled frames from an existing project into a new project</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Dict</code> <p>key to specify entry in DLCProject table to add labeled frames to</p> required <code>import_project_path</code> <code>str</code> <p>absolute path to project directory containing labeled frames to import</p> required <code>video_filenames</code> <code>str or List</code> <p>filename or list of filenames of video(s) from which to import frames. without file extension</p> required Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef import_labeled_frames(\n    cls,\n    key: Dict,\n    import_project_path: Union[str, PosixPath],\n    video_filenames: Union[str, List],\n    **kwargs,\n):\n    \"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n    Parameters\n    ----------\n    key : Dict\n        key to specify entry in DLCProject table to add labeled frames to\n    import_project_path : str\n        absolute path to project directory containing labeled frames to import\n    video_filenames : str or List\n        filename or list of filenames of video(s) from which to import frames.\n        without file extension\n    \"\"\"\n    project_entry = (cls &amp; key).fetch1()\n    team_name = project_entry[\"team_name\"]\n    current_project_path = Path(project_entry[\"config_path\"]).parent\n    current_labeled_data_path = Path(\n        f\"{current_project_path.as_posix()}/labeled-data\"\n    )\n    if isinstance(import_project_path, PosixPath):\n        assert (\n            import_project_path.exists()\n        ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n        import_labeled_data_path = Path(\n            f\"{import_project_path.as_posix()}/labeled-data\"\n        )\n    else:\n        assert Path(\n            import_project_path\n        ).exists(), (\n            f\"import_project_path: {import_project_path} does not exist\"\n        )\n        import_labeled_data_path = Path(\n            f\"{import_project_path}/labeled-data\"\n        )\n    assert (\n        import_labeled_data_path.exists()\n    ), \"import_project has no directory 'labeled-data'\"\n    if not isinstance(video_filenames, List):\n        video_filenames = [video_filenames]\n    for video_file in video_filenames:\n        h5_file = glob.glob(\n            f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n        )[0]\n        dlc_df = pd.read_hdf(h5_file)\n        dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n        dlc_df.to_hdf(\n            Path(\n                f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n            ).as_posix(),\n            \"df_with_missing\",\n        )\n    cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.add_to_config", "title": "<code>add_to_config(config, bodyparts=None, skeleton_node=None, **kwargs)</code>", "text": "<p>Add necessary items to the config.yaml for the model</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to config.yaml</p> required <code>bodyparts</code> <code>list</code> <p>list of bodyparts to add to model</p> <code>None</code> <code>skeleton_node</code> <code>str</code> <p>(default is None) node to link LEDs in skeleton</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Other parameters of config to modify in key:value pairs</p> <code>{}</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>def add_to_config(\n    config, bodyparts: List = None, skeleton_node: str = None, **kwargs\n):\n    \"\"\"\n    Add necessary items to the config.yaml for the model\n    Parameters\n    ----------\n    config : str\n        Path to config.yaml\n    bodyparts : list\n        list of bodyparts to add to model\n    skeleton_node : str\n        (default is None) node to link LEDs in skeleton\n    kwargs : dict\n        Other parameters of config to modify in key:value pairs\n    \"\"\"\n\n    yaml = ruamel.yaml.YAML()\n    with open(config) as fp:\n        data = yaml.load(fp)\n    if bodyparts:\n        data[\"bodyparts\"] = bodyparts\n        led_parts = [element for element in bodyparts if \"LED\" in element]\n        if skeleton_node is not None:\n            bodypart_skeleton = [\n                list(link)\n                for link in combinations(led_parts, 2)\n                if skeleton_node in link\n            ]\n        else:\n            bodypart_skeleton = list(combinations(led_parts, 2))\n        other_parts = list(set(bodyparts) - set(led_parts))\n        for ind, part in enumerate(other_parts):\n            other_parts[ind] = [part, part]\n        bodypart_skeleton.append(other_parts)\n        data[\"skeleton\"] = bodypart_skeleton\n    for kwarg, val in kwargs.items():\n        if not isinstance(kwarg, str):\n            kwarg = str(kwarg)\n        data[kwarg] = val\n    with open(config, \"w\") as fw:\n        yaml.dump(data, fw)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/", "title": "position_dlc_selection.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPosSelection", "title": "<code>DLCPosSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Specify collection of upstream DLCCentroid and DLCOrientation entries to combine into a set of position information</p> Source code in <code>src/spyglass/position/v1/position_dlc_selection.py</code> <pre><code>@schema\nclass DLCPosSelection(dj.Manual):\n    \"\"\"\n    Specify collection of upstream DLCCentroid and DLCOrientation entries\n    to combine into a set of position information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCCentroid.proj(dlc_si_cohort_centroid='dlc_si_cohort_selection_name', centroid_analysis_file_name='analysis_file_name')\n    -&gt; DLCOrientation.proj(dlc_si_cohort_orientation='dlc_si_cohort_selection_name', orientation_analysis_file_name='analysis_file_name')\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPosV1", "title": "<code>DLCPosV1</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Combines upstream DLCCentroid and DLCOrientation entries into a single entry with a single Analysis NWB file</p> Source code in <code>src/spyglass/position/v1/position_dlc_selection.py</code> <pre><code>@schema\nclass DLCPosV1(dj.Computed):\n    \"\"\"\n    Combines upstream DLCCentroid and DLCOrientation\n    entries into a single entry with a single Analysis NWB file\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCPosSelection\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id      : varchar(80)\n    orientation_object_id   : varchar(80)\n    velocity_object_id      : varchar(80)\n    pose_eval_result        : longblob\n    \"\"\"\n\n    def make(self, key):\n        orig_key = copy.deepcopy(key)\n        key[\"pose_eval_result\"] = self.evaluate_pose_estimation(key)\n\n        pos_nwb = (DLCCentroid &amp; key).fetch_nwb()[0]\n        ori_nwb = (DLCOrientation &amp; key).fetch_nwb()[0]\n\n        pos_obj = pos_nwb[\"dlc_position\"].spatial_series[\"position\"]\n        vel_obj = pos_nwb[\"dlc_velocity\"].time_series[\"velocity\"]\n        vid_frame_obj = pos_nwb[\"dlc_velocity\"].time_series[\"video_frame_ind\"]\n        ori_obj = ori_nwb[\"dlc_orientation\"].spatial_series[\"orientation\"]\n\n        position = pynwb.behavior.Position()\n        orientation = pynwb.behavior.CompassDirection()\n        velocity = pynwb.behavior.BehavioralTimeSeries()\n\n        position.create_spatial_series(\n            name=pos_obj.name,\n            timestamps=np.asarray(pos_obj.timestamps),\n            conversion=pos_obj.conversion,\n            data=np.asarray(pos_obj.data),\n            reference_frame=pos_obj.reference_frame,\n            comments=pos_obj.comments,\n            description=pos_obj.description,\n        )\n\n        orientation.create_spatial_series(\n            name=ori_obj.name,\n            timestamps=np.asarray(ori_obj.timestamps),\n            conversion=ori_obj.conversion,\n            data=np.asarray(ori_obj.data),\n            reference_frame=ori_obj.reference_frame,\n            comments=ori_obj.comments,\n            description=ori_obj.description,\n        )\n\n        velocity.create_timeseries(\n            name=vel_obj.name,\n            timestamps=np.asarray(vel_obj.timestamps),\n            conversion=vel_obj.conversion,\n            unit=vel_obj.unit,\n            data=np.asarray(vel_obj.data),\n            comments=vel_obj.comments,\n            description=vel_obj.description,\n        )\n\n        velocity.create_timeseries(\n            name=vid_frame_obj.name,\n            unit=vid_frame_obj.unit,\n            timestamps=np.asarray(vid_frame_obj.timestamps),\n            data=np.asarray(vid_frame_obj.data),\n            description=vid_frame_obj.description,\n            comments=vid_frame_obj.comments,\n        )\n\n        # Add to Analysis NWB file\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n        key[\"position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], position\n        )\n        key[\"velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], velocity\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n        self.insert1(key)\n\n        from ..position_merge import PositionOutput\n\n        part_name = to_camel_case(self.table_name.split(\"__\")[-1])\n        # TODO: The next line belongs in a merge table function\n        PositionOutput._merge_insert(\n            [orig_key], part_name=part_name, skip_duplicates=True\n        )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(nwb_data[\"position\"].get_spatial_series().timestamps),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"orientation\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(nwb_data[\"position\"].get_spatial_series().data),\n                    np.asarray(\n                        nwb_data[\"orientation\"].get_spatial_series().data\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n\n    @classmethod\n    def evaluate_pose_estimation(cls, key):\n        likelihood_thresh = []\n        valid_fields = (\n            DLCSmoothInterpCohort.BodyPart().fetch().dtype.fields.keys()\n        )\n        centroid_key = {k: val for k, val in key.items() if k in valid_fields}\n        centroid_key[\"dlc_si_cohort_selection_name\"] = key[\n            \"dlc_si_cohort_centroid\"\n        ]\n        orientation_key = centroid_key.copy()\n        orientation_key[\"dlc_si_cohort_selection_name\"] = key[\n            \"dlc_si_cohort_orientation\"\n        ]\n        centroid_bodyparts, centroid_si_params = (\n            DLCSmoothInterpCohort.BodyPart &amp; centroid_key\n        ).fetch(\"bodypart\", \"dlc_si_params_name\")\n        orientation_bodyparts, orientation_si_params = (\n            DLCSmoothInterpCohort.BodyPart &amp; orientation_key\n        ).fetch(\"bodypart\", \"dlc_si_params_name\")\n        for param in np.unique(\n            np.concatenate((centroid_si_params, orientation_si_params))\n        ):\n            likelihood_thresh.append(\n                (\n                    DLCSmoothInterpParams() &amp; {\"dlc_si_params_name\": param}\n                ).fetch1(\"params\")[\"likelihood_thresh\"]\n            )\n\n        if len(np.unique(likelihood_thresh)) &gt; 1:\n            raise ValueError(\"more than one likelihood threshold used\")\n        like_thresh = likelihood_thresh[0]\n        bodyparts = np.unique([*centroid_bodyparts, *orientation_bodyparts])\n        fields = list(DLCPoseEstimation.BodyPart.fetch().dtype.fields.keys())\n        pose_estimation_key = {k: v for k, v in key.items() if k in fields}\n        pose_estimation_df = pd.concat(\n            {\n                bodypart: (\n                    DLCPoseEstimation.BodyPart()\n                    &amp; {**pose_estimation_key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in bodyparts.tolist()\n            },\n            axis=1,\n        )\n        df_filter = {\n            bodypart: pose_estimation_df[bodypart][\"likelihood\"] &lt; like_thresh\n            for bodypart in bodyparts\n            if bodypart in pose_estimation_df.columns\n        }\n        sub_thresh_ind_dict = {\n            bodypart: {\n                \"inds\": np.where(\n                    ~np.isnan(\n                        pose_estimation_df[bodypart][\"likelihood\"].where(\n                            df_filter[bodypart]\n                        )\n                    )\n                )[0],\n            }\n            for bodypart in bodyparts\n        }\n        sub_thresh_percent_dict = {\n            bodypart: (\n                len(\n                    np.where(\n                        ~np.isnan(\n                            pose_estimation_df[bodypart][\"likelihood\"].where(\n                                df_filter[bodypart]\n                            )\n                        )\n                    )[0]\n                )\n                / len(pose_estimation_df)\n            )\n            * 100\n            for bodypart in bodyparts\n        }\n        return sub_thresh_percent_dict\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPosVideo", "title": "<code>DLCPosVideo</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/position/v1/position_dlc_selection.py</code> <pre><code>@schema\nclass DLCPosVideo(dj.Computed):\n    \"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCPosVideoSelection\n    ---\n    \"\"\"\n\n    def make(self, key):\n        from tqdm import tqdm as tqdm\n\n        params = (DLCPosVideoParams &amp; key).fetch1(\"params\")\n        if \"video_params\" not in params:\n            params[\"video_params\"] = {}\n        M_TO_CM = 100\n        interval_list_name = (\n            convert_epoch_interval_name_to_position_interval_name(\n                {\n                    \"nwb_file_name\": key[\"nwb_file_name\"],\n                    \"epoch\": key[\"epoch\"],\n                },\n                populate_missing=False,\n            )\n        )\n        key[\"interval_list_name\"] = interval_list_name\n        epoch = (\n            int(\n                key[\"interval_list_name\"]\n                .replace(\"pos \", \"\")\n                .replace(\" valid times\", \"\")\n            )\n            + 1\n        )\n        pose_estimation_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"epoch\": epoch,\n            \"dlc_model_name\": key[\"dlc_model_name\"],\n            \"dlc_model_params_name\": key[\"dlc_model_params_name\"],\n        }\n        pose_estimation_params, video_filename, output_dir = (\n            DLCPoseEstimationSelection() &amp; pose_estimation_key\n        ).fetch1(\n            \"pose_estimation_params\", \"video_path\", \"pose_estimation_output_dir\"\n        )\n        print(f\"video filename: {video_filename}\")\n        meters_per_pixel = (DLCPoseEstimation() &amp; pose_estimation_key).fetch1(\n            \"meters_per_pixel\"\n        )\n        crop = None\n        if \"cropping\" in pose_estimation_params:\n            crop = pose_estimation_params[\"cropping\"]\n        print(\"Loading position data...\")\n        position_info_df = (\n            DLCPosV1()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"epoch\": epoch,\n                \"dlc_si_cohort_centroid\": key[\"dlc_si_cohort_centroid\"],\n                \"dlc_centroid_params_name\": key[\"dlc_centroid_params_name\"],\n                \"dlc_si_cohort_orientation\": key[\"dlc_si_cohort_orientation\"],\n                \"dlc_orientation_params_name\": key[\n                    \"dlc_orientation_params_name\"\n                ],\n            }\n        ).fetch1_dataframe()\n        pose_estimation_df = pd.concat(\n            {\n                bodypart: (\n                    DLCPoseEstimation.BodyPart()\n                    &amp; {**pose_estimation_key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in (\n                    DLCSmoothInterpCohort.BodyPart &amp; pose_estimation_key\n                )\n                .fetch(\"bodypart\")\n                .tolist()\n            },\n            axis=1,\n        )\n        assert len(pose_estimation_df) == len(position_info_df), (\n            f\"length of pose_estimation_df: {len(pose_estimation_df)} \"\n            f\"does not match the length of position_info_df: {len(position_info_df)}.\"\n        )\n\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        if Path(output_dir).exists():\n            output_video_filename = (\n                f\"{Path(output_dir).as_posix()}/\"\n                f\"{nwb_base_filename}_{epoch:02d}_\"\n                f'{key[\"dlc_si_cohort_centroid\"]}_'\n                f'{key[\"dlc_centroid_params_name\"]}'\n                f'{key[\"dlc_orientation_params_name\"]}.mp4'\n            )\n        else:\n            output_video_filename = (\n                f\"{nwb_base_filename}_{epoch:02d}_\"\n                f'{key[\"dlc_si_cohort_centroid\"]}_'\n                f'{key[\"dlc_centroid_params_name\"]}'\n                f'{key[\"dlc_orientation_params_name\"]}.mp4'\n            )\n        idx = pd.IndexSlice\n        video_frame_inds = (\n            position_info_df[\"video_frame_ind\"].astype(int).to_numpy()\n        )\n        centroids = {\n            bodypart: pose_estimation_df.loc[\n                :, idx[bodypart, (\"x\", \"y\")]\n            ].to_numpy()\n            for bodypart in pose_estimation_df.columns.levels[0]\n        }\n        if params.get(\"incl_likelihood\", None):\n            likelihoods = {\n                bodypart: pose_estimation_df.loc[\n                    :, idx[bodypart, (\"likelihood\")]\n                ].to_numpy()\n                for bodypart in pose_estimation_df.columns.levels[0]\n            }\n        else:\n            likelihoods = None\n        position_mean = {\n            \"DLC\": np.asarray(position_info_df[[\"position_x\", \"position_y\"]])\n        }\n        orientation_mean = {\n            \"DLC\": np.asarray(position_info_df[[\"orientation\"]])\n        }\n        position_time = np.asarray(position_info_df.index)\n        cm_per_pixel = meters_per_pixel * M_TO_CM\n        percent_frames = params.get(\"percent_frames\", None)\n        frames = params.get(\"frames\", None)\n        if frames is not None:\n            frames_arr = np.arange(frames[0], frames[1])\n        else:\n            frames_arr = frames\n\n        print(\"Making video...\")\n        make_video(\n            video_filename=video_filename,\n            video_frame_inds=video_frame_inds,\n            position_mean=position_mean,\n            orientation_mean=orientation_mean,\n            centroids=centroids,\n            likelihoods=likelihoods,\n            position_time=position_time,\n            video_time=None,\n            processor=params.get(\"processor\", \"matplotlib\"),\n            frames=frames_arr,\n            percent_frames=percent_frames,\n            output_video_filename=output_video_filename,\n            cm_to_pixels=cm_per_pixel,\n            disable_progressbar=False,\n            crop=crop,\n            **params[\"video_params\"],\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/", "title": "position_dlc_training.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTrainingParams", "title": "<code>DLCModelTrainingParams</code>", "text": "<p>             Bases: <code>Lookup</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>@schema\nclass DLCModelTrainingParams(dj.Lookup):\n    definition = \"\"\"\n    # Parameters to specify a DLC model training instance\n    # For DLC \u2264 v2.0, include scorer_lecacy = True in params\n    dlc_training_params_name      : varchar(50) # descriptive name of parameter set\n    ---\n    params                        : longblob    # dictionary of all applicable parameters\n    \"\"\"\n\n    required_parameters = (\n        \"shuffle\",\n        \"trainingsetindex\",\n        \"net_type\",\n        \"gputouse\",\n    )\n    skipped_parameters = (\"project_path\", \"video_sets\")\n\n    @classmethod\n    def insert_new_params(cls, paramset_name: str, params: dict, **kwargs):\n        \"\"\"\n        Insert a new set of training parameters into dlc.TrainingParamSet.\n\n        Parameters\n        ----------\n        paramset_name : str\n            Description of parameter set to be inserted\n        params : dict\n            Dictionary including all settings to specify model training.\n            Must include shuffle &amp; trainingsetindex b/c not in config.yaml.\n            project_path and video_sets will be overwritten by config.yaml.\n            Note that trainingsetindex is 0-indexed\n        \"\"\"\n\n        for required_param in cls.required_parameters:\n            assert required_param in params, (\n                \"Missing required parameter: \" + required_param\n            )\n        for skipped_param in cls.skipped_parameters:\n            if skipped_param in params:\n                params.pop(skipped_param)\n\n        param_dict = {\n            \"dlc_training_params_name\": paramset_name,\n            \"params\": params,\n        }\n        param_query = cls &amp; {\n            \"dlc_training_params_name\": param_dict[\"dlc_training_params_name\"]\n        }\n        # If the specified param-set already exists\n        # Not sure we need this part, as much just a check if the name is the same\n        if param_query:\n            existing_paramset_name = param_query.fetch1(\n                \"dlc_training_params_name\"\n            )\n            if (\n                existing_paramset_name == paramset_name\n            ):  # If existing name same:\n                return print(\n                    f\"New param set not added\\n\"\n                    f\"A param set with name: {paramset_name} already exists\"\n                )\n        else:\n            cls.insert1(\n                param_dict, **kwargs\n            )  # if duplicate, will raise duplicate error\n            # if this will raise duplicate error, why is above check needed? @datajoint\n\n    @classmethod\n    def get_accepted_params(cls):\n        from deeplabcut import create_training_dataset, train_network\n\n        return list(\n            set(\n                [\n                    *list(inspect.signature(train_network).parameters),\n                    *list(\n                        inspect.signature(create_training_dataset).parameters\n                    ),\n                ]\n            )\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTrainingParams.insert_new_params", "title": "<code>insert_new_params(paramset_name, params, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Insert a new set of training parameters into dlc.TrainingParamSet.</p> <p>Parameters:</p> Name Type Description Default <code>paramset_name</code> <code>str</code> <p>Description of parameter set to be inserted</p> required <code>params</code> <code>dict</code> <p>Dictionary including all settings to specify model training. Must include shuffle &amp; trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed</p> required Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>@classmethod\ndef insert_new_params(cls, paramset_name: str, params: dict, **kwargs):\n    \"\"\"\n    Insert a new set of training parameters into dlc.TrainingParamSet.\n\n    Parameters\n    ----------\n    paramset_name : str\n        Description of parameter set to be inserted\n    params : dict\n        Dictionary including all settings to specify model training.\n        Must include shuffle &amp; trainingsetindex b/c not in config.yaml.\n        project_path and video_sets will be overwritten by config.yaml.\n        Note that trainingsetindex is 0-indexed\n    \"\"\"\n\n    for required_param in cls.required_parameters:\n        assert required_param in params, (\n            \"Missing required parameter: \" + required_param\n        )\n    for skipped_param in cls.skipped_parameters:\n        if skipped_param in params:\n            params.pop(skipped_param)\n\n    param_dict = {\n        \"dlc_training_params_name\": paramset_name,\n        \"params\": params,\n    }\n    param_query = cls &amp; {\n        \"dlc_training_params_name\": param_dict[\"dlc_training_params_name\"]\n    }\n    # If the specified param-set already exists\n    # Not sure we need this part, as much just a check if the name is the same\n    if param_query:\n        existing_paramset_name = param_query.fetch1(\n            \"dlc_training_params_name\"\n        )\n        if (\n            existing_paramset_name == paramset_name\n        ):  # If existing name same:\n            return print(\n                f\"New param set not added\\n\"\n                f\"A param set with name: {paramset_name} already exists\"\n            )\n    else:\n        cls.insert1(\n            param_dict, **kwargs\n        )  # if duplicate, will raise duplicate error\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTraining", "title": "<code>DLCModelTraining</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>@schema\nclass DLCModelTraining(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCModelTrainingSelection\n    ---\n    project_path         : varchar(255) # Path to project directory\n    latest_snapshot: int unsigned # latest exact snapshot index (i.e., never -1)\n    config_template: longblob     # stored full config file\n    \"\"\"\n\n    # To continue from previous training snapshot, devs suggest editing pose_cfg.yml\n    # https://github.com/DeepLabCut/DeepLabCut/issues/70\n\n    def make(self, key):\n        \"\"\"Launch training for each entry in DLCModelTrainingSelection via `.populate()`.\"\"\"\n        model_prefix = (DLCModelTrainingSelection &amp; key).fetch1(\"model_prefix\")\n        from deeplabcut import create_training_dataset, train_network\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        from . import dlc_reader\n\n        try:\n            from deeplabcut.utils.auxiliaryfunctions import get_model_folder\n        except ImportError:\n            from deeplabcut.utils.auxiliaryfunctions import (\n                GetModelFolder as get_model_folder,\n            )\n        config_path, project_name = (DLCProject() &amp; key).fetch1(\n            \"config_path\", \"project_name\"\n        )\n        with OutputLogger(\n            name=\"DLC_project_{project_name}_training\",\n            path=f\"{os.path.dirname(config_path)}/log.log\",\n            print_console=True,\n        ) as logger:\n            dlc_config = read_config(config_path)\n            project_path = dlc_config[\"project_path\"]\n            key[\"project_path\"] = project_path\n            # ---- Build and save DLC configuration (yaml) file ----\n            _, dlc_config = dlc_reader.read_yaml(project_path)\n            if not dlc_config:\n                dlc_config = read_config(config_path)\n            dlc_config.update((DLCModelTrainingParams &amp; key).fetch1(\"params\"))\n            dlc_config.update(\n                {\n                    \"project_path\": Path(project_path).as_posix(),\n                    \"modelprefix\": model_prefix,\n                    \"train_fraction\": dlc_config[\"TrainingFraction\"][\n                        int(dlc_config[\"trainingsetindex\"])\n                    ],\n                    \"training_filelist_datajoint\": [  # don't overwrite origin video_sets\n                        Path(fp).as_posix()\n                        for fp in (DLCProject.File &amp; key).fetch(\"file_path\")\n                    ],\n                }\n            )\n            # Write dlc config file to base project folder\n            # TODO: need to make sure this will work\n            dlc_cfg_filepath = dlc_reader.save_yaml(project_path, dlc_config)\n            # ---- create training dataset ----\n            training_dataset_input_args = list(\n                inspect.signature(create_training_dataset).parameters\n            )\n            training_dataset_kwargs = {\n                k: v\n                for k, v in dlc_config.items()\n                if k in training_dataset_input_args\n            }\n            logger.logger.info(\"creating training dataset\")\n            create_training_dataset(dlc_cfg_filepath, **training_dataset_kwargs)\n            # ---- Trigger DLC model training job ----\n            train_network_input_args = list(\n                inspect.signature(train_network).parameters\n            )\n            train_network_kwargs = {\n                k: v\n                for k, v in dlc_config.items()\n                if k in train_network_input_args\n            }\n            for k in [\"shuffle\", \"trainingsetindex\", \"maxiters\"]:\n                if k in train_network_kwargs:\n                    train_network_kwargs[k] = int(train_network_kwargs[k])\n            try:\n                train_network(dlc_cfg_filepath, **train_network_kwargs)\n            except (\n                KeyboardInterrupt\n            ):  # Instructions indicate to train until interrupt\n                logger.logger.info(\n                    \"DLC training stopped via Keyboard Interrupt\"\n                )\n\n            snapshots = list(\n                (\n                    project_path\n                    / get_model_folder(\n                        trainFraction=dlc_config[\"train_fraction\"],\n                        shuffle=dlc_config[\"shuffle\"],\n                        cfg=dlc_config,\n                        modelprefix=dlc_config[\"modelprefix\"],\n                    )\n                    / \"train\"\n                ).glob(\"*index*\")\n            )\n            max_modified_time = 0\n            # DLC goes by snapshot magnitude when judging 'latest' for evaluation\n            # Here, we mean most recently generated\n            for snapshot in snapshots:\n                modified_time = os.path.getmtime(snapshot)\n                if modified_time &gt; max_modified_time:\n                    latest_snapshot = int(snapshot.stem[9:])\n                    max_modified_time = modified_time\n\n            self.insert1(\n                {\n                    **key,\n                    \"latest_snapshot\": latest_snapshot,\n                    \"config_template\": dlc_config,\n                }\n            )\n            from .position_dlc_model import DLCModelSource\n\n            dlc_model_name = f\"{key['project_name']}_{key['dlc_training_params_name']}_{key['training_id']:02d}\"\n            DLCModelSource.insert_entry(\n                dlc_model_name=dlc_model_name,\n                project_name=key[\"project_name\"],\n                source=\"FromUpstream\",\n                key=key,\n                skip_duplicates=True,\n            )\n        print(\n            f\"Inserted {dlc_model_name} from {key['project_name']} into DLCModelSource\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTraining.make", "title": "<code>make(key)</code>", "text": "<p>Launch training for each entry in DLCModelTrainingSelection via <code>.populate()</code>.</p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>def make(self, key):\n    \"\"\"Launch training for each entry in DLCModelTrainingSelection via `.populate()`.\"\"\"\n    model_prefix = (DLCModelTrainingSelection &amp; key).fetch1(\"model_prefix\")\n    from deeplabcut import create_training_dataset, train_network\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    from . import dlc_reader\n\n    try:\n        from deeplabcut.utils.auxiliaryfunctions import get_model_folder\n    except ImportError:\n        from deeplabcut.utils.auxiliaryfunctions import (\n            GetModelFolder as get_model_folder,\n        )\n    config_path, project_name = (DLCProject() &amp; key).fetch1(\n        \"config_path\", \"project_name\"\n    )\n    with OutputLogger(\n        name=\"DLC_project_{project_name}_training\",\n        path=f\"{os.path.dirname(config_path)}/log.log\",\n        print_console=True,\n    ) as logger:\n        dlc_config = read_config(config_path)\n        project_path = dlc_config[\"project_path\"]\n        key[\"project_path\"] = project_path\n        # ---- Build and save DLC configuration (yaml) file ----\n        _, dlc_config = dlc_reader.read_yaml(project_path)\n        if not dlc_config:\n            dlc_config = read_config(config_path)\n        dlc_config.update((DLCModelTrainingParams &amp; key).fetch1(\"params\"))\n        dlc_config.update(\n            {\n                \"project_path\": Path(project_path).as_posix(),\n                \"modelprefix\": model_prefix,\n                \"train_fraction\": dlc_config[\"TrainingFraction\"][\n                    int(dlc_config[\"trainingsetindex\"])\n                ],\n                \"training_filelist_datajoint\": [  # don't overwrite origin video_sets\n                    Path(fp).as_posix()\n                    for fp in (DLCProject.File &amp; key).fetch(\"file_path\")\n                ],\n            }\n        )\n        # Write dlc config file to base project folder\n        # TODO: need to make sure this will work\n        dlc_cfg_filepath = dlc_reader.save_yaml(project_path, dlc_config)\n        # ---- create training dataset ----\n        training_dataset_input_args = list(\n            inspect.signature(create_training_dataset).parameters\n        )\n        training_dataset_kwargs = {\n            k: v\n            for k, v in dlc_config.items()\n            if k in training_dataset_input_args\n        }\n        logger.logger.info(\"creating training dataset\")\n        create_training_dataset(dlc_cfg_filepath, **training_dataset_kwargs)\n        # ---- Trigger DLC model training job ----\n        train_network_input_args = list(\n            inspect.signature(train_network).parameters\n        )\n        train_network_kwargs = {\n            k: v\n            for k, v in dlc_config.items()\n            if k in train_network_input_args\n        }\n        for k in [\"shuffle\", \"trainingsetindex\", \"maxiters\"]:\n            if k in train_network_kwargs:\n                train_network_kwargs[k] = int(train_network_kwargs[k])\n        try:\n            train_network(dlc_cfg_filepath, **train_network_kwargs)\n        except (\n            KeyboardInterrupt\n        ):  # Instructions indicate to train until interrupt\n            logger.logger.info(\n                \"DLC training stopped via Keyboard Interrupt\"\n            )\n\n        snapshots = list(\n            (\n                project_path\n                / get_model_folder(\n                    trainFraction=dlc_config[\"train_fraction\"],\n                    shuffle=dlc_config[\"shuffle\"],\n                    cfg=dlc_config,\n                    modelprefix=dlc_config[\"modelprefix\"],\n                )\n                / \"train\"\n            ).glob(\"*index*\")\n        )\n        max_modified_time = 0\n        # DLC goes by snapshot magnitude when judging 'latest' for evaluation\n        # Here, we mean most recently generated\n        for snapshot in snapshots:\n            modified_time = os.path.getmtime(snapshot)\n            if modified_time &gt; max_modified_time:\n                latest_snapshot = int(snapshot.stem[9:])\n                max_modified_time = modified_time\n\n        self.insert1(\n            {\n                **key,\n                \"latest_snapshot\": latest_snapshot,\n                \"config_template\": dlc_config,\n            }\n        )\n        from .position_dlc_model import DLCModelSource\n\n        dlc_model_name = f\"{key['project_name']}_{key['dlc_training_params_name']}_{key['training_id']:02d}\"\n        DLCModelSource.insert_entry(\n            dlc_model_name=dlc_model_name,\n            project_name=key[\"project_name\"],\n            source=\"FromUpstream\",\n            key=key,\n            skip_duplicates=True,\n        )\n    print(\n        f\"Inserted {dlc_model_name} from {key['project_name']} into DLCModelSource\"\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/", "title": "position_trodes_position.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosParams", "title": "<code>TrodesPosParams</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Parameters for calculating the position (centroid, velocity, orientation)</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosParams(dj.Manual):\n    \"\"\"\n    Parameters for calculating the position (centroid, velocity, orientation)\n    \"\"\"\n\n    definition = \"\"\"\n    trodes_pos_params_name: varchar(80) # name for this set of parameters\n    ---\n    params: longblob\n    \"\"\"\n\n    @property\n    def default_pk(self):\n        return {\"trodes_pos_params_name\": \"default\"}\n\n    @property\n    def default_params(self):\n        return {\n            \"max_LED_separation\": 9.0,\n            \"max_plausible_speed\": 300.0,\n            \"position_smoothing_duration\": 0.125,\n            \"speed_smoothing_std_dev\": 0.100,\n            \"orient_smoothing_std_dev\": 0.001,\n            \"led1_is_front\": 1,\n            \"is_upsampled\": 0,\n            \"upsampling_sampling_rate\": None,\n            \"upsampling_interpolation_method\": \"linear\",\n        }\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        \"\"\"\n        Insert default parameter set for position determination\n        \"\"\"\n        cls.insert1(\n            {**cls().default_pk, \"params\": cls().default_params},\n            skip_duplicates=True,\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; cls().default_pk\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            return (cls &amp; cls().default_pk).fetch1()\n\n        return query.fetch1()\n\n    @classmethod\n    def get_accepted_params(cls):\n        return [k for k in cls().default_params.keys()]\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosParams.insert_default", "title": "<code>insert_default(**kwargs)</code>  <code>classmethod</code>", "text": "<p>Insert default parameter set for position determination</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@classmethod\ndef insert_default(cls, **kwargs):\n    \"\"\"\n    Insert default parameter set for position determination\n    \"\"\"\n    cls.insert1(\n        {**cls().default_pk, \"params\": cls().default_params},\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosSelection", "title": "<code>TrodesPosSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Table to pair an interval with position data and position determination parameters</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosSelection(dj.Manual):\n    \"\"\"\n    Table to pair an interval with position data\n    and position determination parameters\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; RawPosition\n    -&gt; TrodesPosParams\n    \"\"\"\n\n    @classmethod\n    def insert_with_default(\n        cls,\n        key: dict,\n        skip_duplicates: bool = False,\n        edit_defaults: dict = {},\n        edit_name: str = None,\n    ) -&gt; None:\n        \"\"\"Insert key with default parameters.\n\n        To change defaults, supply a dict as edit_defaults with a name for\n        the new paramset as edit_name.\n\n        Parameters\n        ----------\n        key: Union[dict, str]\n            Restriction uniquely identifying entr(y/ies) in RawPosition.\n        skip_duplicates: bool, optional\n            Skip duplicate entries.\n        edit_defaults: dict, optional\n            Dictionary of overrides to default parameters.\n        edit_name: str, optional\n            If edit_defauts is passed, the name of the new entry\n\n        Raises\n        ------\n        ValueError\n            Key does not identify any entries in RawPosition.\n        \"\"\"\n        query = RawPosition &amp; key\n        if not query:\n            raise ValueError(f\"Found no entries found for {key}\")\n\n        param_pk, param_name = list(TrodesPosParams().default_pk.items())[0]\n\n        if bool(edit_defaults) ^ bool(edit_name):  # XOR: only one of them\n            raise ValueError(\"Must specify both edit_defauts and edit_name\")\n\n        elif edit_defaults and edit_name:\n            TrodesPosParams.insert1(\n                {\n                    param_pk: edit_name,\n                    \"params\": {\n                        **TrodesPosParams().default_params,\n                        **edit_defaults,\n                    },\n                },\n                skip_duplicates=skip_duplicates,\n            )\n\n        cls.insert(\n            [\n                {**k, param_pk: edit_name or param_name}\n                for k in query.fetch(\"KEY\", as_dict=True)\n            ],\n            skip_duplicates=skip_duplicates,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosSelection.insert_with_default", "title": "<code>insert_with_default(key, skip_duplicates=False, edit_defaults={}, edit_name=None)</code>  <code>classmethod</code>", "text": "<p>Insert key with default parameters.</p> <p>To change defaults, supply a dict as edit_defaults with a name for the new paramset as edit_name.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Restriction uniquely identifying entr(y/ies) in RawPosition.</p> required <code>skip_duplicates</code> <code>bool</code> <p>Skip duplicate entries.</p> <code>False</code> <code>edit_defaults</code> <code>dict</code> <p>Dictionary of overrides to default parameters.</p> <code>{}</code> <code>edit_name</code> <code>str</code> <p>If edit_defauts is passed, the name of the new entry</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Key does not identify any entries in RawPosition.</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@classmethod\ndef insert_with_default(\n    cls,\n    key: dict,\n    skip_duplicates: bool = False,\n    edit_defaults: dict = {},\n    edit_name: str = None,\n) -&gt; None:\n    \"\"\"Insert key with default parameters.\n\n    To change defaults, supply a dict as edit_defaults with a name for\n    the new paramset as edit_name.\n\n    Parameters\n    ----------\n    key: Union[dict, str]\n        Restriction uniquely identifying entr(y/ies) in RawPosition.\n    skip_duplicates: bool, optional\n        Skip duplicate entries.\n    edit_defaults: dict, optional\n        Dictionary of overrides to default parameters.\n    edit_name: str, optional\n        If edit_defauts is passed, the name of the new entry\n\n    Raises\n    ------\n    ValueError\n        Key does not identify any entries in RawPosition.\n    \"\"\"\n    query = RawPosition &amp; key\n    if not query:\n        raise ValueError(f\"Found no entries found for {key}\")\n\n    param_pk, param_name = list(TrodesPosParams().default_pk.items())[0]\n\n    if bool(edit_defaults) ^ bool(edit_name):  # XOR: only one of them\n        raise ValueError(\"Must specify both edit_defauts and edit_name\")\n\n    elif edit_defaults and edit_name:\n        TrodesPosParams.insert1(\n            {\n                param_pk: edit_name,\n                \"params\": {\n                    **TrodesPosParams().default_params,\n                    **edit_defaults,\n                },\n            },\n            skip_duplicates=skip_duplicates,\n        )\n\n    cls.insert(\n        [\n            {**k, param_pk: edit_name or param_name}\n            for k in query.fetch(\"KEY\", as_dict=True)\n        ],\n        skip_duplicates=skip_duplicates,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosV1", "title": "<code>TrodesPosV1</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Table to calculate the position based on Trodes tracking</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosV1(dj.Computed):\n    \"\"\"\n    Table to calculate the position based on Trodes tracking\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TrodesPosSelection\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id : varchar(80)\n    orientation_object_id : varchar(80)\n    velocity_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing position for: {key}\")\n        orig_key = copy.deepcopy(key)\n\n        analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n\n        raw_position = RawPosition.PosObject &amp; key\n        spatial_series = raw_position.fetch_nwb()[0][\"raw_position\"]\n        spatial_df = raw_position.fetch1_dataframe()\n\n        position_info_parameters = (TrodesPosParams() &amp; key).fetch1(\"params\")\n        position_info = self.calculate_position_info(\n            spatial_df=spatial_df,\n            meters_to_pixels=spatial_series.conversion,\n            **position_info_parameters,\n        )\n\n        key.update(\n            dict(\n                analysis_file_name=analysis_file_name,\n                **self.generate_pos_components(\n                    spatial_series=spatial_series,\n                    position_info=position_info,\n                    analysis_fname=analysis_file_name,\n                    prefix=\"\",\n                    add_frame_ind=True,\n                    video_frame_ind=getattr(\n                        spatial_df, \"video_frame_ind\", None\n                    ),\n                ),\n            )\n        )\n\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n        self.insert1(key)\n\n        from ..position_merge import PositionOutput\n\n        part_name = to_camel_case(self.table_name.split(\"__\")[-1])\n\n        # TODO: The next line belongs in a merge table function\n        PositionOutput._merge_insert(\n            [orig_key], part_name=part_name, skip_duplicates=True\n        )\n\n    @staticmethod\n    def generate_pos_components(*args, **kwargs):\n        return IntervalPositionInfo.generate_pos_components(*args, **kwargs)\n\n    @staticmethod\n    def calculate_position_info(*args, **kwargs):\n        \"\"\"Calculate position info from 2D spatial series.\"\"\"\n        return IntervalPositionInfo.calculate_position_info(*args, **kwargs)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return IntervalPositionInfo._data_to_df(\n            self.fetch_nwb()[0], prefix=\"\", add_frame_ind=True\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosV1.calculate_position_info", "title": "<code>calculate_position_info(*args, **kwargs)</code>  <code>staticmethod</code>", "text": "<p>Calculate position info from 2D spatial series.</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@staticmethod\ndef calculate_position_info(*args, **kwargs):\n    \"\"\"Calculate position info from 2D spatial series.\"\"\"\n    return IntervalPositionInfo.calculate_position_info(*args, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosVideo", "title": "<code>TrodesPosVideo</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosVideo(dj.Computed):\n    \"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; TrodesPosV1\n    ---\n    has_video : bool\n    \"\"\"\n\n    def make(self, key):\n        M_TO_CM = 100\n\n        print(\"Loading position data...\")\n        raw_position_df = (\n            RawPosition.PosObject\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch1_dataframe()\n        position_info_df = (TrodesPosV1() &amp; key).fetch1_dataframe()\n\n        print(\"Loading video data...\")\n        epoch = (\n            int(\n                key[\"interval_list_name\"]\n                .replace(\"pos \", \"\")\n                .replace(\" valid times\", \"\")\n            )\n            + 1\n        )\n\n        (\n            video_path,\n            video_filename,\n            meters_per_pixel,\n            video_time,\n        ) = get_video_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch}\n        )\n\n        if not video_path:\n            self.insert1(dict(**key, has_video=False))\n            return\n\n        video_dir = os.path.dirname(video_path) + \"/\"\n        video_path = check_videofile(\n            video_path=video_dir, video_filename=video_filename\n        )[0].as_posix()\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        current_dir = Path(os.getcwd())\n        output_video_filename = (\n            f\"{current_dir.as_posix()}/{nwb_base_filename}_\"\n            f\"{epoch:02d}_{key['trodes_pos_params_name']}.mp4\"\n        )\n        centroids = {\n            \"red\": np.asarray(raw_position_df[[\"xloc\", \"yloc\"]]),\n            \"green\": np.asarray(raw_position_df[[\"xloc2\", \"yloc2\"]]),\n        }\n        position_mean = np.asarray(\n            position_info_df[[\"position_x\", \"position_y\"]]\n        )\n        orientation_mean = np.asarray(position_info_df[[\"orientation\"]])\n        position_time = np.asarray(position_info_df.index)\n        cm_per_pixel = meters_per_pixel * M_TO_CM\n\n        print(\"Making video...\")\n        self.make_video(\n            video_path,\n            centroids,\n            position_mean,\n            orientation_mean,\n            video_time,\n            position_time,\n            output_video_filename=output_video_filename,\n            cm_to_pixels=cm_per_pixel,\n            disable_progressbar=False,\n        )\n        self.insert1(dict(**key, has_video=True))\n\n    @staticmethod\n    def convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n        \"\"\"Converts from cm to pixels and flips the y-axis.\n        Parameters\n        ----------\n        data : ndarray, shape (n_time, 2)\n        frame_size : array_like, shape (2,)\n        cm_to_pixels : float\n\n        Returns\n        -------\n        converted_data : ndarray, shape (n_time, 2)\n        \"\"\"\n        return data / cm_to_pixels\n\n    @staticmethod\n    def fill_nan(variable, video_time, variable_time):\n        video_ind = np.digitize(variable_time, video_time[1:])\n\n        n_video_time = len(video_time)\n        try:\n            n_variable_dims = variable.shape[1]\n            filled_variable = np.full((n_video_time, n_variable_dims), np.nan)\n        except IndexError:\n            filled_variable = np.full((n_video_time,), np.nan)\n        filled_variable[video_ind] = variable\n\n        return filled_variable\n\n    def make_video(\n        self,\n        video_filename,\n        centroids,\n        position_mean,\n        orientation_mean,\n        video_time,\n        position_time,\n        output_video_filename=\"output.mp4\",\n        cm_to_pixels=1.0,\n        disable_progressbar=False,\n        arrow_radius=15,\n        circle_radius=8,\n    ):\n        RGB_PINK = (234, 82, 111)\n        RGB_YELLOW = (253, 231, 76)\n        RGB_WHITE = (255, 255, 255)\n\n        video = cv2.VideoCapture(video_filename)\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        frame_size = (int(video.get(3)), int(video.get(4)))\n        frame_rate = video.get(5)\n        n_frames = int(orientation_mean.shape[0])\n        print(f\"video filepath: {output_video_filename}\")\n        out = cv2.VideoWriter(\n            output_video_filename, fourcc, frame_rate, frame_size, True\n        )\n\n        centroids = {\n            color: self.fill_nan(data, video_time, position_time)\n            for color, data in centroids.items()\n        }\n        position_mean = self.fill_nan(position_mean, video_time, position_time)\n        orientation_mean = self.fill_nan(\n            orientation_mean, video_time, position_time\n        )\n\n        for time_ind in tqdm(\n            range(n_frames - 1), desc=\"frames\", disable=disable_progressbar\n        ):\n            is_grabbed, frame = video.read()\n            if is_grabbed:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                red_centroid = centroids[\"red\"][time_ind]\n                green_centroid = centroids[\"green\"][time_ind]\n\n                position = position_mean[time_ind]\n                position = self.convert_to_pixels(\n                    position, frame_size, cm_to_pixels\n                )\n                orientation = orientation_mean[time_ind]\n\n                if np.all(~np.isnan(red_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(red_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_YELLOW,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(green_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(green_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_PINK,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(position)) &amp; np.all(~np.isnan(orientation)):\n                    arrow_tip = (\n                        int(position[0] + arrow_radius * np.cos(orientation)),\n                        int(position[1] + arrow_radius * np.sin(orientation)),\n                    )\n                    cv2.arrowedLine(\n                        img=frame,\n                        pt1=tuple(position.astype(int)),\n                        pt2=arrow_tip,\n                        color=RGB_WHITE,\n                        thickness=4,\n                        line_type=8,\n                        shift=cv2.CV_8U,\n                        tipLength=0.25,\n                    )\n\n                if np.all(~np.isnan(position)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(position.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_WHITE,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n                out.write(frame)\n            else:\n                break\n\n        video.release()\n        out.release()\n        cv2.destroyAllWindows()\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosVideo.convert_to_pixels", "title": "<code>convert_to_pixels(data, frame_size, cm_to_pixels=1.0)</code>  <code>staticmethod</code>", "text": "<p>Converts from cm to pixels and flips the y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>(ndarray, shape(n_time, 2))</code> required <code>frame_size</code> <code>(array_like, shape(2))</code> required <code>cm_to_pixels</code> <code>float</code> <code>1.0</code> <p>Returns:</p> Name Type Description <code>converted_data</code> <code>(ndarray, shape(n_time, 2))</code> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@staticmethod\ndef convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n    \"\"\"Converts from cm to pixels and flips the y-axis.\n    Parameters\n    ----------\n    data : ndarray, shape (n_time, 2)\n    frame_size : array_like, shape (2,)\n    cm_to_pixels : float\n\n    Returns\n    -------\n    converted_data : ndarray, shape (n_time, 2)\n    \"\"\"\n    return data / cm_to_pixels\n</code></pre>"}, {"location": "api/src/spyglass/position_linearization/position_linearization_merge/", "title": "position_linearization_merge.py", "text": ""}, {"location": "api/src/spyglass/position_linearization/v1/linearization/", "title": "linearization.py", "text": ""}, {"location": "api/src/spyglass/position_linearization/v1/linearization/#src.spyglass.position_linearization.v1.linearization.LinearizationParameters", "title": "<code>LinearizationParameters</code>", "text": "<p>             Bases: <code>Lookup</code></p> <p>Choose whether to use an HMM to linearize position. This can help when the eucledian distances between separate arms are too close and the previous position has some information about which arm the animal is on.</p> Source code in <code>src/spyglass/position_linearization/v1/linearization.py</code> <pre><code>@schema\nclass LinearizationParameters(dj.Lookup):\n    \"\"\"Choose whether to use an HMM to linearize position. This can help when\n    the eucledian distances between separate arms are too close and the previous\n    position has some information about which arm the animal is on.\"\"\"\n\n    definition = \"\"\"\n    linearization_param_name : varchar(80)   # name for this set of parameters\n    ---\n    use_hmm = 0 : int   # use HMM to determine linearization\n    # How much to prefer route distances between successive time points that are closer to the euclidean distance. Smaller numbers mean the route distance is more likely to be close to the euclidean distance.\n    route_euclidean_distance_scaling = 1.0 : float\n    sensor_std_dev = 5.0 : float   # Uncertainty of position sensor (in cm).\n    # Biases the transition matrix to prefer the current track segment.\n    diagonal_bias = 0.5 : float\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position_linearization/v1/linearization/#src.spyglass.position_linearization.v1.linearization.TrackGraph", "title": "<code>TrackGraph</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Graph representation of track representing the spatial environment. Used for linearizing position.</p> Source code in <code>src/spyglass/position_linearization/v1/linearization.py</code> <pre><code>@schema\nclass TrackGraph(dj.Manual):\n    \"\"\"Graph representation of track representing the spatial environment.\n    Used for linearizing position.\"\"\"\n\n    definition = \"\"\"\n    track_graph_name : varchar(80)\n    ----\n    environment : varchar(80)    # Type of Environment\n    node_positions : blob  # 2D position of track_graph nodes, shape (n_nodes, 2)\n    edges: blob                  # shape (n_edges, 2)\n    linear_edge_order : blob  # order of track graph edges in the linear space, shape (n_edges, 2)\n    linear_edge_spacing : blob  # amount of space between edges in the linear space, shape (n_edges,)\n    \"\"\"\n\n    def get_networkx_track_graph(self, track_graph_parameters=None):\n        if track_graph_parameters is None:\n            track_graph_parameters = self.fetch1()\n        return make_track_graph(\n            node_positions=track_graph_parameters[\"node_positions\"],\n            edges=track_graph_parameters[\"edges\"],\n        )\n\n    def plot_track_graph(self, ax=None, draw_edge_labels=False, **kwds):\n        \"\"\"Plot the track graph in 2D position space.\"\"\"\n        track_graph = self.get_networkx_track_graph()\n        plot_track_graph(\n            track_graph, ax=ax, draw_edge_labels=draw_edge_labels, **kwds\n        )\n\n    def plot_track_graph_as_1D(\n        self,\n        ax=None,\n        axis=\"x\",\n        other_axis_start=0.0,\n        draw_edge_labels=False,\n        node_size=300,\n        node_color=\"#1f77b4\",\n    ):\n        \"\"\"Plot the track graph in 1D to see how the linearization is set up.\"\"\"\n        track_graph_parameters = self.fetch1()\n        track_graph = self.get_networkx_track_graph(\n            track_graph_parameters=track_graph_parameters\n        )\n        plot_graph_as_1D(\n            track_graph,\n            edge_order=track_graph_parameters[\"linear_edge_order\"],\n            edge_spacing=track_graph_parameters[\"linear_edge_spacing\"],\n            ax=ax,\n            axis=axis,\n            other_axis_start=other_axis_start,\n            draw_edge_labels=draw_edge_labels,\n            node_size=node_size,\n            node_color=node_color,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position_linearization/v1/linearization/#src.spyglass.position_linearization.v1.linearization.TrackGraph.plot_track_graph", "title": "<code>plot_track_graph(ax=None, draw_edge_labels=False, **kwds)</code>", "text": "<p>Plot the track graph in 2D position space.</p> Source code in <code>src/spyglass/position_linearization/v1/linearization.py</code> <pre><code>def plot_track_graph(self, ax=None, draw_edge_labels=False, **kwds):\n    \"\"\"Plot the track graph in 2D position space.\"\"\"\n    track_graph = self.get_networkx_track_graph()\n    plot_track_graph(\n        track_graph, ax=ax, draw_edge_labels=draw_edge_labels, **kwds\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position_linearization/v1/linearization/#src.spyglass.position_linearization.v1.linearization.TrackGraph.plot_track_graph_as_1D", "title": "<code>plot_track_graph_as_1D(ax=None, axis='x', other_axis_start=0.0, draw_edge_labels=False, node_size=300, node_color='#1f77b4')</code>", "text": "<p>Plot the track graph in 1D to see how the linearization is set up.</p> Source code in <code>src/spyglass/position_linearization/v1/linearization.py</code> <pre><code>def plot_track_graph_as_1D(\n    self,\n    ax=None,\n    axis=\"x\",\n    other_axis_start=0.0,\n    draw_edge_labels=False,\n    node_size=300,\n    node_color=\"#1f77b4\",\n):\n    \"\"\"Plot the track graph in 1D to see how the linearization is set up.\"\"\"\n    track_graph_parameters = self.fetch1()\n    track_graph = self.get_networkx_track_graph(\n        track_graph_parameters=track_graph_parameters\n    )\n    plot_graph_as_1D(\n        track_graph,\n        edge_order=track_graph_parameters[\"linear_edge_order\"],\n        edge_spacing=track_graph_parameters[\"linear_edge_spacing\"],\n        ax=ax,\n        axis=axis,\n        other_axis_start=other_axis_start,\n        draw_edge_labels=draw_edge_labels,\n        node_size=node_size,\n        node_color=node_color,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position_linearization/v1/linearization/#src.spyglass.position_linearization.v1.linearization.LinearizedPositionV1", "title": "<code>LinearizedPositionV1</code>", "text": "<p>             Bases: <code>Computed</code></p> <p>Linearized position for a given interval</p> Source code in <code>src/spyglass/position_linearization/v1/linearization.py</code> <pre><code>@schema\nclass LinearizedPositionV1(dj.Computed):\n    \"\"\"Linearized position for a given interval\"\"\"\n\n    definition = \"\"\"\n    -&gt; LinearizationSelection\n    ---\n    -&gt; AnalysisNwbfile\n    linearized_position_object_id : varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        orig_key = copy.deepcopy(key)\n        print(f\"Computing linear position for: {key}\")\n\n        position_nwb = PositionOutput.fetch_nwb(key)[0]\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        position = np.asarray(\n            position_nwb[\"position\"].get_spatial_series().data\n        )\n        time = np.asarray(\n            position_nwb[\"position\"].get_spatial_series().timestamps\n        )\n\n        linearization_parameters = (\n            LinearizationParameters()\n            &amp; {\"linearization_param_name\": key[\"linearization_param_name\"]}\n        ).fetch1()\n        track_graph_info = (\n            TrackGraph() &amp; {\"track_graph_name\": key[\"track_graph_name\"]}\n        ).fetch1()\n\n        track_graph = make_track_graph(\n            node_positions=track_graph_info[\"node_positions\"],\n            edges=track_graph_info[\"edges\"],\n        )\n\n        linear_position_df = get_linearized_position(\n            position=position,\n            track_graph=track_graph,\n            edge_spacing=track_graph_info[\"linear_edge_spacing\"],\n            edge_order=track_graph_info[\"linear_edge_order\"],\n            use_HMM=linearization_parameters[\"use_hmm\"],\n            route_euclidean_distance_scaling=linearization_parameters[\n                \"route_euclidean_distance_scaling\"\n            ],\n            sensor_std_dev=linearization_parameters[\"sensor_std_dev\"],\n            diagonal_bias=linearization_parameters[\"diagonal_bias\"],\n        )\n\n        linear_position_df[\"time\"] = time\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n\n        key[\"linearized_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=linear_position_df,\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n        from ..position_linearization_merge import LinearizedPositionOutput\n\n        part_name = to_camel_case(self.table_name.split(\"__\")[-1])\n\n        LinearizedPositionOutput._merge_insert(\n            [orig_key], part_name=part_name, skip_duplicates=True\n        )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_nwb()[0][\"linearized_position\"].set_index(\"time\")\n</code></pre>"}, {"location": "api/src/spyglass/ripple/v1/ripple/", "title": "ripple.py", "text": ""}, {"location": "api/src/spyglass/ripple/v1/ripple/#src.spyglass.ripple.v1.ripple.RippleLFPSelection", "title": "<code>RippleLFPSelection</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/ripple/v1/ripple.py</code> <pre><code>@schema\nclass RippleLFPSelection(dj.Manual):\n    definition = \"\"\"\n     -&gt; LFPBandV1\n     group_name = 'CA1' : varchar(80)\n     \"\"\"\n\n    class RippleLFPElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; RippleLFPSelection\n        -&gt; LFPBandSelection.LFPBandElectrode\n        \"\"\"\n\n    @staticmethod\n    def validate_key(key):\n        filter_name = (LFPBandV1 &amp; key).fetch1(\"filter_name\")\n        if \"ripple\" not in filter_name.lower():\n            raise UserWarning(\"Please use a ripple filter\")\n\n    @staticmethod\n    def set_lfp_electrodes(\n        key,\n        electrode_list=None,\n        group_name=\"CA1\",\n        **kwargs,\n    ):\n        \"\"\"Removes all electrodes for the specified nwb file and then\n        adds back the electrodes in the list\n\n        Parameters\n        ----------\n        key : dict\n            dictionary corresponding to the LFPBand entry to use for\n            ripple detection\n        electrode_list : list\n            list of electrodes from LFPBandSelection.LFPBandElectrode\n            to be used as the ripple LFP during detection\n        group_name : str, optional\n            description of the electrode group, by default \"CA1\"\n        \"\"\"\n        if electrode_list is None:\n            electrode_list = (\n                (LFPBandSelection.LFPBandElectrode &amp; key)\n                .fetch(\"electrode_id\")\n                .tolist()\n            )\n        electrode_list.sort()\n        try:\n            electrode_keys = (\n                pd.DataFrame(LFPBandSelection.LFPBandElectrode() &amp; key)\n                .set_index(\"electrode_id\")\n                .loc[np.asarray(electrode_list)]\n                .reset_index()\n                .loc[:, LFPBandSelection.LFPBandElectrode.primary_key]\n            )\n        except KeyError as err:\n            print(err)\n            raise KeyError(\n                \"Attempting to use electrode_ids that aren't in the associated\"\n                \" LFPBand filtered dataset.\"\n            ) from err\n        electrode_keys[\"group_name\"] = group_name\n        electrode_keys = electrode_keys.sort_values(by=[\"electrode_id\"])\n        RippleLFPSelection.validate_key(key)\n        RippleLFPSelection().insert1(\n            {**key, \"group_name\": group_name},\n            skip_duplicates=True,\n            **kwargs,\n        )\n        RippleLFPSelection().RippleLFPElectrode.insert(\n            electrode_keys.to_dict(orient=\"records\"),\n            replace=True,\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/ripple/v1/ripple/#src.spyglass.ripple.v1.ripple.RippleLFPSelection.set_lfp_electrodes", "title": "<code>set_lfp_electrodes(key, electrode_list=None, group_name='CA1', **kwargs)</code>  <code>staticmethod</code>", "text": "<p>Removes all electrodes for the specified nwb file and then adds back the electrodes in the list</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary corresponding to the LFPBand entry to use for ripple detection</p> required <code>electrode_list</code> <code>list</code> <p>list of electrodes from LFPBandSelection.LFPBandElectrode to be used as the ripple LFP during detection</p> <code>None</code> <code>group_name</code> <code>str</code> <p>description of the electrode group, by default \"CA1\"</p> <code>'CA1'</code> Source code in <code>src/spyglass/ripple/v1/ripple.py</code> <pre><code>@staticmethod\ndef set_lfp_electrodes(\n    key,\n    electrode_list=None,\n    group_name=\"CA1\",\n    **kwargs,\n):\n    \"\"\"Removes all electrodes for the specified nwb file and then\n    adds back the electrodes in the list\n\n    Parameters\n    ----------\n    key : dict\n        dictionary corresponding to the LFPBand entry to use for\n        ripple detection\n    electrode_list : list\n        list of electrodes from LFPBandSelection.LFPBandElectrode\n        to be used as the ripple LFP during detection\n    group_name : str, optional\n        description of the electrode group, by default \"CA1\"\n    \"\"\"\n    if electrode_list is None:\n        electrode_list = (\n            (LFPBandSelection.LFPBandElectrode &amp; key)\n            .fetch(\"electrode_id\")\n            .tolist()\n        )\n    electrode_list.sort()\n    try:\n        electrode_keys = (\n            pd.DataFrame(LFPBandSelection.LFPBandElectrode() &amp; key)\n            .set_index(\"electrode_id\")\n            .loc[np.asarray(electrode_list)]\n            .reset_index()\n            .loc[:, LFPBandSelection.LFPBandElectrode.primary_key]\n        )\n    except KeyError as err:\n        print(err)\n        raise KeyError(\n            \"Attempting to use electrode_ids that aren't in the associated\"\n            \" LFPBand filtered dataset.\"\n        ) from err\n    electrode_keys[\"group_name\"] = group_name\n    electrode_keys = electrode_keys.sort_values(by=[\"electrode_id\"])\n    RippleLFPSelection.validate_key(key)\n    RippleLFPSelection().insert1(\n        {**key, \"group_name\": group_name},\n        skip_duplicates=True,\n        **kwargs,\n    )\n    RippleLFPSelection().RippleLFPElectrode.insert(\n        electrode_keys.to_dict(orient=\"records\"),\n        replace=True,\n        **kwargs,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/ripple/v1/ripple/#src.spyglass.ripple.v1.ripple.RippleParameters", "title": "<code>RippleParameters</code>", "text": "<p>             Bases: <code>Lookup</code></p> Source code in <code>src/spyglass/ripple/v1/ripple.py</code> <pre><code>@schema\nclass RippleParameters(dj.Lookup):\n    definition = \"\"\"\n    ripple_param_name : varchar(80) # a name for this set of parameters\n    ----\n    ripple_param_dict : BLOB    # dictionary of parameters\n    \"\"\"\n\n    def insert_default(self):\n        \"\"\"Insert the default parameter set\"\"\"\n        default_dict = {\n            \"speed_name\": \"head_speed\",\n            \"ripple_detection_algorithm\": \"Kay_ripple_detector\",\n            \"ripple_detection_params\": dict(\n                speed_threshold=4.0,  # cm/s\n                minimum_duration=0.015,  # sec\n                zscore_threshold=2.0,  # std\n                smoothing_sigma=0.004,  # sec\n                close_ripple_threshold=0.0,  # sec\n            ),\n        }\n        self.insert1(\n            {\"ripple_param_name\": \"default\", \"ripple_param_dict\": default_dict},\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/ripple/v1/ripple/#src.spyglass.ripple.v1.ripple.RippleParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default parameter set</p> Source code in <code>src/spyglass/ripple/v1/ripple.py</code> <pre><code>def insert_default(self):\n    \"\"\"Insert the default parameter set\"\"\"\n    default_dict = {\n        \"speed_name\": \"head_speed\",\n        \"ripple_detection_algorithm\": \"Kay_ripple_detector\",\n        \"ripple_detection_params\": dict(\n            speed_threshold=4.0,  # cm/s\n            minimum_duration=0.015,  # sec\n            zscore_threshold=2.0,  # std\n            smoothing_sigma=0.004,  # sec\n            close_ripple_threshold=0.0,  # sec\n        ),\n    }\n    self.insert1(\n        {\"ripple_param_name\": \"default\", \"ripple_param_dict\": default_dict},\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/ripple/v1/ripple/#src.spyglass.ripple.v1.ripple.RippleTimesV1", "title": "<code>RippleTimesV1</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/ripple/v1/ripple.py</code> <pre><code>@schema\nclass RippleTimesV1(dj.Computed):\n    definition = \"\"\"\n    -&gt; RippleLFPSelection\n    -&gt; RippleParameters\n    -&gt; PositionOutput.proj(pos_merge_id='merge_id')\n\n    ---\n    -&gt; AnalysisNwbfile\n    ripple_times_object_id : varchar(40)\n     \"\"\"\n\n    def make(self, key):\n        nwb_file_name, interval_list_name = (LFPBandV1 &amp; key).fetch1(\n            \"nwb_file_name\", \"target_interval_list_name\"\n        )\n\n        print(f\"Computing ripple times for: {key}\")\n        ripple_params = (\n            RippleParameters &amp; {\"ripple_param_name\": key[\"ripple_param_name\"]}\n        ).fetch1(\"ripple_param_dict\")\n\n        ripple_detection_algorithm = ripple_params[\"ripple_detection_algorithm\"]\n        ripple_detection_params = ripple_params[\"ripple_detection_params\"]\n\n        (\n            speed,\n            interval_ripple_lfps,\n            sampling_frequency,\n        ) = self.get_ripple_lfps_and_position_info(\n            key, nwb_file_name, interval_list_name\n        )\n        ripple_times = RIPPLE_DETECTION_ALGORITHMS[ripple_detection_algorithm](\n            time=np.asarray(interval_ripple_lfps.index),\n            filtered_lfps=np.asarray(interval_ripple_lfps),\n            speed=np.asarray(speed),\n            sampling_frequency=sampling_frequency,\n            **ripple_detection_params,\n        )\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(nwb_file_name)\n        key[\"ripple_times_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=ripple_times,\n        )\n        nwb_analysis_file.add(\n            nwb_file_name=nwb_file_name,\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        \"\"\"Convenience function for returning the marks in a readable format\"\"\"\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [data[\"ripple_times\"] for data in self.fetch_nwb()]\n\n    @staticmethod\n    def get_ripple_lfps_and_position_info(\n        key, nwb_file_name, interval_list_name\n    ):\n        ripple_params = (\n            RippleParameters &amp; {\"ripple_param_name\": key[\"ripple_param_name\"]}\n        ).fetch1(\"ripple_param_dict\")\n\n        speed_name = ripple_params[\"speed_name\"]\n\n        electrode_keys = (RippleLFPSelection.RippleLFPElectrode() &amp; key).fetch(\n            \"electrode_id\"\n        )\n\n        # warn/validate that there is only one wire per electrode\n        ripple_lfp_nwb = (LFPBandV1 &amp; key).fetch_nwb()[0]\n        ripple_lfp_electrodes = ripple_lfp_nwb[\"lfp_band\"].electrodes.data[:]\n        elec_mask = np.full_like(ripple_lfp_electrodes, 0, dtype=bool)\n        valid_elecs = [\n            elec for elec in electrode_keys if elec in ripple_lfp_electrodes\n        ]\n        lfp_indexed_elec_ids = get_electrode_indices(\n            ripple_lfp_nwb[\"lfp_band\"], valid_elecs\n        )\n        elec_mask[lfp_indexed_elec_ids] = True\n        ripple_lfp = pd.DataFrame(\n            ripple_lfp_nwb[\"lfp_band\"].data,\n            index=pd.Index(ripple_lfp_nwb[\"lfp_band\"].timestamps, name=\"time\"),\n        )\n        sampling_frequency = ripple_lfp_nwb[\"lfp_band_sampling_rate\"]\n\n        ripple_lfp = ripple_lfp.loc[:, elec_mask]\n\n        position_valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        position_info = (\n            PositionOutput() &amp; {\"merge_id\": key[\"pos_merge_id\"]}\n        ).fetch1_dataframe()\n\n        # restrict valid times to position time\n        valid_times_interval = np.array(\n            [position_info.index[0], position_info.index[-1]]\n        )\n        position_valid_times = interval_list_intersect(\n            position_valid_times, valid_times_interval\n        )\n\n        position_info = pd.concat(\n            [\n                position_info.loc[slice(valid_time[0], valid_time[1])]\n                for valid_time in position_valid_times\n            ],\n            axis=0,\n        )\n        interval_ripple_lfps = pd.concat(\n            [\n                ripple_lfp.loc[slice(valid_time[0], valid_time[1])]\n                for valid_time in position_valid_times\n            ],\n            axis=0,\n        )\n\n        position_info = interpolate_to_new_time(\n            position_info, interval_ripple_lfps.index\n        )\n\n        return (\n            position_info[speed_name],\n            interval_ripple_lfps,\n            sampling_frequency,\n        )\n\n    @staticmethod\n    def get_Kay_ripple_consensus_trace(\n        ripple_filtered_lfps, sampling_frequency, smoothing_sigma=0.004\n    ):\n        ripple_consensus_trace = np.full_like(ripple_filtered_lfps, np.nan)\n        not_null = np.all(pd.notnull(ripple_filtered_lfps), axis=1)\n\n        ripple_consensus_trace[not_null] = get_envelope(\n            np.asarray(ripple_filtered_lfps)[not_null]\n        )\n        ripple_consensus_trace = np.sum(ripple_consensus_trace**2, axis=1)\n        ripple_consensus_trace[not_null] = gaussian_smooth(\n            ripple_consensus_trace[not_null],\n            smoothing_sigma,\n            sampling_frequency,\n        )\n        return pd.DataFrame(\n            np.sqrt(ripple_consensus_trace), index=ripple_filtered_lfps.index\n        )\n\n    @staticmethod\n    def plot_ripple_consensus_trace(\n        ripple_consensus_trace,\n        ripple_times,\n        ripple_label=1,\n        offset=0.100,\n        relative=True,\n        ax=None,\n    ):\n        ripple_start = ripple_times.loc[ripple_label].start_time\n        ripple_end = ripple_times.loc[ripple_label].end_time\n        time_slice = slice(ripple_start - offset, ripple_end + offset)\n\n        start_offset = ripple_start if relative else 0\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(12, 1))\n        ax.plot(\n            ripple_consensus_trace.loc[time_slice].index - start_offset,\n            ripple_consensus_trace.loc[time_slice],\n        )\n        ax.axvspan(\n            ripple_start - start_offset,\n            ripple_end - start_offset,\n            zorder=-1,\n            alpha=0.5,\n            color=\"lightgrey\",\n        )\n        ax.set_xlabel(\"Time [s]\")\n        ax.set_xlim(\n            (time_slice.start - start_offset, time_slice.stop - start_offset)\n        )\n\n    @staticmethod\n    def plot_ripple(\n        lfps, ripple_times, ripple_label=1, offset=0.100, relative=True, ax=None\n    ):\n        lfp_labels = lfps.columns\n        n_lfps = len(lfp_labels)\n        ripple_start = ripple_times.loc[ripple_label].start_time\n        ripple_end = ripple_times.loc[ripple_label].end_time\n        time_slice = slice(ripple_start - offset, ripple_end + offset)\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(12, n_lfps * 0.20))\n\n        start_offset = ripple_start if relative else 0\n\n        for lfp_ind, lfp_label in enumerate(lfp_labels):\n            lfp = lfps.loc[time_slice, lfp_label]\n            ax.plot(\n                lfp.index - start_offset,\n                lfp_ind + (lfp - lfp.mean()) / (lfp.max() - lfp.min()),\n                color=\"black\",\n            )\n\n        ax.axvspan(\n            ripple_start - start_offset,\n            ripple_end - start_offset,\n            zorder=-1,\n            alpha=0.5,\n            color=\"lightgrey\",\n        )\n        ax.set_ylim((-1, n_lfps))\n        ax.set_xlim(\n            (time_slice.start - start_offset, time_slice.stop - start_offset)\n        )\n        ax.set_ylabel(\"LFPs\")\n        ax.set_xlabel(\"Time [s]\")\n</code></pre>"}, {"location": "api/src/spyglass/ripple/v1/ripple/#src.spyglass.ripple.v1.ripple.RippleTimesV1.fetch1_dataframe", "title": "<code>fetch1_dataframe()</code>", "text": "<p>Convenience function for returning the marks in a readable format</p> Source code in <code>src/spyglass/ripple/v1/ripple.py</code> <pre><code>def fetch1_dataframe(self):\n    \"\"\"Convenience function for returning the marks in a readable format\"\"\"\n    return self.fetch_dataframe()[0]\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/", "title": "sharing_kachery.py", "text": ""}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.kachery_download_file", "title": "<code>kachery_download_file(uri, dest, kachery_zone_name)</code>", "text": "<p>set the kachery resource url and attempt to down load the uri into the destination path</p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>def kachery_download_file(uri: str, dest: str, kachery_zone_name: str) -&gt; str:\n    \"\"\"set the kachery resource url and attempt to down load the uri into the destination path\"\"\"\n    KacheryZone.set_resource_url({\"kachery_zone_name\": kachery_zone_name})\n    return kcl.load_file(uri, dest=dest)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone", "title": "<code>KacheryZone</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@schema\nclass KacheryZone(dj.Manual):\n    definition = \"\"\"\n    kachery_zone_name: varchar(200) # the name of the kachery zone. Note that this is the same as the name of the kachery resource.\n    ---\n    description: varchar(200) # description of this zone\n    kachery_cloud_dir: varchar(200) # kachery cloud directory on local machine where files are linked\n    kachery_proxy: varchar(200) # kachery sharing proxy\n    -&gt; Lab\n    \"\"\"\n\n    @staticmethod\n    def set_zone(key: dict):\n        \"\"\"Set the kachery zone based on the key to KacheryZone\n\n        Parameters\n        ----------\n        key : dict\n            key defining a single KacheryZone\n\n        \"\"\"\n        try:\n            kachery_zone_name, kachery_cloud_dir = (KacheryZone &amp; key).fetch1(\n                \"kachery_zone_name\", \"kachery_cloud_dir\"\n            )\n        except:\n            raise Exception(\n                f\"{key} does not correspond to a single entry in KacheryZone.\"\n            )\n            return None\n        # set the new zone and cloud directory\n        os.environ[kachery_zone_envar] = kachery_zone_name\n        os.environ[kachery_cloud_dir_envar] = kachery_cloud_dir\n\n    @staticmethod\n    def reset_zone():\n        \"\"\"Resets the kachery zone environment variable to the default values.\"\"\"\n        if default_kachery_zone is not None:\n            os.environ[kachery_zone_envar] = default_kachery_zone\n        if default_kachery_cloud_dir is not None:\n            os.environ[kachery_cloud_dir_envar] = default_kachery_cloud_dir\n\n    @staticmethod\n    def set_resource_url(key: dict):\n        \"\"\"Sets the KACHERY_RESOURCE_URL based on the key corresponding to a single Kachery Zone\n\n        Parameters\n        ----------\n        key : dict\n            key to retrieve a single kachery zone\n        \"\"\"\n        try:\n            kachery_zone_name, kachery_proxy = (KacheryZone &amp; key).fetch1(\n                \"kachery_zone_name\", \"kachery_proxy\"\n            )\n        except:\n            raise Exception(\n                f\"{key} does not correspond to a single entry in KacheryZone.\"\n            )\n        # set the new zone and cloud directory\n        os.environ[kachery_zone_envar] = kachery_zone_name\n        os.environ[kachery_resource_url_envar] = (\n            kachery_proxy + \"/r/\" + kachery_zone_name\n        )\n\n    @staticmethod\n    def reset_resource_url():\n        KacheryZone.reset_zone()\n        if default_kachery_resource_url is not None:\n            os.environ[\n                kachery_resource_url_envar\n            ] = default_kachery_resource_url\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone.set_zone", "title": "<code>set_zone(key)</code>  <code>staticmethod</code>", "text": "<p>Set the kachery zone based on the key to KacheryZone</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key defining a single KacheryZone</p> required Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef set_zone(key: dict):\n    \"\"\"Set the kachery zone based on the key to KacheryZone\n\n    Parameters\n    ----------\n    key : dict\n        key defining a single KacheryZone\n\n    \"\"\"\n    try:\n        kachery_zone_name, kachery_cloud_dir = (KacheryZone &amp; key).fetch1(\n            \"kachery_zone_name\", \"kachery_cloud_dir\"\n        )\n    except:\n        raise Exception(\n            f\"{key} does not correspond to a single entry in KacheryZone.\"\n        )\n        return None\n    # set the new zone and cloud directory\n    os.environ[kachery_zone_envar] = kachery_zone_name\n    os.environ[kachery_cloud_dir_envar] = kachery_cloud_dir\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone.reset_zone", "title": "<code>reset_zone()</code>  <code>staticmethod</code>", "text": "<p>Resets the kachery zone environment variable to the default values.</p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef reset_zone():\n    \"\"\"Resets the kachery zone environment variable to the default values.\"\"\"\n    if default_kachery_zone is not None:\n        os.environ[kachery_zone_envar] = default_kachery_zone\n    if default_kachery_cloud_dir is not None:\n        os.environ[kachery_cloud_dir_envar] = default_kachery_cloud_dir\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone.set_resource_url", "title": "<code>set_resource_url(key)</code>  <code>staticmethod</code>", "text": "<p>Sets the KACHERY_RESOURCE_URL based on the key corresponding to a single Kachery Zone</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key to retrieve a single kachery zone</p> required Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef set_resource_url(key: dict):\n    \"\"\"Sets the KACHERY_RESOURCE_URL based on the key corresponding to a single Kachery Zone\n\n    Parameters\n    ----------\n    key : dict\n        key to retrieve a single kachery zone\n    \"\"\"\n    try:\n        kachery_zone_name, kachery_proxy = (KacheryZone &amp; key).fetch1(\n            \"kachery_zone_name\", \"kachery_proxy\"\n        )\n    except:\n        raise Exception(\n            f\"{key} does not correspond to a single entry in KacheryZone.\"\n        )\n    # set the new zone and cloud directory\n    os.environ[kachery_zone_envar] = kachery_zone_name\n    os.environ[kachery_resource_url_envar] = (\n        kachery_proxy + \"/r/\" + kachery_zone_name\n    )\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.AnalysisNwbfileKachery", "title": "<code>AnalysisNwbfileKachery</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@schema\nclass AnalysisNwbfileKachery(dj.Computed):\n    definition = \"\"\"\n    -&gt; AnalysisNwbfileKacherySelection\n    ---\n    analysis_file_uri='': varchar(200)  # the uri of the file\n    \"\"\"\n\n    class LinkedFile(dj.Part):\n        definition = \"\"\"\n        -&gt; AnalysisNwbfileKachery\n        linked_file_rel_path: varchar(200) # the path for the linked file relative to the SPYGLASS_BASE_DIR environment variable\n        ---\n        linked_file_uri='': varchar(200) # the uri for the linked file\n        \"\"\"\n\n    def make(self, key):\n        # note that we're assuming that the user has initialized a kachery-cloud\n        # client with kachery-cloud-init. Uncomment the line below once we are\n        # sharing linked files as well.\n\n        # linked_key = copy.deepcopy(key)\n\n        print(f'Linking {key[\"analysis_file_name\"]} in kachery-cloud...')\n        # set the kachery zone\n\n        KacheryZone.set_zone(key)\n\n        key[\"analysis_file_uri\"] = kcl.link_file(\n            AnalysisNwbfile().get_abs_path(key[\"analysis_file_name\"])\n        )\n        print(\n            os.environ[kachery_zone_envar], os.environ[kachery_cloud_dir_envar]\n        )\n        print(AnalysisNwbfile().get_abs_path(key[\"analysis_file_name\"]))\n        print(kcl.load_file(key[\"analysis_file_uri\"]))\n        self.insert1(key)\n\n        # we also need to insert any linked files\n        # TODO: change this to automatically detect all linked files\n        # self.LinkedFile.insert1(key)\n\n        # reset the Kachery zone and cloud_dir to the defaults\n        KacheryZone.reset_zone()\n\n    @staticmethod\n    def download_file(analysis_file_name: str) -&gt; bool:\n        \"\"\"Download the specified analysis file and associated linked files from kachery-cloud if possible\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis file\n\n        Returns\n        ----------\n        is_success : bool\n            True if the file was successfully downloaded, False otherwise\n        \"\"\"\n        fetched_list = (\n            AnalysisNwbfileKachery &amp; {\"analysis_file_name\": analysis_file_name}\n        ).fetch(\"analysis_file_uri\", \"kachery_zone_name\")\n        downloaded = False\n        for uri, kachery_zone_name in zip(fetched_list[0], fetched_list[1]):\n            if len(uri) == 0:\n                return False\n            print(\"uri:\", uri)\n            if kachery_download_file(\n                uri=uri,\n                dest=AnalysisNwbfile.get_abs_path(analysis_file_name),\n                kachery_zone_name=kachery_zone_name,\n            ):\n                downloaded = True\n                # now download the linked file(s)\n                linked_files = (\n                    AnalysisNwbfileKachery.LinkedFile\n                    &amp; {\"analysis_file_name\": analysis_file_name}\n                ).fetch(as_dict=True)\n                for file in linked_files:\n                    uri = file[\"linked_file_uri\"]\n                    print(f\"attempting to download linked file uri {uri}\")\n                    linked_file_path = (\n                        os.environ[\"SPYGLASS_BASE_DIR\"]\n                        + file[\"linked_file_rel_path\"]\n                    )\n                    if not kachery_download_file(\n                        uri=uri,\n                        dest=linked_file_path,\n                        kachery_zone_name=kachery_zone_name,\n                    ):\n                        raise Exception(\n                            f\"Linked file {linked_file_path} cannot be downloaded\"\n                        )\n        if not downloaded:\n            raise Exception(f\"{analysis_file_name} cannot be downloaded\")\n\n        return True\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.AnalysisNwbfileKachery.download_file", "title": "<code>download_file(analysis_file_name)</code>  <code>staticmethod</code>", "text": "<p>Download the specified analysis file and associated linked files from kachery-cloud if possible</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis file</p> required <p>Returns:</p> Name Type Description <code>is_success</code> <code>bool</code> <p>True if the file was successfully downloaded, False otherwise</p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef download_file(analysis_file_name: str) -&gt; bool:\n    \"\"\"Download the specified analysis file and associated linked files from kachery-cloud if possible\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis file\n\n    Returns\n    ----------\n    is_success : bool\n        True if the file was successfully downloaded, False otherwise\n    \"\"\"\n    fetched_list = (\n        AnalysisNwbfileKachery &amp; {\"analysis_file_name\": analysis_file_name}\n    ).fetch(\"analysis_file_uri\", \"kachery_zone_name\")\n    downloaded = False\n    for uri, kachery_zone_name in zip(fetched_list[0], fetched_list[1]):\n        if len(uri) == 0:\n            return False\n        print(\"uri:\", uri)\n        if kachery_download_file(\n            uri=uri,\n            dest=AnalysisNwbfile.get_abs_path(analysis_file_name),\n            kachery_zone_name=kachery_zone_name,\n        ):\n            downloaded = True\n            # now download the linked file(s)\n            linked_files = (\n                AnalysisNwbfileKachery.LinkedFile\n                &amp; {\"analysis_file_name\": analysis_file_name}\n            ).fetch(as_dict=True)\n            for file in linked_files:\n                uri = file[\"linked_file_uri\"]\n                print(f\"attempting to download linked file uri {uri}\")\n                linked_file_path = (\n                    os.environ[\"SPYGLASS_BASE_DIR\"]\n                    + file[\"linked_file_rel_path\"]\n                )\n                if not kachery_download_file(\n                    uri=uri,\n                    dest=linked_file_path,\n                    kachery_zone_name=kachery_zone_name,\n                ):\n                    raise Exception(\n                        f\"Linked file {linked_file_path} cannot be downloaded\"\n                    )\n    if not downloaded:\n        raise Exception(f\"{analysis_file_name} cannot be downloaded\")\n\n    return True\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/", "title": "curation_figurl.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.curation_figurl.CurationFigurl", "title": "<code>CurationFigurl</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/spikesorting/curation_figurl.py</code> <pre><code>@schema\nclass CurationFigurl(dj.Computed):\n    definition = \"\"\"\n    -&gt; CurationFigurlSelection\n    ---\n    url: varchar(2000)\n    initial_curation_uri: varchar(2000)\n    new_curation_uri: varchar(2000)\n    \"\"\"\n\n    def make(self, key: dict):\n        \"\"\"Create a Curation Figurl\n        Parameters\n        ----------\n        key : dict\n            primary key of an entry from CurationFigurlSelection table\n        \"\"\"\n\n        # get new_curation_uri from selection table\n        new_curation_uri = (CurationFigurlSelection &amp; key).fetch1(\n            \"new_curation_uri\"\n        )\n\n        # fetch\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        recording_label = SpikeSortingRecording._get_recording_name(key)\n        sorting_label = SpikeSorting._get_sorting_name(key)\n        unit_metrics = _reformat_metrics(\n            (Curation &amp; key).fetch1(\"quality_metrics\")\n        )\n        initial_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n        initial_merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n\n        # new_curation_uri = key[\"new_curation_uri\"]\n\n        # Create the initial curation and store it in kachery\n        for k, v in initial_labels.items():\n            new_list = []\n            for item in v:\n                if item not in new_list:\n                    new_list.append(item)\n            initial_labels[k] = new_list\n        initial_curation = {\n            \"labelsByUnit\": initial_labels,\n            \"mergeGroups\": initial_merge_groups,\n        }\n        initial_curation_uri = kcl.store_json(initial_curation)\n\n        # Get the recording/sorting extractors\n        R = si.load_extractor(recording_path)\n        if R.get_num_segments() &gt; 1:\n            R = si.concatenate_recordings([R])\n        S = si.load_extractor(sorting_path)\n\n        # Generate the figURL\n        url = _generate_the_figurl(\n            R=R,\n            S=S,\n            initial_curation_uri=initial_curation_uri,\n            new_curation_uri=new_curation_uri,\n            recording_label=recording_label,\n            sorting_label=sorting_label,\n            unit_metrics=unit_metrics,\n        )\n\n        # insert\n        key[\"url\"] = url\n        key[\"initial_curation_uri\"] = initial_curation_uri\n        key[\"new_curation_uri\"] = new_curation_uri\n        self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.curation_figurl.CurationFigurl.make", "title": "<code>make(key)</code>", "text": "<p>Create a Curation Figurl</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>primary key of an entry from CurationFigurlSelection table</p> required Source code in <code>src/spyglass/spikesorting/curation_figurl.py</code> <pre><code>def make(self, key: dict):\n    \"\"\"Create a Curation Figurl\n    Parameters\n    ----------\n    key : dict\n        primary key of an entry from CurationFigurlSelection table\n    \"\"\"\n\n    # get new_curation_uri from selection table\n    new_curation_uri = (CurationFigurlSelection &amp; key).fetch1(\n        \"new_curation_uri\"\n    )\n\n    # fetch\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    recording_label = SpikeSortingRecording._get_recording_name(key)\n    sorting_label = SpikeSorting._get_sorting_name(key)\n    unit_metrics = _reformat_metrics(\n        (Curation &amp; key).fetch1(\"quality_metrics\")\n    )\n    initial_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n    initial_merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n\n    # new_curation_uri = key[\"new_curation_uri\"]\n\n    # Create the initial curation and store it in kachery\n    for k, v in initial_labels.items():\n        new_list = []\n        for item in v:\n            if item not in new_list:\n                new_list.append(item)\n        initial_labels[k] = new_list\n    initial_curation = {\n        \"labelsByUnit\": initial_labels,\n        \"mergeGroups\": initial_merge_groups,\n    }\n    initial_curation_uri = kcl.store_json(initial_curation)\n\n    # Get the recording/sorting extractors\n    R = si.load_extractor(recording_path)\n    if R.get_num_segments() &gt; 1:\n        R = si.concatenate_recordings([R])\n    S = si.load_extractor(sorting_path)\n\n    # Generate the figURL\n    url = _generate_the_figurl(\n        R=R,\n        S=S,\n        initial_curation_uri=initial_curation_uri,\n        new_curation_uri=new_curation_uri,\n        recording_label=recording_label,\n        sorting_label=sorting_label,\n        unit_metrics=unit_metrics,\n    )\n\n    # insert\n    key[\"url\"] = url\n    key[\"initial_curation_uri\"] = initial_curation_uri\n    key[\"new_curation_uri\"] = new_curation_uri\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/merged_sorting_extractor/", "title": "merged_sorting_extractor.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/sortingview/", "title": "sortingview.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace", "title": "<code>SortingviewWorkspace</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>@schema\nclass SortingviewWorkspace(dj.Computed):\n    definition = \"\"\"\n    -&gt; SortingviewWorkspaceSelection\n    ---\n    workspace_uri: varchar(1000)\n    sortingview_recording_id: varchar(30)\n    sortingview_sorting_id: varchar(30)\n    channel = NULL : varchar(80)        # the name of kachery channel for data sharing (for kachery daemon, deprecated)\n    \"\"\"\n\n    # make class for parts table to hold URLs\n    class URL(dj.Part):\n        # Table for holding URLs\n        definition = \"\"\"\n        -&gt; SortingviewWorkspace\n        ---\n        curation_url: varchar(1000)   # URL with sortingview data\n        curation_jot: varchar(200)   # URI for saving manual curation tags\n        \"\"\"\n\n    def make(self, key: dict):\n        \"\"\"Create a Sortingview workspace\n\n        Parameters\n        ----------\n        key : dict\n            primary key of an entry from SortingviewWorkspaceSelection table\n        \"\"\"\n\n        # fetch\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n        workspace_label = SpikeSortingRecording._get_recording_name(key)\n        recording_label = SpikeSortingRecording._get_recording_name(key)\n        sorting_label = SpikeSorting._get_sorting_name(key)\n        metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n        curation_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n        team_name = (SpikeSortingRecording &amp; key).fetch1()[\"team_name\"]\n        team_members = (LabTeam.LabTeamMember &amp; {\"team_name\": team_name}).fetch(\n            \"lab_member_name\"\n        )\n        google_user_ids = []\n        for team_member in team_members:\n            google_user_id = (\n                LabMember.LabMemberInfo &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if len(google_user_id) != 1:\n                print(\n                    f\"Google user ID for {team_member} does not exist or more than one ID detected;\\\n                        permission not given to {team_member}, skipping...\"\n                )\n                continue\n            google_user_ids.append(google_user_id[0])\n\n        # do\n        (\n            workspace_uri,\n            recording_id,\n            sorting_id,\n        ) = _create_spikesortingview_workspace(\n            recording_path=recording_path,\n            sorting_path=sorting_path,\n            merge_groups=merge_groups,\n            workspace_label=workspace_label,\n            recording_label=recording_label,\n            sorting_label=sorting_label,\n            metrics=metrics,\n            curation_labels=curation_labels,\n            google_user_ids=google_user_ids,\n        )\n\n        # insert\n        key[\"workspace_uri\"] = workspace_uri\n        key[\"sortingview_recording_id\"] = recording_id\n        key[\"sortingview_sorting_id\"] = sorting_id\n        self.insert1(key)\n\n        # insert URLs\n        # remove non-primary keys\n        del key[\"workspace_uri\"]\n        del key[\"sortingview_recording_id\"]\n        del key[\"sortingview_sorting_id\"]\n\n        # generate URLs and add to key\n        url = self.url_trythis(key)\n        # url = _generate_url(key)\n        # print(\"URL:\", url)\n        key[\"curation_url\"] = url\n        key[\"curation_jot\"] = \"not ready yet\"\n\n        SortingviewWorkspace.URL.insert1(key)\n\n    def remove_sorting_from_workspace(self, key):\n        return NotImplementedError\n\n    def url_trythis(self, key: dict, sortingview_sorting_id: str = None):\n        \"\"\"Generate a URL for visualizing and curating a sorting on the web.\n        Will print instructions on how to do the curation.\n\n        Parameters\n        ----------\n        key : dict\n            An entry from SortingviewWorkspace table\n        sortingview_sorting_id : str, optional\n            sortingview sorting ID to visualize. If None then chooses the first one\n\n        Returns\n        -------\n        url : str\n        \"\"\"\n        workspace_uri = (self &amp; key).fetch1(\"workspace_uri\")\n        workspace = sv.load_workspace(workspace_uri)\n        recording_id = workspace.recording_ids[0]\n        if sortingview_sorting_id is None:\n            sortingview_sorting_id = workspace.sorting_ids[0]\n\n        R = workspace.get_recording_extractor(recording_id)\n        S = workspace.get_sorting_extractor(sortingview_sorting_id)\n\n        initial_labels = (Curation &amp; key).fetch(\"curation_labels\")[0]\n        for k, v in initial_labels.items():\n            new_list = []\n            for item in v:\n                if item not in new_list:\n                    new_list.append(item)\n            initial_labels[k] = new_list\n        initial_curation = {\"labelsByUnit\": initial_labels}\n\n        # custom metrics\n        unit_metrics = workspace.get_unit_metrics_for_sorting(\n            sortingview_sorting_id\n        )\n\n        # This will print some instructions on how to do the curation\n        # old: sv.trythis_start_sorting_curation\n        url = _generate_url(\n            recording=R,\n            sorting=S,\n            label=workspace.label,\n            initial_curation=initial_curation,\n            raster_plot_subsample_max_firing_rate=50,\n            spike_amplitudes_subsample_max_firing_rate=50,\n            unit_metrics=unit_metrics,\n        )\n        return url\n\n    def insert_manual_curation(\n        self, key: dict, url: str, description=\"manually curated\"\n    ):\n        \"\"\"Based on information in key for an SortingviewWorkspace, loads the\n        curated sorting from sortingview, saves it (with labels and the\n        optional description) and inserts it to CuratedSorting\n\n        Assumes that the workspace corresponding to the recording and (original) sorting exists\n\n        Parameters\n        ----------\n        key : dict\n            primary key of AutomaticCuration\n        description: str, optional\n            description of curated sorting\n        \"\"\"\n\n        # get the labels and remove the non-primary merged units\n        # labels = workspace.get_sorting_curation(sorting_id=sortingview_sorting_id)\n        # labels = sv.trythis_load_sorting_curation('jot://xTzzyDieQPkW')\n        labels = sv.trythis_load_sorting_curation(url)\n\n        # turn labels to list of str, only including accepted units.\n        # if bool(labels[\"mergeGroups\"]):\n        if bool(labels.get(\"mergeGroups\", [])):\n            # clusters were merged, so we empty out metrics\n            metrics = {}\n        else:\n            # get the metrics from the parent curation\n            metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n\n        # insert this curation into the  Table\n        return Curation.insert_curation(\n            key,\n            parent_curation_id=key[\"curation_id\"],\n            labels=labels[\"labelsByUnit\"],\n            merge_groups=labels.get(\"mergeGroups\", []),\n            metrics=metrics,\n            description=description,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace.make", "title": "<code>make(key)</code>", "text": "<p>Create a Sortingview workspace</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>primary key of an entry from SortingviewWorkspaceSelection table</p> required Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>def make(self, key: dict):\n    \"\"\"Create a Sortingview workspace\n\n    Parameters\n    ----------\n    key : dict\n        primary key of an entry from SortingviewWorkspaceSelection table\n    \"\"\"\n\n    # fetch\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n    workspace_label = SpikeSortingRecording._get_recording_name(key)\n    recording_label = SpikeSortingRecording._get_recording_name(key)\n    sorting_label = SpikeSorting._get_sorting_name(key)\n    metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n    curation_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n    team_name = (SpikeSortingRecording &amp; key).fetch1()[\"team_name\"]\n    team_members = (LabTeam.LabTeamMember &amp; {\"team_name\": team_name}).fetch(\n        \"lab_member_name\"\n    )\n    google_user_ids = []\n    for team_member in team_members:\n        google_user_id = (\n            LabMember.LabMemberInfo &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if len(google_user_id) != 1:\n            print(\n                f\"Google user ID for {team_member} does not exist or more than one ID detected;\\\n                    permission not given to {team_member}, skipping...\"\n            )\n            continue\n        google_user_ids.append(google_user_id[0])\n\n    # do\n    (\n        workspace_uri,\n        recording_id,\n        sorting_id,\n    ) = _create_spikesortingview_workspace(\n        recording_path=recording_path,\n        sorting_path=sorting_path,\n        merge_groups=merge_groups,\n        workspace_label=workspace_label,\n        recording_label=recording_label,\n        sorting_label=sorting_label,\n        metrics=metrics,\n        curation_labels=curation_labels,\n        google_user_ids=google_user_ids,\n    )\n\n    # insert\n    key[\"workspace_uri\"] = workspace_uri\n    key[\"sortingview_recording_id\"] = recording_id\n    key[\"sortingview_sorting_id\"] = sorting_id\n    self.insert1(key)\n\n    # insert URLs\n    # remove non-primary keys\n    del key[\"workspace_uri\"]\n    del key[\"sortingview_recording_id\"]\n    del key[\"sortingview_sorting_id\"]\n\n    # generate URLs and add to key\n    url = self.url_trythis(key)\n    # url = _generate_url(key)\n    # print(\"URL:\", url)\n    key[\"curation_url\"] = url\n    key[\"curation_jot\"] = \"not ready yet\"\n\n    SortingviewWorkspace.URL.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace.url_trythis", "title": "<code>url_trythis(key, sortingview_sorting_id=None)</code>", "text": "<p>Generate a URL for visualizing and curating a sorting on the web. Will print instructions on how to do the curation.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>An entry from SortingviewWorkspace table</p> required <code>sortingview_sorting_id</code> <code>str</code> <p>sortingview sorting ID to visualize. If None then chooses the first one</p> <code>None</code> <p>Returns:</p> Name Type Description <code>url</code> <code>str</code> Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>def url_trythis(self, key: dict, sortingview_sorting_id: str = None):\n    \"\"\"Generate a URL for visualizing and curating a sorting on the web.\n    Will print instructions on how to do the curation.\n\n    Parameters\n    ----------\n    key : dict\n        An entry from SortingviewWorkspace table\n    sortingview_sorting_id : str, optional\n        sortingview sorting ID to visualize. If None then chooses the first one\n\n    Returns\n    -------\n    url : str\n    \"\"\"\n    workspace_uri = (self &amp; key).fetch1(\"workspace_uri\")\n    workspace = sv.load_workspace(workspace_uri)\n    recording_id = workspace.recording_ids[0]\n    if sortingview_sorting_id is None:\n        sortingview_sorting_id = workspace.sorting_ids[0]\n\n    R = workspace.get_recording_extractor(recording_id)\n    S = workspace.get_sorting_extractor(sortingview_sorting_id)\n\n    initial_labels = (Curation &amp; key).fetch(\"curation_labels\")[0]\n    for k, v in initial_labels.items():\n        new_list = []\n        for item in v:\n            if item not in new_list:\n                new_list.append(item)\n        initial_labels[k] = new_list\n    initial_curation = {\"labelsByUnit\": initial_labels}\n\n    # custom metrics\n    unit_metrics = workspace.get_unit_metrics_for_sorting(\n        sortingview_sorting_id\n    )\n\n    # This will print some instructions on how to do the curation\n    # old: sv.trythis_start_sorting_curation\n    url = _generate_url(\n        recording=R,\n        sorting=S,\n        label=workspace.label,\n        initial_curation=initial_curation,\n        raster_plot_subsample_max_firing_rate=50,\n        spike_amplitudes_subsample_max_firing_rate=50,\n        unit_metrics=unit_metrics,\n    )\n    return url\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace.insert_manual_curation", "title": "<code>insert_manual_curation(key, url, description='manually curated')</code>", "text": "<p>Based on information in key for an SortingviewWorkspace, loads the curated sorting from sortingview, saves it (with labels and the optional description) and inserts it to CuratedSorting</p> <p>Assumes that the workspace corresponding to the recording and (original) sorting exists</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>primary key of AutomaticCuration</p> required <code>description</code> <p>description of curated sorting</p> <code>'manually curated'</code> Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>def insert_manual_curation(\n    self, key: dict, url: str, description=\"manually curated\"\n):\n    \"\"\"Based on information in key for an SortingviewWorkspace, loads the\n    curated sorting from sortingview, saves it (with labels and the\n    optional description) and inserts it to CuratedSorting\n\n    Assumes that the workspace corresponding to the recording and (original) sorting exists\n\n    Parameters\n    ----------\n    key : dict\n        primary key of AutomaticCuration\n    description: str, optional\n        description of curated sorting\n    \"\"\"\n\n    # get the labels and remove the non-primary merged units\n    # labels = workspace.get_sorting_curation(sorting_id=sortingview_sorting_id)\n    # labels = sv.trythis_load_sorting_curation('jot://xTzzyDieQPkW')\n    labels = sv.trythis_load_sorting_curation(url)\n\n    # turn labels to list of str, only including accepted units.\n    # if bool(labels[\"mergeGroups\"]):\n    if bool(labels.get(\"mergeGroups\", [])):\n        # clusters were merged, so we empty out metrics\n        metrics = {}\n    else:\n        # get the metrics from the parent curation\n        metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n\n    # insert this curation into the  Table\n    return Curation.insert_curation(\n        key,\n        parent_curation_id=key[\"curation_id\"],\n        labels=labels[\"labelsByUnit\"],\n        merge_groups=labels.get(\"mergeGroups\", []),\n        metrics=metrics,\n        description=description,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview_helper_fn/", "title": "sortingview_helper_fn.py", "text": "<p>Sortingview helper functions</p>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/", "title": "spikesorting_artifact.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.ArtifactDetectionParameters", "title": "<code>ArtifactDetectionParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_artifact.py</code> <pre><code>@schema\nclass ArtifactDetectionParameters(dj.Manual):\n    definition = \"\"\"\n    # Parameters for detecting artifact times within a sort group.\n    artifact_params_name: varchar(200)\n    ---\n    artifact_params: blob  # dictionary of parameters\n    \"\"\"\n\n    def insert_default(self):\n        \"\"\"Insert the default artifact parameters with an appropriate parameter dict.\"\"\"\n        artifact_params = {}\n        artifact_params[\"zscore_thresh\"] = None  # must be None or &gt;= 0\n        artifact_params[\"amplitude_thresh\"] = 3000  # must be None or &gt;= 0\n        # all electrodes of sort group\n        artifact_params[\"proportion_above_thresh\"] = 1.0\n        artifact_params[\"removal_window_ms\"] = 1.0  # in milliseconds\n        self.insert1([\"default\", artifact_params], skip_duplicates=True)\n\n        artifact_params_none = {}\n        artifact_params_none[\"zscore_thresh\"] = None\n        artifact_params_none[\"amplitude_thresh\"] = None\n        self.insert1([\"none\", artifact_params_none], skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.ArtifactDetectionParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default artifact parameters with an appropriate parameter dict.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_artifact.py</code> <pre><code>def insert_default(self):\n    \"\"\"Insert the default artifact parameters with an appropriate parameter dict.\"\"\"\n    artifact_params = {}\n    artifact_params[\"zscore_thresh\"] = None  # must be None or &gt;= 0\n    artifact_params[\"amplitude_thresh\"] = 3000  # must be None or &gt;= 0\n    # all electrodes of sort group\n    artifact_params[\"proportion_above_thresh\"] = 1.0\n    artifact_params[\"removal_window_ms\"] = 1.0  # in milliseconds\n    self.insert1([\"default\", artifact_params], skip_duplicates=True)\n\n    artifact_params_none = {}\n    artifact_params_none[\"zscore_thresh\"] = None\n    artifact_params_none[\"amplitude_thresh\"] = None\n    self.insert1([\"none\", artifact_params_none], skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/", "title": "spikesorting_curation.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation", "title": "<code>Curation</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n    definition = \"\"\"\n    # Stores each spike sorting; similar to IntervalList\n    curation_id: int # a number corresponding to the index of this curation\n    -&gt; SpikeSorting\n    ---\n    parent_curation_id=-1: int\n    curation_labels: blob # a dictionary of labels for the units\n    merge_groups: blob # a list of merge groups for the units\n    quality_metrics: blob # a list of quality metrics for the units (if available)\n    description='': varchar(1000) #optional description for this curated sort\n    time_of_creation: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    @staticmethod\n    def insert_curation(\n        sorting_key: dict,\n        parent_curation_id: int = -1,\n        labels=None,\n        merge_groups=None,\n        metrics=None,\n        description=\"\",\n    ):\n        \"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n        arguments) insert an entry into Curation.\n\n\n        Parameters\n        ----------\n        sorting_key : dict\n            The key for the original SpikeSorting\n        parent_curation_id : int, optional\n            The id of the parent sorting\n        labels : dict or None, optional\n        merge_groups : dict or None, optional\n        metrics : dict or None, optional\n            Computed metrics for sorting\n        description : str, optional\n            text description of this sort\n\n        Returns\n        -------\n        curation_key : dict\n\n        \"\"\"\n        if parent_curation_id == -1:\n            # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n            inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n            if len(inserted_curation) &gt; 0:\n                Warning(\n                    \"Sorting has already been inserted, returning key to previously\"\n                    \"inserted curation\"\n                )\n                return inserted_curation[0]\n\n        if labels is None:\n            labels = {}\n        if merge_groups is None:\n            merge_groups = []\n        if metrics is None:\n            metrics = {}\n\n        # generate a unique number for this curation\n        id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n        if len(id) &gt; 0:\n            curation_id = max(id) + 1\n        else:\n            curation_id = 0\n\n        # convert unit_ids in labels to integers for labels from sortingview.\n        new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n        sorting_key[\"curation_id\"] = curation_id\n        sorting_key[\"parent_curation_id\"] = parent_curation_id\n        sorting_key[\"description\"] = description\n        sorting_key[\"curation_labels\"] = new_labels\n        sorting_key[\"merge_groups\"] = merge_groups\n        sorting_key[\"quality_metrics\"] = metrics\n        sorting_key[\"time_of_creation\"] = int(time.time())\n\n        # mike: added skip duplicates\n        Curation.insert1(sorting_key, skip_duplicates=True)\n\n        # get the primary key for this curation\n        c_key = Curation.fetch(\"KEY\")[0]\n        curation_key = {item: sorting_key[item] for item in c_key}\n\n        return curation_key\n\n    @staticmethod\n    def get_recording(key: dict):\n        \"\"\"Returns the recording extractor for the recording related to this curation\n\n        Parameters\n        ----------\n        key : dict\n            SpikeSortingRecording key\n\n        Returns\n        -------\n        recording_extractor : spike interface recording extractor\n\n        \"\"\"\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        return si.load_extractor(recording_path)\n\n    @staticmethod\n    def get_curated_sorting(key: dict):\n        \"\"\"Returns the sorting extractor related to this curation,\n        with merges applied.\n\n        Parameters\n        ----------\n        key : dict\n            Curation key\n\n        Returns\n        -------\n        sorting_extractor: spike interface sorting extractor\n\n        \"\"\"\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        sorting = si.load_extractor(sorting_path)\n        merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n        # TODO: write code to get merged sorting extractor\n        if len(merge_groups) != 0:\n            return MergedSortingExtractor(\n                parent_sorting=sorting, merge_groups=merge_groups\n            )\n        else:\n            return sorting\n\n    @staticmethod\n    def save_sorting_nwb(\n        key,\n        sorting,\n        timestamps,\n        sort_interval_list_name,\n        sort_interval,\n        labels=None,\n        metrics=None,\n        unit_ids=None,\n    ):\n        \"\"\"Store a sorting in a new AnalysisNwbfile\n\n        Parameters\n        ----------\n        key : dict\n            key to SpikeSorting table\n        sorting : si.Sorting\n            sorting\n        timestamps : array_like\n            Time stamps of the sorted recoridng;\n            used to convert the spike timings from index to real time\n        sort_interval_list_name : str\n            name of sort interval\n        sort_interval : list\n            interval for start and end of sort\n        labels : dict, optional\n            curation labels, by default None\n        metrics : dict, optional\n            quality metrics, by default None\n        unit_ids : list, optional\n            IDs of units whose spiketrains to save, by default None\n\n        Returns\n        -------\n        analysis_file_name : str\n        units_object_id : str\n\n        \"\"\"\n\n        sort_interval_valid_times = (\n            IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n        ).fetch1(\"valid_times\")\n\n        units = dict()\n        units_valid_times = dict()\n        units_sort_interval = dict()\n\n        if unit_ids is None:\n            unit_ids = sorting.get_unit_ids()\n\n        for unit_id in unit_ids:\n            spike_times_in_samples = sorting.get_unit_spike_train(\n                unit_id=unit_id\n            )\n            units[unit_id] = timestamps[spike_times_in_samples]\n            units_valid_times[unit_id] = sort_interval_valid_times\n            units_sort_interval[unit_id] = [sort_interval]\n\n        analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n        object_ids = AnalysisNwbfile().add_units(\n            analysis_file_name,\n            units,\n            units_valid_times,\n            units_sort_interval,\n            metrics=metrics,\n            labels=labels,\n        )\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n        if object_ids == \"\":\n            print(\n                \"Sorting contains no units.\"\n                \"Created an empty analysis nwb file anyway.\"\n            )\n            units_object_id = \"\"\n        else:\n            units_object_id = object_ids[0]\n\n        return analysis_file_name, units_object_id\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.insert_curation", "title": "<code>insert_curation(sorting_key, parent_curation_id=-1, labels=None, merge_groups=None, metrics=None, description='')</code>  <code>staticmethod</code>", "text": "<p>Given a SpikeSorting key and the parent_sorting_id (and optional arguments) insert an entry into Curation.</p> <p>Parameters:</p> Name Type Description Default <code>sorting_key</code> <code>dict</code> <p>The key for the original SpikeSorting</p> required <code>parent_curation_id</code> <code>int</code> <p>The id of the parent sorting</p> <code>-1</code> <code>labels</code> <code>dict or None</code> <code>None</code> <code>merge_groups</code> <code>dict or None</code> <code>None</code> <code>metrics</code> <code>dict or None</code> <p>Computed metrics for sorting</p> <code>None</code> <code>description</code> <code>str</code> <p>text description of this sort</p> <code>''</code> <p>Returns:</p> Name Type Description <code>curation_key</code> <code>dict</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef insert_curation(\n    sorting_key: dict,\n    parent_curation_id: int = -1,\n    labels=None,\n    merge_groups=None,\n    metrics=None,\n    description=\"\",\n):\n    \"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n    arguments) insert an entry into Curation.\n\n\n    Parameters\n    ----------\n    sorting_key : dict\n        The key for the original SpikeSorting\n    parent_curation_id : int, optional\n        The id of the parent sorting\n    labels : dict or None, optional\n    merge_groups : dict or None, optional\n    metrics : dict or None, optional\n        Computed metrics for sorting\n    description : str, optional\n        text description of this sort\n\n    Returns\n    -------\n    curation_key : dict\n\n    \"\"\"\n    if parent_curation_id == -1:\n        # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n        inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n        if len(inserted_curation) &gt; 0:\n            Warning(\n                \"Sorting has already been inserted, returning key to previously\"\n                \"inserted curation\"\n            )\n            return inserted_curation[0]\n\n    if labels is None:\n        labels = {}\n    if merge_groups is None:\n        merge_groups = []\n    if metrics is None:\n        metrics = {}\n\n    # generate a unique number for this curation\n    id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n    if len(id) &gt; 0:\n        curation_id = max(id) + 1\n    else:\n        curation_id = 0\n\n    # convert unit_ids in labels to integers for labels from sortingview.\n    new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n    sorting_key[\"curation_id\"] = curation_id\n    sorting_key[\"parent_curation_id\"] = parent_curation_id\n    sorting_key[\"description\"] = description\n    sorting_key[\"curation_labels\"] = new_labels\n    sorting_key[\"merge_groups\"] = merge_groups\n    sorting_key[\"quality_metrics\"] = metrics\n    sorting_key[\"time_of_creation\"] = int(time.time())\n\n    # mike: added skip duplicates\n    Curation.insert1(sorting_key, skip_duplicates=True)\n\n    # get the primary key for this curation\n    c_key = Curation.fetch(\"KEY\")[0]\n    curation_key = {item: sorting_key[item] for item in c_key}\n\n    return curation_key\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_recording", "title": "<code>get_recording(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the recording extractor for the recording related to this curation</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>SpikeSortingRecording key</p> required <p>Returns:</p> Name Type Description <code>recording_extractor</code> <code>spike interface recording extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_recording(key: dict):\n    \"\"\"Returns the recording extractor for the recording related to this curation\n\n    Parameters\n    ----------\n    key : dict\n        SpikeSortingRecording key\n\n    Returns\n    -------\n    recording_extractor : spike interface recording extractor\n\n    \"\"\"\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    return si.load_extractor(recording_path)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_curated_sorting", "title": "<code>get_curated_sorting(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the sorting extractor related to this curation, with merges applied.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Curation key</p> required <p>Returns:</p> Name Type Description <code>sorting_extractor</code> <code>spike interface sorting extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_curated_sorting(key: dict):\n    \"\"\"Returns the sorting extractor related to this curation,\n    with merges applied.\n\n    Parameters\n    ----------\n    key : dict\n        Curation key\n\n    Returns\n    -------\n    sorting_extractor: spike interface sorting extractor\n\n    \"\"\"\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    sorting = si.load_extractor(sorting_path)\n    merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n    # TODO: write code to get merged sorting extractor\n    if len(merge_groups) != 0:\n        return MergedSortingExtractor(\n            parent_sorting=sorting, merge_groups=merge_groups\n        )\n    else:\n        return sorting\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.save_sorting_nwb", "title": "<code>save_sorting_nwb(key, sorting, timestamps, sort_interval_list_name, sort_interval, labels=None, metrics=None, unit_ids=None)</code>  <code>staticmethod</code>", "text": "<p>Store a sorting in a new AnalysisNwbfile</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key to SpikeSorting table</p> required <code>sorting</code> <code>Sorting</code> <p>sorting</p> required <code>timestamps</code> <code>array_like</code> <p>Time stamps of the sorted recoridng; used to convert the spike timings from index to real time</p> required <code>sort_interval_list_name</code> <code>str</code> <p>name of sort interval</p> required <code>sort_interval</code> <code>list</code> <p>interval for start and end of sort</p> required <code>labels</code> <code>dict</code> <p>curation labels, by default None</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>quality metrics, by default None</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>IDs of units whose spiketrains to save, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <code>units_object_id</code> <code>str</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef save_sorting_nwb(\n    key,\n    sorting,\n    timestamps,\n    sort_interval_list_name,\n    sort_interval,\n    labels=None,\n    metrics=None,\n    unit_ids=None,\n):\n    \"\"\"Store a sorting in a new AnalysisNwbfile\n\n    Parameters\n    ----------\n    key : dict\n        key to SpikeSorting table\n    sorting : si.Sorting\n        sorting\n    timestamps : array_like\n        Time stamps of the sorted recoridng;\n        used to convert the spike timings from index to real time\n    sort_interval_list_name : str\n        name of sort interval\n    sort_interval : list\n        interval for start and end of sort\n    labels : dict, optional\n        curation labels, by default None\n    metrics : dict, optional\n        quality metrics, by default None\n    unit_ids : list, optional\n        IDs of units whose spiketrains to save, by default None\n\n    Returns\n    -------\n    analysis_file_name : str\n    units_object_id : str\n\n    \"\"\"\n\n    sort_interval_valid_times = (\n        IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n    ).fetch1(\"valid_times\")\n\n    units = dict()\n    units_valid_times = dict()\n    units_sort_interval = dict()\n\n    if unit_ids is None:\n        unit_ids = sorting.get_unit_ids()\n\n    for unit_id in unit_ids:\n        spike_times_in_samples = sorting.get_unit_spike_train(\n            unit_id=unit_id\n        )\n        units[unit_id] = timestamps[spike_times_in_samples]\n        units_valid_times[unit_id] = sort_interval_valid_times\n        units_sort_interval[unit_id] = [sort_interval]\n\n    analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n    object_ids = AnalysisNwbfile().add_units(\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=metrics,\n        labels=labels,\n    )\n    AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n    if object_ids == \"\":\n        print(\n            \"Sorting contains no units.\"\n            \"Created an empty analysis nwb file anyway.\"\n        )\n        units_object_id = \"\"\n    else:\n        units_object_id = object_ids[0]\n\n    return analysis_file_name, units_object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Waveforms", "title": "<code>Waveforms</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass Waveforms(dj.Computed):\n    definition = \"\"\"\n    -&gt; WaveformSelection\n    ---\n    waveform_extractor_path: varchar(400)\n    -&gt; AnalysisNwbfile\n    waveforms_object_id: varchar(40)   # Object ID for the waveforms in NWB file\n    \"\"\"\n\n    def make(self, key):\n        recording = Curation.get_recording(key)\n        if recording.get_num_segments() &gt; 1:\n            recording = si.concatenate_recordings([recording])\n\n        sorting = Curation.get_curated_sorting(key)\n\n        print(\"Extracting waveforms...\")\n        waveform_params = (WaveformParameters &amp; key).fetch1(\"waveform_params\")\n        if \"whiten\" in waveform_params:\n            if waveform_params.pop(\"whiten\"):\n                recording = sip.whiten(recording, dtype=\"float32\")\n\n        waveform_extractor_name = self._get_waveform_extractor_name(key)\n        key[\"waveform_extractor_path\"] = str(\n            Path(os.environ[\"SPYGLASS_WAVEFORMS_DIR\"])\n            / Path(waveform_extractor_name)\n        )\n        if os.path.exists(key[\"waveform_extractor_path\"]):\n            shutil.rmtree(key[\"waveform_extractor_path\"])\n        waveforms = si.extract_waveforms(\n            recording=recording,\n            sorting=sorting,\n            folder=key[\"waveform_extractor_path\"],\n            **waveform_params,\n        )\n\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        object_id = AnalysisNwbfile().add_units_waveforms(\n            key[\"analysis_file_name\"], waveform_extractor=waveforms\n        )\n        key[\"waveforms_object_id\"] = object_id\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n\n        self.insert1(key)\n\n    def load_waveforms(self, key: dict):\n        \"\"\"Returns a spikeinterface waveform extractor specified by key\n\n        Parameters\n        ----------\n        key : dict\n            Could be an entry in Waveforms, or some other key that uniquely defines\n            an entry in Waveforms\n\n        Returns\n        -------\n        we : spikeinterface.WaveformExtractor\n        \"\"\"\n        we_path = (self &amp; key).fetch1(\"waveform_extractor_path\")\n        we = si.WaveformExtractor.load_from_folder(we_path)\n        return we\n\n    def fetch_nwb(self, key):\n        # TODO: implement fetching waveforms from NWB\n        return NotImplementedError\n\n    def _get_waveform_extractor_name(self, key):\n        waveform_params_name = (WaveformParameters &amp; key).fetch1(\n            \"waveform_params_name\"\n        )\n\n        return (\n            f'{key[\"nwb_file_name\"]}_{str(uuid.uuid4())[0:8]}_'\n            f'{key[\"curation_id\"]}_{waveform_params_name}_waveforms'\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Waveforms.load_waveforms", "title": "<code>load_waveforms(key)</code>", "text": "<p>Returns a spikeinterface waveform extractor specified by key</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Could be an entry in Waveforms, or some other key that uniquely defines an entry in Waveforms</p> required <p>Returns:</p> Name Type Description <code>we</code> <code>WaveformExtractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def load_waveforms(self, key: dict):\n    \"\"\"Returns a spikeinterface waveform extractor specified by key\n\n    Parameters\n    ----------\n    key : dict\n        Could be an entry in Waveforms, or some other key that uniquely defines\n        an entry in Waveforms\n\n    Returns\n    -------\n    we : spikeinterface.WaveformExtractor\n    \"\"\"\n    we_path = (self &amp; key).fetch1(\"waveform_extractor_path\")\n    we = si.WaveformExtractor.load_from_folder(we_path)\n    return we\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.MetricParameters", "title": "<code>MetricParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass MetricParameters(dj.Manual):\n    definition = \"\"\"\n    # Parameters for computing quality metrics of sorted units\n    metric_params_name: varchar(200)\n    ---\n    metric_params: blob\n    \"\"\"\n    metric_default_params = {\n        \"snr\": {\n            \"peak_sign\": \"neg\",\n            \"random_chunk_kwargs_dict\": {\n                \"num_chunks_per_segment\": 20,\n                \"chunk_size\": 10000,\n                \"seed\": 0,\n            },\n        },\n        \"isi_violation\": {\"isi_threshold_ms\": 1.5, \"min_isi_ms\": 0.0},\n        \"nn_isolation\": {\n            \"max_spikes\": 1000,\n            \"min_spikes\": 10,\n            \"n_neighbors\": 5,\n            \"n_components\": 7,\n            \"radius_um\": 100,\n            \"seed\": 0,\n        },\n        \"nn_noise_overlap\": {\n            \"max_spikes\": 1000,\n            \"min_spikes\": 10,\n            \"n_neighbors\": 5,\n            \"n_components\": 7,\n            \"radius_um\": 100,\n            \"seed\": 0,\n        },\n        \"peak_channel\": {\"peak_sign\": \"neg\"},\n        \"num_spikes\": {},\n    }\n    # Example of peak_offset parameters 'peak_offset': {'peak_sign': 'neg'}\n    available_metrics = [\n        \"snr\",\n        \"isi_violation\",\n        \"nn_isolation\",\n        \"nn_noise_overlap\",\n        \"peak_offset\",\n        \"peak_channel\",\n        \"num_spikes\",\n    ]\n\n    def get_metric_default_params(self, metric: str):\n        \"Returns default params for the given metric\"\n        return self.metric_default_params(metric)\n\n    def insert_default(self):\n        self.insert1(\n            [\"franklab_default3\", self.metric_default_params],\n            skip_duplicates=True,\n        )\n\n    def get_available_metrics(self):\n        for metric in _metric_name_to_func:\n            if metric in self.available_metrics:\n                metric_doc = _metric_name_to_func[metric].__doc__.split(\"\\n\")[0]\n                metric_string = (\"{metric_name} : {metric_doc}\").format(\n                    metric_name=metric, metric_doc=metric_doc\n                )\n                print(metric_string + \"\\n\")\n\n    # TODO\n    def _validate_metrics_list(self, key):\n        \"\"\"Checks whether a row to be inserted contains only the available metrics\"\"\"\n        # get available metrics list\n        # get metric list from key\n        # compare\n        return NotImplementedError\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.MetricParameters.get_metric_default_params", "title": "<code>get_metric_default_params(metric)</code>", "text": "<p>Returns default params for the given metric</p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def get_metric_default_params(self, metric: str):\n    \"Returns default params for the given metric\"\n    return self.metric_default_params(metric)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.AutomaticCuration", "title": "<code>AutomaticCuration</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass AutomaticCuration(dj.Computed):\n    definition = \"\"\"\n    -&gt; AutomaticCurationSelection\n    ---\n    auto_curation_key: blob # the key to the curation inserted by make\n    \"\"\"\n\n    def make(self, key):\n        metrics_path = (QualityMetrics &amp; key).fetch1(\"quality_metrics_path\")\n        with open(metrics_path) as f:\n            quality_metrics = json.load(f)\n\n        # get the curation information and the curated sorting\n        parent_curation = (Curation &amp; key).fetch(as_dict=True)[0]\n        parent_merge_groups = parent_curation[\"merge_groups\"]\n        parent_labels = parent_curation[\"curation_labels\"]\n        parent_curation_id = parent_curation[\"curation_id\"]\n        parent_sorting = Curation.get_curated_sorting(key)\n\n        merge_params = (AutomaticCurationParameters &amp; key).fetch1(\n            \"merge_params\"\n        )\n        merge_groups, units_merged = self.get_merge_groups(\n            parent_sorting, parent_merge_groups, quality_metrics, merge_params\n        )\n\n        label_params = (AutomaticCurationParameters &amp; key).fetch1(\n            \"label_params\"\n        )\n        labels = self.get_labels(\n            parent_sorting, parent_labels, quality_metrics, label_params\n        )\n\n        # keep the quality metrics only if no merging occurred.\n        metrics = quality_metrics if not units_merged else None\n\n        # insert this sorting into the CuratedSpikeSorting Table\n        # first remove keys that aren't part of the Sorting (the primary key of curation)\n        c_key = (SpikeSorting &amp; key).fetch(\"KEY\")[0]\n        curation_key = {item: key[item] for item in key if item in c_key}\n        key[\"auto_curation_key\"] = Curation.insert_curation(\n            curation_key,\n            parent_curation_id=parent_curation_id,\n            labels=labels,\n            merge_groups=merge_groups,\n            metrics=metrics,\n            description=\"auto curated\",\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def get_merge_groups(\n        sorting, parent_merge_groups, quality_metrics, merge_params\n    ):\n        \"\"\"Identifies units to be merged based on the quality_metrics and\n        merge parameters and returns an updated list of merges for the curation.\n\n        Parameters\n        ---------\n        sorting : spikeinterface.sorting\n        parent_merge_groups : list\n            Information about previous merges\n        quality_metrics : list\n        merge_params : dict\n\n        Returns\n        -------\n        merge_groups : list of lists\n        merge_occurred : bool\n\n        \"\"\"\n\n        # overview:\n        # 1. Use quality metrics to determine merge groups for units\n        # 2. Combine merge groups with current merge groups to produce union of merges\n\n        if not merge_params:\n            return parent_merge_groups, False\n        else:\n            # TODO: use the metrics to identify clusters that should be merged\n            # new_merges should then reflect those merges and the line below should be deleted.\n            new_merges = []\n            # append these merges to the parent merge_groups\n            for new_merge in new_merges:\n                # check to see if the first cluster listed is in a current merge group\n                for previous_merge in parent_merge_groups:\n                    if new_merge[0] == previous_merge[0]:\n                        # add the additional units in new_merge to the identified merge group.\n                        previous_merge.extend(new_merge[1:])\n                        previous_merge.sort()\n                        break\n                else:\n                    # append this merge group to the list if no previous merge\n                    parent_merge_groups.append(new_merge)\n            return parent_merge_groups.sort(), True\n\n    @staticmethod\n    def get_labels(sorting, parent_labels, quality_metrics, label_params):\n        \"\"\"Returns a dictionary of labels using quality_metrics and label\n        parameters.\n\n        Parameters\n        ---------\n        sorting : spikeinterface.sorting\n        parent_labels : list\n            Information about previous merges\n        quality_metrics : list\n        label_params : dict\n\n        Returns\n        -------\n        parent_labels : list\n\n        \"\"\"\n        # overview:\n        # 1. Use quality metrics to determine labels for units\n        # 2. Append labels to current labels, checking for inconsistencies\n        if not label_params:\n            return parent_labels\n        else:\n            for metric in label_params:\n                if metric not in quality_metrics:\n                    Warning(f\"{metric} not found in quality metrics; skipping\")\n                else:\n                    compare = _comparison_to_function[label_params[metric][0]]\n\n                    for unit_id in quality_metrics[metric].keys():\n                        # compare the quality metric to the threshold with the specified operator\n                        # note that label_params[metric] is a three element list with a comparison operator as a string,\n                        # the threshold value, and a list of labels to be applied if the comparison is true\n                        if compare(\n                            quality_metrics[metric][unit_id],\n                            label_params[metric][1],\n                        ):\n                            if unit_id not in parent_labels:\n                                parent_labels[unit_id] = label_params[metric][2]\n                            # check if the label is already there, and if not, add it\n                            elif (\n                                label_params[metric][2]\n                                not in parent_labels[unit_id]\n                            ):\n                                parent_labels[unit_id].extend(\n                                    label_params[metric][2]\n                                )\n            return parent_labels\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.AutomaticCuration.get_merge_groups", "title": "<code>get_merge_groups(sorting, parent_merge_groups, quality_metrics, merge_params)</code>  <code>staticmethod</code>", "text": "<p>Identifies units to be merged based on the quality_metrics and merge parameters and returns an updated list of merges for the curation.</p> <p>Parameters:</p> Name Type Description Default <code>sorting</code> <code>sorting</code> required <code>parent_merge_groups</code> <code>list</code> <p>Information about previous merges</p> required <code>quality_metrics</code> <code>list</code> required <code>merge_params</code> <code>dict</code> required <p>Returns:</p> Name Type Description <code>merge_groups</code> <code>list of lists</code> <code>merge_occurred</code> <code>bool</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_merge_groups(\n    sorting, parent_merge_groups, quality_metrics, merge_params\n):\n    \"\"\"Identifies units to be merged based on the quality_metrics and\n    merge parameters and returns an updated list of merges for the curation.\n\n    Parameters\n    ---------\n    sorting : spikeinterface.sorting\n    parent_merge_groups : list\n        Information about previous merges\n    quality_metrics : list\n    merge_params : dict\n\n    Returns\n    -------\n    merge_groups : list of lists\n    merge_occurred : bool\n\n    \"\"\"\n\n    # overview:\n    # 1. Use quality metrics to determine merge groups for units\n    # 2. Combine merge groups with current merge groups to produce union of merges\n\n    if not merge_params:\n        return parent_merge_groups, False\n    else:\n        # TODO: use the metrics to identify clusters that should be merged\n        # new_merges should then reflect those merges and the line below should be deleted.\n        new_merges = []\n        # append these merges to the parent merge_groups\n        for new_merge in new_merges:\n            # check to see if the first cluster listed is in a current merge group\n            for previous_merge in parent_merge_groups:\n                if new_merge[0] == previous_merge[0]:\n                    # add the additional units in new_merge to the identified merge group.\n                    previous_merge.extend(new_merge[1:])\n                    previous_merge.sort()\n                    break\n            else:\n                # append this merge group to the list if no previous merge\n                parent_merge_groups.append(new_merge)\n        return parent_merge_groups.sort(), True\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.AutomaticCuration.get_labels", "title": "<code>get_labels(sorting, parent_labels, quality_metrics, label_params)</code>  <code>staticmethod</code>", "text": "<p>Returns a dictionary of labels using quality_metrics and label parameters.</p> <p>Parameters:</p> Name Type Description Default <code>sorting</code> <code>sorting</code> required <code>parent_labels</code> <code>list</code> <p>Information about previous merges</p> required <code>quality_metrics</code> <code>list</code> required <code>label_params</code> <code>dict</code> required <p>Returns:</p> Name Type Description <code>parent_labels</code> <code>list</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_labels(sorting, parent_labels, quality_metrics, label_params):\n    \"\"\"Returns a dictionary of labels using quality_metrics and label\n    parameters.\n\n    Parameters\n    ---------\n    sorting : spikeinterface.sorting\n    parent_labels : list\n        Information about previous merges\n    quality_metrics : list\n    label_params : dict\n\n    Returns\n    -------\n    parent_labels : list\n\n    \"\"\"\n    # overview:\n    # 1. Use quality metrics to determine labels for units\n    # 2. Append labels to current labels, checking for inconsistencies\n    if not label_params:\n        return parent_labels\n    else:\n        for metric in label_params:\n            if metric not in quality_metrics:\n                Warning(f\"{metric} not found in quality metrics; skipping\")\n            else:\n                compare = _comparison_to_function[label_params[metric][0]]\n\n                for unit_id in quality_metrics[metric].keys():\n                    # compare the quality metric to the threshold with the specified operator\n                    # note that label_params[metric] is a three element list with a comparison operator as a string,\n                    # the threshold value, and a list of labels to be applied if the comparison is true\n                    if compare(\n                        quality_metrics[metric][unit_id],\n                        label_params[metric][1],\n                    ):\n                        if unit_id not in parent_labels:\n                            parent_labels[unit_id] = label_params[metric][2]\n                        # check if the label is already there, and if not, add it\n                        elif (\n                            label_params[metric][2]\n                            not in parent_labels[unit_id]\n                        ):\n                            parent_labels[unit_id].extend(\n                                label_params[metric][2]\n                            )\n        return parent_labels\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.CuratedSpikeSorting", "title": "<code>CuratedSpikeSorting</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass CuratedSpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; CuratedSpikeSortingSelection\n    ---\n    -&gt; AnalysisNwbfile\n    units_object_id: varchar(40)\n    \"\"\"\n\n    class Unit(dj.Part):\n        definition = \"\"\"\n        # Table for holding sorted units\n        -&gt; CuratedSpikeSorting\n        unit_id: int   # ID for each unit\n        ---\n        label='': varchar(200)   # optional set of labels for each unit\n        nn_noise_overlap=-1: float   # noise overlap metric for each unit\n        nn_isolation=-1: float   # isolation score metric for each unit\n        isi_violation=-1: float   # ISI violation score for each unit\n        snr=0: float            # SNR for each unit\n        firing_rate=-1: float   # firing rate\n        num_spikes=-1: int   # total number of spikes\n        peak_channel=null: int # channel of maximum amplitude for each unit\n        \"\"\"\n\n    def make(self, key):\n        unit_labels_to_remove = [\"reject\"]\n        # check that the Curation has metrics\n        metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n        if metrics == {}:\n            Warning(\n                f\"Metrics for Curation {key} should normally be calculated before insertion here\"\n            )\n\n        sorting = Curation.get_curated_sorting(key)\n        unit_ids = sorting.get_unit_ids()\n        # Get the labels for the units, add only those units that do not have 'reject' or 'noise' labels\n        unit_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n        accepted_units = []\n        for unit_id in unit_ids:\n            if unit_id in unit_labels:\n                if (\n                    len(set(unit_labels_to_remove) &amp; set(unit_labels[unit_id]))\n                    == 0\n                ):\n                    accepted_units.append(unit_id)\n            else:\n                accepted_units.append(unit_id)\n\n        # get the labels for the accepted units\n        labels = {}\n        for unit_id in accepted_units:\n            if unit_id in unit_labels:\n                labels[unit_id] = \",\".join(unit_labels[unit_id])\n\n        # convert unit_ids in metrics to integers, including only accepted units.\n        #  TODO: convert to int this somewhere else\n        final_metrics = {}\n        for metric in metrics:\n            final_metrics[metric] = {\n                int(unit_id): metrics[metric][unit_id]\n                for unit_id in metrics[metric]\n                if int(unit_id) in accepted_units\n            }\n\n        print(f\"Found {len(accepted_units)} accepted units\")\n\n        # get the sorting and save it in the NWB file\n        sorting = Curation.get_curated_sorting(key)\n        recording = Curation.get_recording(key)\n\n        # get the sort_interval and sorting interval list\n        sort_interval_name = (SpikeSortingRecording &amp; key).fetch1(\n            \"sort_interval_name\"\n        )\n        sort_interval = (SortInterval &amp; key).fetch1(\"sort_interval\")\n        sort_interval_list_name = (SpikeSorting &amp; key).fetch1(\n            \"artifact_removed_interval_list_name\"\n        )\n\n        timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n\n        (\n            key[\"analysis_file_name\"],\n            key[\"units_object_id\"],\n        ) = Curation().save_sorting_nwb(\n            key,\n            sorting,\n            timestamps,\n            sort_interval_list_name,\n            sort_interval,\n            metrics=final_metrics,\n            unit_ids=accepted_units,\n            labels=labels,\n        )\n        self.insert1(key)\n\n        # now add the units\n        # Remove the non primary key entries.\n        del key[\"units_object_id\"]\n        del key[\"analysis_file_name\"]\n\n        metric_fields = self.metrics_fields()\n        for unit_id in accepted_units:\n            key[\"unit_id\"] = unit_id\n            if unit_id in labels:\n                key[\"label\"] = labels[unit_id]\n            for field in metric_fields:\n                if field in final_metrics:\n                    key[field] = final_metrics[field][unit_id]\n                else:\n                    Warning(\n                        f\"No metric named {field} in computed unit quality metrics; skipping\"\n                    )\n            CuratedSpikeSorting.Unit.insert1(key)\n\n    def metrics_fields(self):\n        \"\"\"Returns a list of the metrics that are currently in the Units table.\"\"\"\n        unit_info = self.Unit().fetch(limit=1, format=\"frame\")\n        unit_fields = [column for column in unit_info.columns]\n        unit_fields.remove(\"label\")\n        return unit_fields\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.CuratedSpikeSorting.metrics_fields", "title": "<code>metrics_fields()</code>", "text": "<p>Returns a list of the metrics that are currently in the Units table.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def metrics_fields(self):\n    \"\"\"Returns a list of the metrics that are currently in the Units table.\"\"\"\n    unit_info = self.Unit().fetch(limit=1, format=\"frame\")\n    unit_fields = [column for column in unit_info.columns]\n    unit_fields.remove(\"label\")\n    return unit_fields\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.UnitInclusionParameters", "title": "<code>UnitInclusionParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass UnitInclusionParameters(dj.Manual):\n    definition = \"\"\"\n    unit_inclusion_param_name: varchar(80) # the name of the list of thresholds for unit inclusion\n    ---\n    inclusion_param_dict: blob # the dictionary of inclusion / exclusion parameters\n    \"\"\"\n\n    def insert1(self, key, **kwargs):\n        # check to see that the dictionary fits the specifications\n        # The inclusion parameter dict has the following form:\n        # param_dict['metric_name'] = (operator, value)\n        #    where operator is '&lt;', '&gt;', &lt;=', '&gt;=', or '==' and value is the comparison (float) value to be used ()\n        # param_dict['exclude_labels'] = [list of labels to exclude]\n        pdict = key[\"inclusion_param_dict\"]\n        metrics_list = CuratedSpikeSorting().metrics_fields()\n\n        for k in pdict:\n            if k not in metrics_list and k != \"exclude_labels\":\n                raise Exception(\n                    f\"key {k} is not a valid element of the inclusion_param_dict\"\n                )\n            if k in metrics_list:\n                if pdict[k][0] not in _comparison_to_function:\n                    raise Exception(\n                        f\"operator {pdict[k][0]} for metric {k} is not in the valid operators list: {_comparison_to_function.keys()}\"\n                    )\n            if k == \"exclude_labels\":\n                for label in pdict[k]:\n                    if label not in valid_labels:\n                        raise Exception(\n                            f\"exclude label {label} is not in the valid_labels list: {valid_labels}\"\n                        )\n        super().insert1(key, **kwargs)\n\n    def get_included_units(\n        self, curated_sorting_key, unit_inclusion_param_name\n    ):\n        \"\"\"given a reference to a set of curated sorting units and the name of a unit inclusion parameter list, returns\n\n        Parameters\n        ----------\n        curated_sorting_key : dict\n            key to select a set of curated sorting\n        unit_inclusion_param_name : str\n            name of a unit inclusion parameter entry\n\n        Returns\n        ------unit key\n        dict\n            key to select all of the included units\n        \"\"\"\n        curated_sortings = (CuratedSpikeSorting() &amp; curated_sorting_key).fetch()\n        inc_param_dict = (\n            UnitInclusionParameters\n            &amp; {\"unit_inclusion_param_name\": unit_inclusion_param_name}\n        ).fetch1(\"inclusion_param_dict\")\n        units = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch()\n        units_key = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch(\n            \"KEY\"\n        )\n        # get a list of the metrics in the units table\n        metrics_list = CuratedSpikeSorting().metrics_fields()\n        # get the list of labels to exclude if there is one\n        if \"exclude_labels\" in inc_param_dict:\n            exclude_labels = inc_param_dict[\"exclude_labels\"]\n            del inc_param_dict[\"exclude_labels\"]\n        else:\n            exclude_labels = []\n\n        # create a list of the units to kepp.\n        keep = np.asarray([True] * len(units))\n        for metric in inc_param_dict:\n            # for all units, go through each metric, compare it to the value specified, and update the list to be kept\n            keep = np.logical_and(\n                keep,\n                _comparison_to_function[inc_param_dict[metric][0]](\n                    units[metric], inc_param_dict[metric][1]\n                ),\n            )\n\n        # now exclude by label if it is specified\n        if len(exclude_labels):\n            included_units = []\n            for unit_ind in np.ravel(np.argwhere(keep)):\n                labels = units[unit_ind][\"label\"].split(\",\")\n                exclude = False\n                for label in labels:\n                    if label in exclude_labels:\n                        keep[unit_ind] = False\n                        break\n        # return units that passed all of the tests\n        # TODO: Make this more efficient\n        return {i: units_key[i] for i in np.ravel(np.argwhere(keep))}\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.UnitInclusionParameters.get_included_units", "title": "<code>get_included_units(curated_sorting_key, unit_inclusion_param_name)</code>", "text": "<p>given a reference to a set of curated sorting units and the name of a unit inclusion parameter list, returns</p> <p>Parameters:</p> Name Type Description Default <code>curated_sorting_key</code> <code>dict</code> <p>key to select a set of curated sorting</p> required <code>unit_inclusion_param_name</code> <code>str</code> <p>name of a unit inclusion parameter entry</p> required <p>Returns ------unit key dict     key to select all of the included units</p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def get_included_units(\n    self, curated_sorting_key, unit_inclusion_param_name\n):\n    \"\"\"given a reference to a set of curated sorting units and the name of a unit inclusion parameter list, returns\n\n    Parameters\n    ----------\n    curated_sorting_key : dict\n        key to select a set of curated sorting\n    unit_inclusion_param_name : str\n        name of a unit inclusion parameter entry\n\n    Returns\n    ------unit key\n    dict\n        key to select all of the included units\n    \"\"\"\n    curated_sortings = (CuratedSpikeSorting() &amp; curated_sorting_key).fetch()\n    inc_param_dict = (\n        UnitInclusionParameters\n        &amp; {\"unit_inclusion_param_name\": unit_inclusion_param_name}\n    ).fetch1(\"inclusion_param_dict\")\n    units = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch()\n    units_key = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch(\n        \"KEY\"\n    )\n    # get a list of the metrics in the units table\n    metrics_list = CuratedSpikeSorting().metrics_fields()\n    # get the list of labels to exclude if there is one\n    if \"exclude_labels\" in inc_param_dict:\n        exclude_labels = inc_param_dict[\"exclude_labels\"]\n        del inc_param_dict[\"exclude_labels\"]\n    else:\n        exclude_labels = []\n\n    # create a list of the units to kepp.\n    keep = np.asarray([True] * len(units))\n    for metric in inc_param_dict:\n        # for all units, go through each metric, compare it to the value specified, and update the list to be kept\n        keep = np.logical_and(\n            keep,\n            _comparison_to_function[inc_param_dict[metric][0]](\n                units[metric], inc_param_dict[metric][1]\n            ),\n        )\n\n    # now exclude by label if it is specified\n    if len(exclude_labels):\n        included_units = []\n        for unit_ind in np.ravel(np.argwhere(keep)):\n            labels = units[unit_ind][\"label\"].split(\",\")\n            exclude = False\n            for label in labels:\n                if label in exclude_labels:\n                    keep[unit_ind] = False\n                    break\n    # return units that passed all of the tests\n    # TODO: Make this more efficient\n    return {i: units_key[i] for i in np.ravel(np.argwhere(keep))}\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_populator/", "title": "spikesorting_populator.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_populator/#src.spyglass.spikesorting.spikesorting_populator.spikesorting_pipeline_populator", "title": "<code>spikesorting_pipeline_populator(nwb_file_name, team_name, fig_url_repo=None, interval_list_name=None, sort_interval_name=None, pipeline_parameters_name=None, probe_restriction={}, artifact_parameters='ampl_2000_prop_75', preproc_params_name='franklab_tetrode_hippocampus', sorter='mountainsort4', sorter_params_name='franklab_tetrode_hippocampus_30KHz_tmp', waveform_params_name='default_whitened', metric_params_name='peak_offest_num_spikes_2', auto_curation_params_name='mike_noise_03_offset_2_isi_0025_mua')</code>", "text": "<p>Automatically populate the spike sorting pipeline for a given epoch</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>Session ID</p> required <code>team_name</code> <code>str</code> <p>Which team to assign the spike sorting to</p> required <code>fig_url_repo</code> <code>str</code> <p>Where to store the curation figurl json files (e.g., 'gh://LorenFrankLab/sorting-curations/main/user/'). Default None to skip figurl</p> <code>None</code> <code>interval_list_name</code> <code>(str)</code> <p>if sort_interval_name not provided, will create a sort interval for the given interval with the same name</p> <code>None</code> <code>sort_interval_name</code> <code>str</code> <p>if provided, will use the given sort interval, requires making this interval yourself</p> <code>None</code> <code>pipeline_parameters_name</code> <code>str</code> <p>If provided, will lookup pipeline parameters from the SpikeSortingPipelineParameters table, supersedes other values provided, by default None</p> <code>None</code> <code>probe_restriction</code> <code>dict</code> <p>Restricts analysis to sort groups with matching keys. Can use keys from the SortGroup and ElectrodeGroup Tables (e.g. electrode_group_name, probe_id, target_hemisphere), by default {}</p> <code>{}</code> <code>artifact_parameters</code> <code>str</code> <p>parameter set for artifact detection, by default \"ampl_2000_prop_75\"</p> <code>'ampl_2000_prop_75'</code> <code>preproc_params_name</code> <code>str</code> <p>parameter set for spikesorting recording, by default \"franklab_tetrode_hippocampus\"</p> <code>'franklab_tetrode_hippocampus'</code> <code>sorter</code> <code>str</code> <p>which spikesorting algorithm to use, by default \"mountainsort4\"</p> <code>'mountainsort4'</code> <code>sorter_params_name</code> <code>str</code> <p>parameters for the spike sorting algorithm, by default \"franklab_tetrode_hippocampus_30KHz_tmp\"</p> <code>'franklab_tetrode_hippocampus_30KHz_tmp'</code> <code>waveform_params_name</code> <code>str</code> <p>Parameters for spike waveform extraction. If empty string, will skip automatic curation steps, by default \"default_whitened\"</p> <code>'default_whitened'</code> <code>metric_params_name</code> <code>str</code> <p>Parameters defining which QualityMetrics to calculate and how. If empty string, will skip automatic curation steps, by default \"peak_offest_num_spikes_2\"</p> <code>'peak_offest_num_spikes_2'</code> <code>auto_curation_params_name</code> <code>str</code> <p>Thresholds applied to Quality metrics for automatic unit curation. If empty string, will skip automatic curation steps, by default \"mike_noise_03_offset_2_isi_0025_mua\"</p> <code>'mike_noise_03_offset_2_isi_0025_mua'</code> Source code in <code>src/spyglass/spikesorting/spikesorting_populator.py</code> <pre><code>def spikesorting_pipeline_populator(\n    nwb_file_name: str,\n    team_name: str,\n    fig_url_repo: str = None,\n    interval_list_name: str = None,\n    sort_interval_name: str = None,\n    pipeline_parameters_name: str = None,\n    probe_restriction: dict = {},\n    artifact_parameters: str = \"ampl_2000_prop_75\",\n    preproc_params_name: str = \"franklab_tetrode_hippocampus\",\n    sorter: str = \"mountainsort4\",\n    sorter_params_name: str = \"franklab_tetrode_hippocampus_30KHz_tmp\",\n    waveform_params_name: str = \"default_whitened\",\n    metric_params_name: str = \"peak_offest_num_spikes_2\",\n    auto_curation_params_name: str = \"mike_noise_03_offset_2_isi_0025_mua\",\n):\n    \"\"\"Automatically populate the spike sorting pipeline for a given epoch\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        Session ID\n    team_name : str\n        Which team to assign the spike sorting to\n    fig_url_repo : str, optional\n        Where to store the curation figurl json files (e.g.,\n        'gh://LorenFrankLab/sorting-curations/main/user/'). Default None to\n        skip figurl\n    interval_list_name : str,\n        if sort_interval_name not provided, will create a sort interval for the\n        given interval with the same name\n    sort_interval_name : str, default None\n        if provided, will use the given sort interval, requires making this\n        interval yourself\n    pipeline_parameters_name : str, optional\n        If provided, will lookup pipeline parameters from the\n        SpikeSortingPipelineParameters table, supersedes other values provided,\n        by default None\n    probe_restriction : dict, optional\n        Restricts analysis to sort groups with matching keys. Can use keys from\n        the SortGroup and ElectrodeGroup Tables (e.g. electrode_group_name,\n        probe_id, target_hemisphere), by default {}\n    artifact_parameters : str, optional\n        parameter set for artifact detection, by default \"ampl_2000_prop_75\"\n    preproc_params_name : str, optional\n        parameter set for spikesorting recording, by default\n        \"franklab_tetrode_hippocampus\"\n    sorter : str, optional\n        which spikesorting algorithm to use, by default \"mountainsort4\"\n    sorter_params_name : str, optional\n        parameters for the spike sorting algorithm, by default\n        \"franklab_tetrode_hippocampus_30KHz_tmp\"\n    waveform_params_name : str, optional\n        Parameters for spike waveform extraction. If empty string, will skip\n        automatic curation steps, by default \"default_whitened\"\n    metric_params_name : str, optional\n        Parameters defining which QualityMetrics to calculate and how. If empty\n        string, will skip automatic curation steps, by default\n        \"peak_offest_num_spikes_2\"\n    auto_curation_params_name : str, optional\n        Thresholds applied to Quality metrics for automatic unit curation. If\n        empty string, will skip automatic curation steps, by default\n        \"mike_noise_03_offset_2_isi_0025_mua\"\n    \"\"\"\n    nwbf_dict = dict(nwb_file_name=nwb_file_name)\n    # Define pipeline parameters\n    if pipeline_parameters_name is not None:\n        print(f\"Using pipeline parameters {pipeline_parameters_name}\")\n        (\n            artifact_parameters,\n            preproc_params_name,\n            sorter,\n            sorter_params_name,\n            waveform_params_name,\n            metric_params_name,\n            auto_curation_params_name,\n        ) = (\n            SpikeSortingPipelineParameters\n            &amp; {\"pipeline_parameters_name\": pipeline_parameters_name}\n        ).fetch1(\n            \"artifact_parameters\",\n            \"preproc_params_name\",\n            \"sorter\",\n            \"sorter_params_name\",\n            \"waveform_params_name\",\n            \"metric_params_name\",\n            \"auto_curation_params_name\",\n        )\n\n    # make sort groups only if not currently available\n    # don't overwrite existing ones!\n    if not SortGroup() &amp; nwbf_dict:\n        print(\"Generating sort groups\")\n        SortGroup().set_group_by_shank(nwb_file_name)\n\n    # Define sort interval\n    interval_dict = dict(**nwbf_dict, interval_list_name=interval_list_name)\n\n    if sort_interval_name is not None:\n        print(f\"Using sort interval {sort_interval_name}\")\n        if not (\n            SortInterval\n            &amp; nwbf_dict\n            &amp; {\"sort_interval_name\": sort_interval_name}\n        ):\n            raise KeyError(f\"Sort interval {sort_interval_name} not found\")\n    else:\n        print(f\"Generating sort interval from {interval_list_name}\")\n        interval_list = (IntervalList &amp; interval_dict).fetch1(\"valid_times\")[0]\n\n        sort_interval_name = interval_list_name\n        sort_interval = interval_list\n\n        SortInterval.insert1(\n            {\n                **nwbf_dict,\n                \"sort_interval_name\": sort_interval_name,\n                \"sort_interval\": sort_interval,\n            },\n            skip_duplicates=True,\n        )\n\n    sort_dict = dict(**nwbf_dict, sort_interval_name=sort_interval_name)\n\n    # find desired sort group(s) for these settings\n    sort_group_id_list = (\n        (SortGroup.SortGroupElectrode * ElectrodeGroup)\n        &amp; nwbf_dict\n        &amp; probe_restriction\n    ).fetch(\"sort_group_id\")\n\n    # make spike sorting recording\n    print(\"Generating spike sorting recording\")\n    for sort_group_id in sort_group_id_list:\n        ssr_key = dict(\n            **sort_dict,\n            sort_group_id=sort_group_id,  # See SortGroup\n            preproc_params_name=preproc_params_name,  # See preproc_params\n            interval_list_name=interval_list_name,\n            team_name=team_name,\n        )\n        SpikeSortingRecordingSelection.insert1(ssr_key, skip_duplicates=True)\n\n    SpikeSortingRecording.populate(interval_dict)\n\n    # Artifact detection\n    print(\"Running artifact detection\")\n    artifact_keys = [\n        {**k, \"artifact_params_name\": artifact_parameters}\n        for k in (SpikeSortingRecordingSelection() &amp; interval_dict).fetch(\"KEY\")\n    ]\n    ArtifactDetectionSelection().insert(artifact_keys, skip_duplicates=True)\n    ArtifactDetection.populate(interval_dict)\n\n    # Spike sorting\n    print(\"Running spike sorting\")\n    for artifact_key in artifact_keys:\n        ss_key = dict(\n            **(ArtifactDetection &amp; artifact_key).fetch1(\"KEY\"),\n            **(ArtifactRemovedIntervalList() &amp; artifact_key).fetch1(\"KEY\"),\n            sorter=sorter,\n            sorter_params_name=sorter_params_name,\n        )\n        ss_key.pop(\"artifact_params_name\")\n        SpikeSortingSelection.insert1(ss_key, skip_duplicates=True)\n    SpikeSorting.populate(sort_dict)\n\n    # initial curation\n    print(\"Beginning curation\")\n    for sorting_key in (SpikeSorting() &amp; sort_dict).fetch(\"KEY\"):\n        Curation.insert_curation(sorting_key)\n\n    # Calculate quality metrics and perform automatic curation if specified\n    if (\n        len(waveform_params_name) &gt; 0\n        and len(metric_params_name) &gt; 0\n        and len(auto_curation_params_name) &gt; 0\n    ):\n        # Extract waveforms\n        print(\"Extracting waveforms\")\n        curation_keys = [\n            {**k, \"waveform_params_name\": waveform_params_name}\n            for k in (Curation() &amp; sort_dict).fetch(\"KEY\")\n        ]\n        WaveformSelection.insert(curation_keys, skip_duplicates=True)\n        Waveforms.populate(sort_dict)\n\n        # Quality Metrics\n        print(\"Calculating quality metrics\")\n        waveform_keys = [\n            {**k, \"metric_params_name\": metric_params_name}\n            for k in (Waveforms() &amp; sort_dict).fetch(\"KEY\")\n        ]\n        MetricSelection.insert(waveform_keys, skip_duplicates=True)\n        QualityMetrics().populate(sort_dict)\n\n        # Automatic Curation\n        print(\"Creating automatic curation\")\n        metric_keys = [\n            {**k, \"auto_curation_params_name\": auto_curation_params_name}\n            for k in (QualityMetrics() &amp; sort_dict).fetch(\"KEY\")\n        ]\n        AutomaticCurationSelection.insert(metric_keys, skip_duplicates=True)\n        AutomaticCuration().populate(sort_dict)\n\n        # Curated Spike Sorting\n        # get curation keys of the automatic curation to populate into curated\n        # spike sorting selection\n        print(\"Creating curated spike sorting\")\n        auto_key_list = (AutomaticCuration() &amp; sort_dict).fetch(\n            \"auto_curation_key\"\n        )\n        for auto_key in auto_key_list:\n            curation_auto_key = (Curation() &amp; auto_key).fetch1(\"KEY\")\n            CuratedSpikeSortingSelection.insert1(\n                curation_auto_key, skip_duplicates=True\n            )\n\n    else:\n        # Perform no automatic curation, just populate curated spike sorting\n        # selection with the initial curation. Used in case of clusterless\n        # decoding\n        print(\"Creating curated spike sorting\")\n        curation_keys = (Curation() &amp; sort_dict).fetch(\"KEY\")\n        for curation_key in curation_keys:\n            CuratedSpikeSortingSelection.insert1(\n                curation_auto_key, skip_duplicates=True\n            )\n\n    # Populate curated spike sorting\n    CuratedSpikeSorting.populate(sort_dict)\n\n    if fig_url_repo:\n        # Curation Figurl\n        print(\"Creating curation figurl\")\n        sort_interval_name = interval_list_name + \"_entire\"\n        gh_url = (\n            fig_url_repo\n            + str(nwb_file_name + \"_\" + sort_interval_name)  # session id\n            + \"/{}\"  # tetrode using auto_id['sort_group_id']\n            + \"/curation.json\"\n        )\n\n        for auto_id in (AutomaticCuration() &amp; sort_dict).fetch(\n            \"auto_curation_key\"\n        ):\n            auto_curation_out_key = dict(\n                **(Curation() &amp; auto_id).fetch1(\"KEY\"),\n                new_curation_uri=gh_url.format(str(auto_id[\"sort_group_id\"])),\n            )\n            CurationFigurlSelection.insert1(\n                auto_curation_out_key, skip_duplicates=True\n            )\n            CurationFigurl.populate(auto_curation_out_key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/", "title": "spikesorting_recording.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup", "title": "<code>SortGroup</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SortGroup(dj.Manual):\n    definition = \"\"\"\n    # Set of electrodes that will be sorted together\n    -&gt; Session\n    sort_group_id: int  # identifier for a group of electrodes\n    ---\n    sort_reference_electrode_id = -1: int  # the electrode to use for reference. -1: no reference, -2: common median\n    \"\"\"\n\n    class SortGroupElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; SortGroup\n        -&gt; Electrode\n        \"\"\"\n\n    def set_group_by_shank(\n        self,\n        nwb_file_name: str,\n        references: dict = None,\n        omit_ref_electrode_group=False,\n        omit_unitrode=True,\n    ):\n        \"\"\"Divides electrodes into groups based on their shank position.\n\n        * Electrodes from probes with 1 shank (e.g. tetrodes) are placed in a\n          single group\n        * Electrodes from probes with multiple shanks (e.g. polymer probes) are\n          placed in one group per shank\n        * Bad channels are omitted\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            the name of the NWB file whose electrodes should be put into\n            sorting groups\n        references : dict, optional\n            If passed, used to set references. Otherwise, references set using\n            original reference electrodes from config. Keys: electrode groups.\n            Values: reference electrode.\n        omit_ref_electrode_group : bool\n            Optional. If True, no sort group is defined for electrode group of\n            reference.\n        omit_unitrode : bool\n            Optional. If True, no sort groups are defined for unitrodes.\n        \"\"\"\n        # delete any current groups\n        (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n        # get the electrodes from this NWB file\n        electrodes = (\n            Electrode()\n            &amp; {\"nwb_file_name\": nwb_file_name}\n            &amp; {\"bad_channel\": \"False\"}\n        ).fetch()\n        e_groups = list(np.unique(electrodes[\"electrode_group_name\"]))\n        e_groups.sort(key=int)  # sort electrode groups numerically\n        sort_group = 0\n        sg_key = dict()\n        sge_key = dict()\n        sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n        for e_group in e_groups:\n            # for each electrode group, get a list of the unique shank numbers\n            shank_list = np.unique(\n                electrodes[\"probe_shank\"][\n                    electrodes[\"electrode_group_name\"] == e_group\n                ]\n            )\n            sge_key[\"electrode_group_name\"] = e_group\n            # get the indices of all electrodes in this group / shank and set their sorting group\n            for shank in shank_list:\n                sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = sort_group\n                # specify reference electrode. Use 'references' if passed, otherwise use reference from config\n                if not references:\n                    shank_elect_ref = electrodes[\n                        \"original_reference_electrode\"\n                    ][\n                        np.logical_and(\n                            electrodes[\"electrode_group_name\"] == e_group,\n                            electrodes[\"probe_shank\"] == shank,\n                        )\n                    ]\n                    if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n                        sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[\n                            0\n                        ]\n                    else:\n                        ValueError(\n                            f\"Error in electrode group {e_group}: reference \"\n                            + \"electrodes are not all the same\"\n                        )\n                else:\n                    if e_group not in references.keys():\n                        raise Exception(\n                            f\"electrode group {e_group} not a key in \"\n                            + \"references, so cannot set reference\"\n                        )\n                    else:\n                        sg_key[\"sort_reference_electrode_id\"] = references[\n                            e_group\n                        ]\n                # Insert sort group and sort group electrodes\n                reference_electrode_group = electrodes[\n                    electrodes[\"electrode_id\"]\n                    == sg_key[\"sort_reference_electrode_id\"]\n                ][\n                    \"electrode_group_name\"\n                ]  # reference for this electrode group\n                if (\n                    len(reference_electrode_group) == 1\n                ):  # unpack single reference\n                    reference_electrode_group = reference_electrode_group[0]\n                elif (int(sg_key[\"sort_reference_electrode_id\"]) &gt; 0) and (\n                    len(reference_electrode_group) != 1\n                ):\n                    raise Exception(\n                        \"Should have found exactly one electrode group for \"\n                        + \"reference electrode, but found \"\n                        + f\"{len(reference_electrode_group)}.\"\n                    )\n                if omit_ref_electrode_group and (\n                    str(e_group) == str(reference_electrode_group)\n                ):\n                    print(\n                        f\"Omitting electrode group {e_group} from sort groups \"\n                        + \"because contains reference.\"\n                    )\n                    continue\n                shank_elect = electrodes[\"electrode_id\"][\n                    np.logical_and(\n                        electrodes[\"electrode_group_name\"] == e_group,\n                        electrodes[\"probe_shank\"] == shank,\n                    )\n                ]\n                if (\n                    omit_unitrode and len(shank_elect) == 1\n                ):  # omit unitrodes if indicated\n                    print(\n                        f\"Omitting electrode group {e_group}, shank {shank} from sort groups because unitrode.\"\n                    )\n                    continue\n                self.insert1(sg_key)\n                for elect in shank_elect:\n                    sge_key[\"electrode_id\"] = elect\n                    self.SortGroupElectrode().insert1(sge_key)\n                sort_group += 1\n\n    def set_group_by_electrode_group(self, nwb_file_name: str):\n        \"\"\"Assign groups to all non-bad channel electrodes based on their electrode group\n        and sets the reference for each group to the reference for the first channel of the group.\n\n        Parameters\n        ----------\n        nwb_file_name: str\n            the name of the nwb whose electrodes should be put into sorting groups\n        \"\"\"\n        # delete any current groups\n        (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n        # get the electrodes from this NWB file\n        electrodes = (\n            Electrode()\n            &amp; {\"nwb_file_name\": nwb_file_name}\n            &amp; {\"bad_channel\": \"False\"}\n        ).fetch()\n        e_groups = np.unique(electrodes[\"electrode_group_name\"])\n        sg_key = dict()\n        sge_key = dict()\n        sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n        sort_group = 0\n        for e_group in e_groups:\n            sge_key[\"electrode_group_name\"] = e_group\n            # sg_key['sort_group_id'] = sge_key['sort_group_id'] = sort_group\n            # TEST\n            sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = int(e_group)\n            # get the list of references and make sure they are all the same\n            shank_elect_ref = electrodes[\"original_reference_electrode\"][\n                electrodes[\"electrode_group_name\"] == e_group\n            ]\n            if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n                sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[0]\n            else:\n                ValueError(\n                    f\"Error in electrode group {e_group}: reference electrodes are not all the same\"\n                )\n            self.insert1(sg_key)\n\n            shank_elect = electrodes[\"electrode_id\"][\n                electrodes[\"electrode_group_name\"] == e_group\n            ]\n            for elect in shank_elect:\n                sge_key[\"electrode_id\"] = elect\n                self.SortGroupElectrode().insert1(sge_key)\n            sort_group += 1\n\n    def set_reference_from_list(self, nwb_file_name, sort_group_ref_list):\n        \"\"\"\n        Set the reference electrode from a list containing sort groups and reference electrodes\n        :param: sort_group_ref_list - 2D array or list where each row is [sort_group_id reference_electrode]\n        :param: nwb_file_name - The name of the NWB file whose electrodes' references should be updated\n        :return: Null\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        sort_group_list = (SortGroup() &amp; key).fetch1()\n        for sort_group in sort_group_list:\n            key[\"sort_group_id\"] = sort_group\n            self.insert(\n                dj_replace(\n                    sort_group_list,\n                    sort_group_ref_list,\n                    \"sort_group_id\",\n                    \"sort_reference_electrode_id\",\n                ),\n                replace=\"True\",\n            )\n\n    def get_geometry(self, sort_group_id, nwb_file_name):\n        \"\"\"\n        Returns a list with the x,y coordinates of the electrodes in the sort group\n        for use with the SpikeInterface package.\n\n        Converts z locations to y where appropriate.\n\n        Parameters\n        ----------\n        sort_group_id : int\n        nwb_file_name : str\n\n        Returns\n        -------\n        geometry : list\n            List of coordinate pairs, one per electrode\n        \"\"\"\n\n        # create the channel_groups dictiorary\n        channel_group = dict()\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        electrodes = (Electrode() &amp; key).fetch()\n\n        key[\"sort_group_id\"] = sort_group_id\n        sort_group_electrodes = (SortGroup.SortGroupElectrode() &amp; key).fetch()\n        electrode_group_name = sort_group_electrodes[\"electrode_group_name\"][0]\n        probe_id = (\n            ElectrodeGroup\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"electrode_group_name\": electrode_group_name,\n            }\n        ).fetch1(\"probe_id\")\n        channel_group[sort_group_id] = dict()\n        channel_group[sort_group_id][\"channels\"] = sort_group_electrodes[\n            \"electrode_id\"\n        ].tolist()\n\n        n_chan = len(channel_group[sort_group_id][\"channels\"])\n\n        geometry = np.zeros((n_chan, 2), dtype=\"float\")\n        tmp_geom = np.zeros((n_chan, 3), dtype=\"float\")\n        for i, electrode_id in enumerate(\n            channel_group[sort_group_id][\"channels\"]\n        ):\n            # get the relative x and y locations of this channel from the probe table\n            probe_electrode = int(\n                electrodes[\"probe_electrode\"][\n                    electrodes[\"electrode_id\"] == electrode_id\n                ]\n            )\n            rel_x, rel_y, rel_z = (\n                Probe().Electrode()\n                &amp; {\"probe_id\": probe_id, \"probe_electrode\": probe_electrode}\n            ).fetch(\"rel_x\", \"rel_y\", \"rel_z\")\n            # TODO: Fix this HACK when we can use probeinterface:\n            rel_x = float(rel_x)\n            rel_y = float(rel_y)\n            rel_z = float(rel_z)\n            tmp_geom[i, :] = [rel_x, rel_y, rel_z]\n\n        # figure out which columns have coordinates\n        n_found = 0\n        for i in range(3):\n            if np.any(np.nonzero(tmp_geom[:, i])):\n                if n_found &lt; 2:\n                    geometry[:, n_found] = tmp_geom[:, i]\n                    n_found += 1\n                else:\n                    Warning(\n                        \"Relative electrode locations have three coordinates; only two are currently supported\"\n                    )\n        return np.ndarray.tolist(geometry)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.set_group_by_shank", "title": "<code>set_group_by_shank(nwb_file_name, references=None, omit_ref_electrode_group=False, omit_unitrode=True)</code>", "text": "<p>Divides electrodes into groups based on their shank position.</p> <ul> <li>Electrodes from probes with 1 shank (e.g. tetrodes) are placed in a   single group</li> <li>Electrodes from probes with multiple shanks (e.g. polymer probes) are   placed in one group per shank</li> <li>Bad channels are omitted</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>the name of the NWB file whose electrodes should be put into sorting groups</p> required <code>references</code> <code>dict</code> <p>If passed, used to set references. Otherwise, references set using original reference electrodes from config. Keys: electrode groups. Values: reference electrode.</p> <code>None</code> <code>omit_ref_electrode_group</code> <code>bool</code> <p>Optional. If True, no sort group is defined for electrode group of reference.</p> <code>False</code> <code>omit_unitrode</code> <code>bool</code> <p>Optional. If True, no sort groups are defined for unitrodes.</p> <code>True</code> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def set_group_by_shank(\n    self,\n    nwb_file_name: str,\n    references: dict = None,\n    omit_ref_electrode_group=False,\n    omit_unitrode=True,\n):\n    \"\"\"Divides electrodes into groups based on their shank position.\n\n    * Electrodes from probes with 1 shank (e.g. tetrodes) are placed in a\n      single group\n    * Electrodes from probes with multiple shanks (e.g. polymer probes) are\n      placed in one group per shank\n    * Bad channels are omitted\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        the name of the NWB file whose electrodes should be put into\n        sorting groups\n    references : dict, optional\n        If passed, used to set references. Otherwise, references set using\n        original reference electrodes from config. Keys: electrode groups.\n        Values: reference electrode.\n    omit_ref_electrode_group : bool\n        Optional. If True, no sort group is defined for electrode group of\n        reference.\n    omit_unitrode : bool\n        Optional. If True, no sort groups are defined for unitrodes.\n    \"\"\"\n    # delete any current groups\n    (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n    # get the electrodes from this NWB file\n    electrodes = (\n        Electrode()\n        &amp; {\"nwb_file_name\": nwb_file_name}\n        &amp; {\"bad_channel\": \"False\"}\n    ).fetch()\n    e_groups = list(np.unique(electrodes[\"electrode_group_name\"]))\n    e_groups.sort(key=int)  # sort electrode groups numerically\n    sort_group = 0\n    sg_key = dict()\n    sge_key = dict()\n    sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n    for e_group in e_groups:\n        # for each electrode group, get a list of the unique shank numbers\n        shank_list = np.unique(\n            electrodes[\"probe_shank\"][\n                electrodes[\"electrode_group_name\"] == e_group\n            ]\n        )\n        sge_key[\"electrode_group_name\"] = e_group\n        # get the indices of all electrodes in this group / shank and set their sorting group\n        for shank in shank_list:\n            sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = sort_group\n            # specify reference electrode. Use 'references' if passed, otherwise use reference from config\n            if not references:\n                shank_elect_ref = electrodes[\n                    \"original_reference_electrode\"\n                ][\n                    np.logical_and(\n                        electrodes[\"electrode_group_name\"] == e_group,\n                        electrodes[\"probe_shank\"] == shank,\n                    )\n                ]\n                if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n                    sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[\n                        0\n                    ]\n                else:\n                    ValueError(\n                        f\"Error in electrode group {e_group}: reference \"\n                        + \"electrodes are not all the same\"\n                    )\n            else:\n                if e_group not in references.keys():\n                    raise Exception(\n                        f\"electrode group {e_group} not a key in \"\n                        + \"references, so cannot set reference\"\n                    )\n                else:\n                    sg_key[\"sort_reference_electrode_id\"] = references[\n                        e_group\n                    ]\n            # Insert sort group and sort group electrodes\n            reference_electrode_group = electrodes[\n                electrodes[\"electrode_id\"]\n                == sg_key[\"sort_reference_electrode_id\"]\n            ][\n                \"electrode_group_name\"\n            ]  # reference for this electrode group\n            if (\n                len(reference_electrode_group) == 1\n            ):  # unpack single reference\n                reference_electrode_group = reference_electrode_group[0]\n            elif (int(sg_key[\"sort_reference_electrode_id\"]) &gt; 0) and (\n                len(reference_electrode_group) != 1\n            ):\n                raise Exception(\n                    \"Should have found exactly one electrode group for \"\n                    + \"reference electrode, but found \"\n                    + f\"{len(reference_electrode_group)}.\"\n                )\n            if omit_ref_electrode_group and (\n                str(e_group) == str(reference_electrode_group)\n            ):\n                print(\n                    f\"Omitting electrode group {e_group} from sort groups \"\n                    + \"because contains reference.\"\n                )\n                continue\n            shank_elect = electrodes[\"electrode_id\"][\n                np.logical_and(\n                    electrodes[\"electrode_group_name\"] == e_group,\n                    electrodes[\"probe_shank\"] == shank,\n                )\n            ]\n            if (\n                omit_unitrode and len(shank_elect) == 1\n            ):  # omit unitrodes if indicated\n                print(\n                    f\"Omitting electrode group {e_group}, shank {shank} from sort groups because unitrode.\"\n                )\n                continue\n            self.insert1(sg_key)\n            for elect in shank_elect:\n                sge_key[\"electrode_id\"] = elect\n                self.SortGroupElectrode().insert1(sge_key)\n            sort_group += 1\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.set_group_by_electrode_group", "title": "<code>set_group_by_electrode_group(nwb_file_name)</code>", "text": "<p>Assign groups to all non-bad channel electrodes based on their electrode group and sets the reference for each group to the reference for the first channel of the group.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>the name of the nwb whose electrodes should be put into sorting groups</p> required Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def set_group_by_electrode_group(self, nwb_file_name: str):\n    \"\"\"Assign groups to all non-bad channel electrodes based on their electrode group\n    and sets the reference for each group to the reference for the first channel of the group.\n\n    Parameters\n    ----------\n    nwb_file_name: str\n        the name of the nwb whose electrodes should be put into sorting groups\n    \"\"\"\n    # delete any current groups\n    (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n    # get the electrodes from this NWB file\n    electrodes = (\n        Electrode()\n        &amp; {\"nwb_file_name\": nwb_file_name}\n        &amp; {\"bad_channel\": \"False\"}\n    ).fetch()\n    e_groups = np.unique(electrodes[\"electrode_group_name\"])\n    sg_key = dict()\n    sge_key = dict()\n    sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n    sort_group = 0\n    for e_group in e_groups:\n        sge_key[\"electrode_group_name\"] = e_group\n        # sg_key['sort_group_id'] = sge_key['sort_group_id'] = sort_group\n        # TEST\n        sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = int(e_group)\n        # get the list of references and make sure they are all the same\n        shank_elect_ref = electrodes[\"original_reference_electrode\"][\n            electrodes[\"electrode_group_name\"] == e_group\n        ]\n        if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n            sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[0]\n        else:\n            ValueError(\n                f\"Error in electrode group {e_group}: reference electrodes are not all the same\"\n            )\n        self.insert1(sg_key)\n\n        shank_elect = electrodes[\"electrode_id\"][\n            electrodes[\"electrode_group_name\"] == e_group\n        ]\n        for elect in shank_elect:\n            sge_key[\"electrode_id\"] = elect\n            self.SortGroupElectrode().insert1(sge_key)\n        sort_group += 1\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.set_reference_from_list", "title": "<code>set_reference_from_list(nwb_file_name, sort_group_ref_list)</code>", "text": "<p>Set the reference electrode from a list containing sort groups and reference electrodes :param: sort_group_ref_list - 2D array or list where each row is [sort_group_id reference_electrode] :param: nwb_file_name - The name of the NWB file whose electrodes' references should be updated :return: Null</p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def set_reference_from_list(self, nwb_file_name, sort_group_ref_list):\n    \"\"\"\n    Set the reference electrode from a list containing sort groups and reference electrodes\n    :param: sort_group_ref_list - 2D array or list where each row is [sort_group_id reference_electrode]\n    :param: nwb_file_name - The name of the NWB file whose electrodes' references should be updated\n    :return: Null\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    sort_group_list = (SortGroup() &amp; key).fetch1()\n    for sort_group in sort_group_list:\n        key[\"sort_group_id\"] = sort_group\n        self.insert(\n            dj_replace(\n                sort_group_list,\n                sort_group_ref_list,\n                \"sort_group_id\",\n                \"sort_reference_electrode_id\",\n            ),\n            replace=\"True\",\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.get_geometry", "title": "<code>get_geometry(sort_group_id, nwb_file_name)</code>", "text": "<p>Returns a list with the x,y coordinates of the electrodes in the sort group for use with the SpikeInterface package.</p> <p>Converts z locations to y where appropriate.</p> <p>Parameters:</p> Name Type Description Default <code>sort_group_id</code> <code>int</code> required <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>geometry</code> <code>list</code> <p>List of coordinate pairs, one per electrode</p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def get_geometry(self, sort_group_id, nwb_file_name):\n    \"\"\"\n    Returns a list with the x,y coordinates of the electrodes in the sort group\n    for use with the SpikeInterface package.\n\n    Converts z locations to y where appropriate.\n\n    Parameters\n    ----------\n    sort_group_id : int\n    nwb_file_name : str\n\n    Returns\n    -------\n    geometry : list\n        List of coordinate pairs, one per electrode\n    \"\"\"\n\n    # create the channel_groups dictiorary\n    channel_group = dict()\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    electrodes = (Electrode() &amp; key).fetch()\n\n    key[\"sort_group_id\"] = sort_group_id\n    sort_group_electrodes = (SortGroup.SortGroupElectrode() &amp; key).fetch()\n    electrode_group_name = sort_group_electrodes[\"electrode_group_name\"][0]\n    probe_id = (\n        ElectrodeGroup\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"electrode_group_name\": electrode_group_name,\n        }\n    ).fetch1(\"probe_id\")\n    channel_group[sort_group_id] = dict()\n    channel_group[sort_group_id][\"channels\"] = sort_group_electrodes[\n        \"electrode_id\"\n    ].tolist()\n\n    n_chan = len(channel_group[sort_group_id][\"channels\"])\n\n    geometry = np.zeros((n_chan, 2), dtype=\"float\")\n    tmp_geom = np.zeros((n_chan, 3), dtype=\"float\")\n    for i, electrode_id in enumerate(\n        channel_group[sort_group_id][\"channels\"]\n    ):\n        # get the relative x and y locations of this channel from the probe table\n        probe_electrode = int(\n            electrodes[\"probe_electrode\"][\n                electrodes[\"electrode_id\"] == electrode_id\n            ]\n        )\n        rel_x, rel_y, rel_z = (\n            Probe().Electrode()\n            &amp; {\"probe_id\": probe_id, \"probe_electrode\": probe_electrode}\n        ).fetch(\"rel_x\", \"rel_y\", \"rel_z\")\n        # TODO: Fix this HACK when we can use probeinterface:\n        rel_x = float(rel_x)\n        rel_y = float(rel_y)\n        rel_z = float(rel_z)\n        tmp_geom[i, :] = [rel_x, rel_y, rel_z]\n\n    # figure out which columns have coordinates\n    n_found = 0\n    for i in range(3):\n        if np.any(np.nonzero(tmp_geom[:, i])):\n            if n_found &lt; 2:\n                geometry[:, n_found] = tmp_geom[:, i]\n                n_found += 1\n            else:\n                Warning(\n                    \"Relative electrode locations have three coordinates; only two are currently supported\"\n                )\n    return np.ndarray.tolist(geometry)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        # Path to files that will hold the recording extractors\n        recording_path = str(recording_dir / Path(recording_name))\n        if os.path.exists(recording_path):\n            shutil.rmtree(recording_path)\n\n        recording.save(\n            folder=recording_path, chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        IntervalList.insert1(\n            {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": recording_name,\n                \"valid_times\": sort_interval_valid_times,\n            },\n            replace=True,\n        )\n\n        self.insert1(\n            {\n                **key,\n                # store the list of valid times for the sort\n                \"sort_interval_list_name\": recording_name,\n                \"recording_path\": recording_path,\n            }\n        )\n\n    @staticmethod\n    def _get_recording_name(key):\n        return \"_\".join(\n            [\n                key[\"nwb_file_name\"],\n                key[\"sort_interval_name\"],\n                str(key[\"sort_group_id\"]),\n                key[\"preproc_params_name\"],\n            ]\n        )\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        num_segments = recording.get_num_segments()\n\n        if num_segments &lt;= 1:\n            return recording.get_times()\n\n        frames_per_segment = [0] + [\n            recording.get_num_frames(segment_index=i)\n            for i in range(num_segments)\n        ]\n\n        cumsum_frames = np.cumsum(frames_per_segment)\n        total_frames = np.sum(frames_per_segment)\n\n        timestamps = np.zeros((total_frames,))\n        for i in range(num_segments):\n            start_index = cumsum_frames[i]\n            end_index = cumsum_frames[i + 1]\n            timestamps[start_index:end_index] = recording.get_times(\n                segment_index=i\n            )\n\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n        \"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n        \"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/", "title": "spikesorting_sorting.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorterParameters", "title": "<code>SpikeSorterParameters</code>", "text": "<p>             Bases: <code>Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>@schema\nclass SpikeSorterParameters(dj.Manual):\n    definition = \"\"\"\n    sorter: varchar(200)\n    sorter_params_name: varchar(200)\n    ---\n    sorter_params: blob\n    \"\"\"\n\n    def insert_default(self):\n        \"\"\"Default params from spike sorters available via spikeinterface\"\"\"\n        sorters = sis.available_sorters()\n        for sorter in sorters:\n            sorter_params = sis.get_default_sorter_params(sorter)\n            self.insert1(\n                [sorter, \"default\", sorter_params], skip_duplicates=True\n            )\n\n        # Insert Frank lab defaults\n        # Hippocampus tetrode default\n        sorter = \"mountainsort4\"\n        sorter_params_name = \"franklab_tetrode_hippocampus_30KHz\"\n        sorter_params = {\n            \"detect_sign\": -1,\n            \"adjacency_radius\": 100,\n            \"freq_min\": 600,\n            \"freq_max\": 6000,\n            \"filter\": False,\n            \"whiten\": True,\n            \"num_workers\": 1,\n            \"clip_size\": 40,\n            \"detect_threshold\": 3,\n            \"detect_interval\": 10,\n        }\n        self.insert1(\n            [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n        )\n\n        # Cortical probe default\n        sorter = \"mountainsort4\"\n        sorter_params_name = \"franklab_probe_ctx_30KHz\"\n        sorter_params = {\n            \"detect_sign\": -1,\n            \"adjacency_radius\": 100,\n            \"freq_min\": 300,\n            \"freq_max\": 6000,\n            \"filter\": False,\n            \"whiten\": True,\n            \"num_workers\": 1,\n            \"clip_size\": 40,\n            \"detect_threshold\": 3,\n            \"detect_interval\": 10,\n        }\n        self.insert1(\n            [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n        )\n\n        # clusterless defaults\n        sorter = \"clusterless_thresholder\"\n        sorter_params_name = \"default_clusterless\"\n        sorter_params = dict(\n            detect_threshold=100.0,  # uV\n            # Locally exclusive means one unit per spike detected\n            method=\"locally_exclusive\",\n            peak_sign=\"neg\",\n            exclude_sweep_ms=0.1,\n            local_radius_um=100,\n            # noise levels needs to be 1.0 so the units are in uV and not MAD\n            noise_levels=np.asarray([1.0]),\n            random_chunk_kwargs={},\n            # output needs to be set to sorting for the rest of the pipeline\n            outputs=\"sorting\",\n        )\n        self.insert1(\n            [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorterParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Default params from spike sorters available via spikeinterface</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def insert_default(self):\n    \"\"\"Default params from spike sorters available via spikeinterface\"\"\"\n    sorters = sis.available_sorters()\n    for sorter in sorters:\n        sorter_params = sis.get_default_sorter_params(sorter)\n        self.insert1(\n            [sorter, \"default\", sorter_params], skip_duplicates=True\n        )\n\n    # Insert Frank lab defaults\n    # Hippocampus tetrode default\n    sorter = \"mountainsort4\"\n    sorter_params_name = \"franklab_tetrode_hippocampus_30KHz\"\n    sorter_params = {\n        \"detect_sign\": -1,\n        \"adjacency_radius\": 100,\n        \"freq_min\": 600,\n        \"freq_max\": 6000,\n        \"filter\": False,\n        \"whiten\": True,\n        \"num_workers\": 1,\n        \"clip_size\": 40,\n        \"detect_threshold\": 3,\n        \"detect_interval\": 10,\n    }\n    self.insert1(\n        [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n    )\n\n    # Cortical probe default\n    sorter = \"mountainsort4\"\n    sorter_params_name = \"franklab_probe_ctx_30KHz\"\n    sorter_params = {\n        \"detect_sign\": -1,\n        \"adjacency_radius\": 100,\n        \"freq_min\": 300,\n        \"freq_max\": 6000,\n        \"filter\": False,\n        \"whiten\": True,\n        \"num_workers\": 1,\n        \"clip_size\": 40,\n        \"detect_threshold\": 3,\n        \"detect_interval\": 10,\n    }\n    self.insert1(\n        [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n    )\n\n    # clusterless defaults\n    sorter = \"clusterless_thresholder\"\n    sorter_params_name = \"default_clusterless\"\n    sorter_params = dict(\n        detect_threshold=100.0,  # uV\n        # Locally exclusive means one unit per spike detected\n        method=\"locally_exclusive\",\n        peak_sign=\"neg\",\n        exclude_sweep_ms=0.1,\n        local_radius_um=100,\n        # noise levels needs to be 1.0 so the units are in uV and not MAD\n        noise_levels=np.asarray([1.0]),\n        random_chunk_kwargs={},\n        # output needs to be set to sorting for the rest of the pipeline\n        outputs=\"sorting\",\n    )\n    self.insert1(\n        [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n    )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting", "title": "<code>SpikeSorting</code>", "text": "<p>             Bases: <code>Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>@schema\nclass SpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingSelection\n    ---\n    sorting_path: varchar(1000)\n    time_of_sort: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    def make(self, key: dict):\n        \"\"\"Runs spike sorting on the data and parameters specified by the\n        SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n        Specifically,\n        1. Loads saved recording and runs the sort on it with spikeinterface\n        2. Saves the sorting with spikeinterface\n        3. Creates an analysis NWB file and saves the sorting there\n           (this is redundant with 2; will change in the future)\n\n        \"\"\"\n        # CBroz: does this not work w/o arg? as .populate() ?\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        recording = si.load_extractor(recording_path)\n\n        # first, get the timestamps\n        timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n        _ = recording.get_sampling_frequency()\n        # then concatenate the recordings\n        # Note: the timestamps are lost upon concatenation,\n        # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n        # but concat_recording.recoring_list[i].get_times() will return correct\n        # timestamps for ith recording.\n        if recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.AppendSegmentRecording\n        ):\n            recording = si.concatenate_recordings(recording.recording_list)\n        elif recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.BinaryRecordingExtractor\n        ):\n            recording = si.concatenate_recordings([recording])\n\n        # load artifact intervals\n        artifact_times = (\n            ArtifactRemovedIntervalList\n            &amp; {\n                \"artifact_removed_interval_list_name\": key[\n                    \"artifact_removed_interval_list_name\"\n                ]\n            }\n        ).fetch1(\"artifact_times\")\n        if len(artifact_times):\n            if artifact_times.ndim == 1:\n                artifact_times = np.expand_dims(artifact_times, 0)\n\n            # convert artifact intervals to indices\n            list_triggers = []\n            for interval in artifact_times:\n                list_triggers.append(\n                    np.arange(\n                        np.searchsorted(timestamps, interval[0]),\n                        np.searchsorted(timestamps, interval[1]),\n                    )\n                )\n            list_triggers = [list(np.concatenate(list_triggers))]\n            recording = sip.remove_artifacts(\n                recording=recording,\n                list_triggers=list_triggers,\n                ms_before=None,\n                ms_after=None,\n                mode=\"zeros\",\n            )\n\n        print(f\"Running spike sorting on {key}...\")\n        sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n            \"sorter\", \"sorter_params\"\n        )\n\n        sorter_temp_dir = tempfile.TemporaryDirectory(dir=temp_dir)\n        # add tempdir option for mountainsort\n        sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n        if sorter == \"clusterless_thresholder\":\n            # need to remove tempdir and whiten from sorter_params\n            sorter_params.pop(\"tempdir\", None)\n            sorter_params.pop(\"whiten\", None)\n            sorter_params.pop(\"outputs\", None)\n\n            # Detect peaks for clusterless decoding\n            detected_spikes = detect_peaks(recording, **sorter_params)\n            sorting = si.NumpySorting.from_times_labels(\n                times_list=detected_spikes[\"sample_index\"],\n                labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n                sampling_frequency=recording.get_sampling_frequency(),\n            )\n        else:\n            if \"whiten\" in sorter_params.keys():\n                if sorter_params[\"whiten\"]:\n                    sorter_params[\"whiten\"] = False  # set whiten to False\n            # whiten recording separately; make sure dtype is float32\n            # to avoid downstream error with svd\n            recording = sip.whiten(recording, dtype=\"float32\")\n            sorting = sis.run_sorter(\n                sorter,\n                recording,\n                output_folder=sorter_temp_dir.name,\n                delete_output_folder=True,\n                **sorter_params,\n            )\n        key[\"time_of_sort\"] = int(time.time())\n\n        print(\"Saving sorting results...\")\n\n        sorting_folder = Path(sorting_dir)\n\n        sorting_name = self._get_sorting_name(key)\n        key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n        if os.path.exists(key[\"sorting_path\"]):\n            shutil.rmtree(key[\"sorting_path\"])\n        sorting = sorting.save(folder=key[\"sorting_path\"])\n        self.insert1(key)\n\n    def delete(self):\n        \"\"\"Extends the delete method of base class to implement permission checking.\n        Note that this is NOT a security feature, as anyone that has access to source code\n        can disable it; it just makes it less likely to accidentally delete entries.\n        \"\"\"\n        current_user_name = dj.config[\"database.user\"]\n        entries = self.fetch()\n        permission_bool = np.zeros((len(entries),))\n        print(\n            f\"Attempting to delete {len(entries)} entries, checking permission...\"\n        )\n\n        for entry_idx in range(len(entries)):\n            # check the team name for the entry, then look up the members in that team,\n            # then get their datajoint user names\n            team_name = (\n                SpikeSortingRecordingSelection\n                &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n            ).fetch1()[\"team_name\"]\n            lab_member_name_list = (\n                LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n            ).fetch(\"lab_member_name\")\n            datajoint_user_names = []\n            for lab_member_name in lab_member_name_list:\n                datajoint_user_names.append(\n                    (\n                        LabMember.LabMemberInfo\n                        &amp; {\"lab_member_name\": lab_member_name}\n                    ).fetch1(\"datajoint_user_name\")\n                )\n            permission_bool[entry_idx] = (\n                current_user_name in datajoint_user_names\n            )\n        if np.sum(permission_bool) == len(entries):\n            print(\"Permission to delete all specified entries granted.\")\n            super().delete()\n        else:\n            raise Exception(\n                \"You do not have permission to delete all specified\"\n                \"entries. Not deleting anything.\"\n            )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        raise NotImplementedError\n        return None\n        # return fetch_nwb(self, (AnalysisNwbfile, 'analysis_file_abs_path'), *attrs, **kwargs)\n\n    def nightly_cleanup(self):\n        \"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n        This should be run after AnalysisNwbFile().nightly_cleanup()\n        \"\"\"\n        # get a list of the files in the spike sorting storage directory\n        dir_names = next(os.walk(sorting_dir))[1]\n        # now retrieve a list of the currently used analysis nwb files\n        analysis_file_names = self.fetch(\"analysis_file_name\")\n        for dir in dir_names:\n            if dir not in analysis_file_names:\n                full_path = str(Path(sorting_dir) / dir)\n                print(f\"removing {full_path}\")\n                shutil.rmtree(str(Path(sorting_dir) / dir))\n\n    @staticmethod\n    def _get_sorting_name(key):\n        recording_name = SpikeSortingRecording._get_recording_name(key)\n        sorting_name = (\n            recording_name + \"_\" + str(uuid.uuid4())[0:8] + \"_spikesorting\"\n        )\n        return sorting_name\n\n    # TODO: write a function to import sorting done outside of dj\n\n    def _import_sorting(self, key):\n        raise NotImplementedError\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.make", "title": "<code>make(key)</code>", "text": "<p>Runs spike sorting on the data and parameters specified by the SpikeSortingSelection table and inserts a new entry to SpikeSorting table.</p> <p>Specifically, 1. Loads saved recording and runs the sort on it with spikeinterface 2. Saves the sorting with spikeinterface 3. Creates an analysis NWB file and saves the sorting there    (this is redundant with 2; will change in the future)</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def make(self, key: dict):\n    \"\"\"Runs spike sorting on the data and parameters specified by the\n    SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n    Specifically,\n    1. Loads saved recording and runs the sort on it with spikeinterface\n    2. Saves the sorting with spikeinterface\n    3. Creates an analysis NWB file and saves the sorting there\n       (this is redundant with 2; will change in the future)\n\n    \"\"\"\n    # CBroz: does this not work w/o arg? as .populate() ?\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    recording = si.load_extractor(recording_path)\n\n    # first, get the timestamps\n    timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n    _ = recording.get_sampling_frequency()\n    # then concatenate the recordings\n    # Note: the timestamps are lost upon concatenation,\n    # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n    # but concat_recording.recoring_list[i].get_times() will return correct\n    # timestamps for ith recording.\n    if recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.AppendSegmentRecording\n    ):\n        recording = si.concatenate_recordings(recording.recording_list)\n    elif recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.BinaryRecordingExtractor\n    ):\n        recording = si.concatenate_recordings([recording])\n\n    # load artifact intervals\n    artifact_times = (\n        ArtifactRemovedIntervalList\n        &amp; {\n            \"artifact_removed_interval_list_name\": key[\n                \"artifact_removed_interval_list_name\"\n            ]\n        }\n    ).fetch1(\"artifact_times\")\n    if len(artifact_times):\n        if artifact_times.ndim == 1:\n            artifact_times = np.expand_dims(artifact_times, 0)\n\n        # convert artifact intervals to indices\n        list_triggers = []\n        for interval in artifact_times:\n            list_triggers.append(\n                np.arange(\n                    np.searchsorted(timestamps, interval[0]),\n                    np.searchsorted(timestamps, interval[1]),\n                )\n            )\n        list_triggers = [list(np.concatenate(list_triggers))]\n        recording = sip.remove_artifacts(\n            recording=recording,\n            list_triggers=list_triggers,\n            ms_before=None,\n            ms_after=None,\n            mode=\"zeros\",\n        )\n\n    print(f\"Running spike sorting on {key}...\")\n    sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n        \"sorter\", \"sorter_params\"\n    )\n\n    sorter_temp_dir = tempfile.TemporaryDirectory(dir=temp_dir)\n    # add tempdir option for mountainsort\n    sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n    if sorter == \"clusterless_thresholder\":\n        # need to remove tempdir and whiten from sorter_params\n        sorter_params.pop(\"tempdir\", None)\n        sorter_params.pop(\"whiten\", None)\n        sorter_params.pop(\"outputs\", None)\n\n        # Detect peaks for clusterless decoding\n        detected_spikes = detect_peaks(recording, **sorter_params)\n        sorting = si.NumpySorting.from_times_labels(\n            times_list=detected_spikes[\"sample_index\"],\n            labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n            sampling_frequency=recording.get_sampling_frequency(),\n        )\n    else:\n        if \"whiten\" in sorter_params.keys():\n            if sorter_params[\"whiten\"]:\n                sorter_params[\"whiten\"] = False  # set whiten to False\n        # whiten recording separately; make sure dtype is float32\n        # to avoid downstream error with svd\n        recording = sip.whiten(recording, dtype=\"float32\")\n        sorting = sis.run_sorter(\n            sorter,\n            recording,\n            output_folder=sorter_temp_dir.name,\n            delete_output_folder=True,\n            **sorter_params,\n        )\n    key[\"time_of_sort\"] = int(time.time())\n\n    print(\"Saving sorting results...\")\n\n    sorting_folder = Path(sorting_dir)\n\n    sorting_name = self._get_sorting_name(key)\n    key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n    if os.path.exists(key[\"sorting_path\"]):\n        shutil.rmtree(key[\"sorting_path\"])\n    sorting = sorting.save(folder=key[\"sorting_path\"])\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.delete", "title": "<code>delete()</code>", "text": "<p>Extends the delete method of base class to implement permission checking. Note that this is NOT a security feature, as anyone that has access to source code can disable it; it just makes it less likely to accidentally delete entries.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def delete(self):\n    \"\"\"Extends the delete method of base class to implement permission checking.\n    Note that this is NOT a security feature, as anyone that has access to source code\n    can disable it; it just makes it less likely to accidentally delete entries.\n    \"\"\"\n    current_user_name = dj.config[\"database.user\"]\n    entries = self.fetch()\n    permission_bool = np.zeros((len(entries),))\n    print(\n        f\"Attempting to delete {len(entries)} entries, checking permission...\"\n    )\n\n    for entry_idx in range(len(entries)):\n        # check the team name for the entry, then look up the members in that team,\n        # then get their datajoint user names\n        team_name = (\n            SpikeSortingRecordingSelection\n            &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n        ).fetch1()[\"team_name\"]\n        lab_member_name_list = (\n            LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n        ).fetch(\"lab_member_name\")\n        datajoint_user_names = []\n        for lab_member_name in lab_member_name_list:\n            datajoint_user_names.append(\n                (\n                    LabMember.LabMemberInfo\n                    &amp; {\"lab_member_name\": lab_member_name}\n                ).fetch1(\"datajoint_user_name\")\n            )\n        permission_bool[entry_idx] = (\n            current_user_name in datajoint_user_names\n        )\n    if np.sum(permission_bool) == len(entries):\n        print(\"Permission to delete all specified entries granted.\")\n        super().delete()\n    else:\n        raise Exception(\n            \"You do not have permission to delete all specified\"\n            \"entries. Not deleting anything.\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.nightly_cleanup", "title": "<code>nightly_cleanup()</code>", "text": "<p>Clean up spike sorting directories that are not in the SpikeSorting table. This should be run after AnalysisNwbFile().nightly_cleanup()</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def nightly_cleanup(self):\n    \"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n    This should be run after AnalysisNwbFile().nightly_cleanup()\n    \"\"\"\n    # get a list of the files in the spike sorting storage directory\n    dir_names = next(os.walk(sorting_dir))[1]\n    # now retrieve a list of the currently used analysis nwb files\n    analysis_file_names = self.fetch(\"analysis_file_name\")\n    for dir in dir_names:\n        if dir not in analysis_file_names:\n            full_path = str(Path(sorting_dir) / dir)\n            print(f\"removing {full_path}\")\n            shutil.rmtree(str(Path(sorting_dir) / dir))\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/", "title": "database_settings.py", "text": ""}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings", "title": "<code>DatabaseSettings</code>", "text": "Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>class DatabaseSettings:\n    def __init__(\n        self, user_name=None, host_name=None, target_group=None, debug=False\n    ):\n        \"\"\"Class to manage common database settings\n\n        Parameters\n        ----------\n        user_name : str, optional\n            The name of the user to add to the database. Default from dj.config\n        host_name : str, optional\n            The name of the host to add to the database. Default from dj.config\n        target_group : str, optional\n            Group to which user belongs. Default is kachery-users\n        debug : bool, optional\n            Default False. If True, print sql instead of running\n        \"\"\"\n        self.shared_modules = [\n            f\"common{ESC}\",\n            f\"spikesorting{ESC}\",\n            f\"decoding{ESC}\",\n            f\"position{ESC}\",\n            f\"position_linearization{ESC}\",\n            f\"ripple{ESC}\",\n            f\"lfp{ESC}\",\n        ]\n        self.user = user_name or dj.config[\"database.user\"]\n        self.host = (\n            host_name or dj.config[\"database.host\"] or \"lmf-db.cin.ucsf.edu\"\n        )\n        self.target_group = target_group or \"kachery-users\"\n        self.debug = debug\n\n    @property\n    def _add_collab_usr_sql(self):\n        return [\n            # Create the user (if not already created) and set the password\n            f\"{CREATE_USR}'{self.user}'@'%'{TEMP_PASS}\\n\",\n            # Grant privileges to databases matching the user_name pattern\n            f\"{GRANT_ALL}`{self.user}{ESC}`.* TO '{self.user}'@'%';\\n\",\n            # Grant SELECT privileges on all databases\n            f\"{GRANT_SEL}`%`.* TO '{self.user}'@'%';\\n\",\n        ]\n\n    def add_collab_user(self):\n        \"\"\"Add collaborator user with full permissions to shared modules\"\"\"\n        file = self.write_temp_file(self._add_collab_usr_sql)\n        self.run_file(file)\n\n    @property\n    def _add_dj_guest_sql(self):\n        return [\n            # Create the user (if not already created) and set the password\n            f\"{CREATE_USR}'{self.user}'@'%' IDENTIFIED BY 'Data_$haring';\\n\",\n            # Grant privileges\n            f\"{GRANT_SEL}`%`.* TO '{self.user}'@'%';\\n\",\n        ]\n\n    def add_dj_guest(self):\n        \"\"\"Add guest user with select permissions to shared modules\"\"\"\n        file = self.write_temp_file(self._add_dj_guest_sql)\n        self.run_file(file)\n\n    def _find_group(self):\n        # find the kachery-users group\n        groups = grp.getgrall()\n        group_found = False  # initialize the flag as False\n        for group in groups:\n            if group.gr_name == self.target_group:\n                group_found = (\n                    True  # set the flag to True when the group is found\n                )\n                break\n\n        # Check if the group was found\n        if not group_found:\n            if self.debug:\n                print(f\"All groups: {[g.gr_name for g in groups]}\")\n            sys.exit(\n                f\"Error: The target group {self.target_group} was not found.\"\n            )\n\n        return group\n\n    def _add_module_sql(self, module_name, group):\n        return [\n            f\"{GRANT_ALL}`{module_name}{ESC}`.* TO `{user}`@'%';\\n\"\n            # get a list of usernames\n            for user in group.gr_mem\n        ]\n\n    def add_module(self, module_name):\n        \"\"\"Add module to database. Grant permissions to all users in group\"\"\"\n        print(f\"Granting everyone permissions to module {module_name}\")\n        group = self._find_group()\n        file = self.write_temp_file(self._add_module_sql(module_name, group))\n        self.run_file(file)\n\n    @property\n    def _add_dj_user_sql(self):\n        return (\n            [\n                f\"{CREATE_USR}'{self.user}'@'%' \"\n                + \"IDENTIFIED BY 'temppass';\\n\",\n                f\"{GRANT_ALL}`{self.user}{ESC}`.* TO '{self.user}'@'%';\" + \"\\n\",\n            ]\n            + [\n                f\"{GRANT_ALL}`{module}`.* TO '{self.user}'@'%';\\n\"\n                for module in self.shared_modules\n            ]\n            + [f\"{GRANT_SEL}`%`.* TO '{self.user}'@'%';\\n\"]\n        )\n\n    def add_dj_user(self, check_exists=True):\n        \"\"\"Add user to database with permissions to shared modules\"\"\"\n        if check_exists:\n            user_home = Path.home().parent / self.user\n            if user_home.exists():\n                print(\"Creating database user \", self.user)\n            else:\n                sys.exit(\n                    f\"Error: could not find {self.user} in home dir: {user_home}\"\n                )\n\n        file = self.write_temp_file(self._add_dj_user_sql)\n        self.run_file(file)\n\n    def write_temp_file(self, content: list) -&gt; tempfile.NamedTemporaryFile:\n        \"\"\"Write content to a temporary file and return the file object\"\"\"\n        file = tempfile.NamedTemporaryFile(mode=\"w\")\n        for line in content:\n            file.write(line)\n        file.flush()\n\n        if self.debug:\n            from pprint import pprint  # noqa F401\n\n            pprint(file.name)\n            pprint(content)\n\n        return file\n\n    def run_file(self, file):\n        \"\"\"Run commands saved to file in sql\"\"\"\n\n        if self.debug:\n            return\n\n        os.system(f\"mysql -p -h {self.host} &lt; {file.name}\")\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings.__init__", "title": "<code>__init__(user_name=None, host_name=None, target_group=None, debug=False)</code>", "text": "<p>Class to manage common database settings</p> <p>Parameters:</p> Name Type Description Default <code>user_name</code> <code>str</code> <p>The name of the user to add to the database. Default from dj.config</p> <code>None</code> <code>host_name</code> <code>str</code> <p>The name of the host to add to the database. Default from dj.config</p> <code>None</code> <code>target_group</code> <code>str</code> <p>Group to which user belongs. Default is kachery-users</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Default False. If True, print sql instead of running</p> <code>False</code> Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>def __init__(\n    self, user_name=None, host_name=None, target_group=None, debug=False\n):\n    \"\"\"Class to manage common database settings\n\n    Parameters\n    ----------\n    user_name : str, optional\n        The name of the user to add to the database. Default from dj.config\n    host_name : str, optional\n        The name of the host to add to the database. Default from dj.config\n    target_group : str, optional\n        Group to which user belongs. Default is kachery-users\n    debug : bool, optional\n        Default False. If True, print sql instead of running\n    \"\"\"\n    self.shared_modules = [\n        f\"common{ESC}\",\n        f\"spikesorting{ESC}\",\n        f\"decoding{ESC}\",\n        f\"position{ESC}\",\n        f\"position_linearization{ESC}\",\n        f\"ripple{ESC}\",\n        f\"lfp{ESC}\",\n    ]\n    self.user = user_name or dj.config[\"database.user\"]\n    self.host = (\n        host_name or dj.config[\"database.host\"] or \"lmf-db.cin.ucsf.edu\"\n    )\n    self.target_group = target_group or \"kachery-users\"\n    self.debug = debug\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings.add_collab_user", "title": "<code>add_collab_user()</code>", "text": "<p>Add collaborator user with full permissions to shared modules</p> Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>def add_collab_user(self):\n    \"\"\"Add collaborator user with full permissions to shared modules\"\"\"\n    file = self.write_temp_file(self._add_collab_usr_sql)\n    self.run_file(file)\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings.add_dj_guest", "title": "<code>add_dj_guest()</code>", "text": "<p>Add guest user with select permissions to shared modules</p> Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>def add_dj_guest(self):\n    \"\"\"Add guest user with select permissions to shared modules\"\"\"\n    file = self.write_temp_file(self._add_dj_guest_sql)\n    self.run_file(file)\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings.add_module", "title": "<code>add_module(module_name)</code>", "text": "<p>Add module to database. Grant permissions to all users in group</p> Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>def add_module(self, module_name):\n    \"\"\"Add module to database. Grant permissions to all users in group\"\"\"\n    print(f\"Granting everyone permissions to module {module_name}\")\n    group = self._find_group()\n    file = self.write_temp_file(self._add_module_sql(module_name, group))\n    self.run_file(file)\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings.add_dj_user", "title": "<code>add_dj_user(check_exists=True)</code>", "text": "<p>Add user to database with permissions to shared modules</p> Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>def add_dj_user(self, check_exists=True):\n    \"\"\"Add user to database with permissions to shared modules\"\"\"\n    if check_exists:\n        user_home = Path.home().parent / self.user\n        if user_home.exists():\n            print(\"Creating database user \", self.user)\n        else:\n            sys.exit(\n                f\"Error: could not find {self.user} in home dir: {user_home}\"\n            )\n\n    file = self.write_temp_file(self._add_dj_user_sql)\n    self.run_file(file)\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings.write_temp_file", "title": "<code>write_temp_file(content)</code>", "text": "<p>Write content to a temporary file and return the file object</p> Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>def write_temp_file(self, content: list) -&gt; tempfile.NamedTemporaryFile:\n    \"\"\"Write content to a temporary file and return the file object\"\"\"\n    file = tempfile.NamedTemporaryFile(mode=\"w\")\n    for line in content:\n        file.write(line)\n    file.flush()\n\n    if self.debug:\n        from pprint import pprint  # noqa F401\n\n        pprint(file.name)\n        pprint(content)\n\n    return file\n</code></pre>"}, {"location": "api/src/spyglass/utils/database_settings/#src.spyglass.utils.database_settings.DatabaseSettings.run_file", "title": "<code>run_file(file)</code>", "text": "<p>Run commands saved to file in sql</p> Source code in <code>src/spyglass/utils/database_settings.py</code> <pre><code>def run_file(self, file):\n    \"\"\"Run commands saved to file in sql\"\"\"\n\n    if self.debug:\n        return\n\n    os.system(f\"mysql -p -h {self.host} &lt; {file.name}\")\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_helper_fn/", "title": "dj_helper_fn.py", "text": "<p>Helper functions for manipulating information from DataJoint fetch calls.</p>"}, {"location": "api/src/spyglass/utils/dj_helper_fn/#src.spyglass.utils.dj_helper_fn.dj_replace", "title": "<code>dj_replace(original_table, new_values, key_column, replace_column)</code>", "text": "<p>Given the output of a fetch() call from a schema and a 2D array made up of (key_value, replace_value) tuples, find each instance of key_value in the key_column of the original table and replace the specified replace_column with the associated replace_value. Key values must be unique.</p> <p>Parameters:</p> Name Type Description Default <code>original_table</code> <p>Result of a datajoint .fetch() call on a schema query.</p> required <code>new_values</code> <code>list</code> <p>List of tuples, each containing (key_value, replace_value).</p> required <code>replace_column</code> <code>str</code> <p>The name of the column where to-be-replaced values are located.</p> required <p>Returns:</p> Type Description <code>original_table</code> <p>Structured array of new table entries that can be inserted back into the schema</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def dj_replace(original_table, new_values, key_column, replace_column):\n    \"\"\"Given the output of a fetch() call from a schema and a 2D array made up\n    of (key_value, replace_value) tuples, find each instance of key_value in\n    the key_column of the original table and replace the specified\n    replace_column with the associated replace_value. Key values must be\n    unique.\n\n    Parameters\n    ----------\n    original_table\n        Result of a datajoint .fetch() call on a schema query.\n    new_values : list\n        List of tuples, each containing (key_value, replace_value).\n    replace_column : str\n        The name of the column where to-be-replaced values are located.\n\n    Returns\n    -------\n    original_table\n        Structured array of new table entries that can be inserted back into the schema\n    \"\"\"\n\n    # check to make sure the new_values are a list or array of tuples and fix if not\n    if isinstance(new_values, tuple):\n        tmp = list()\n        tmp.append(new_values)\n        new_values = tmp\n\n    new_val_array = np.asarray(new_values)\n    replace_ind = np.where(\n        np.isin(original_table[key_column], new_val_array[:, 0])\n    )\n    original_table[replace_column][replace_ind] = new_val_array[:, 1]\n    return original_table\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_helper_fn/#src.spyglass.utils.dj_helper_fn.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n    \"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/", "title": "dj_merge_tables.py", "text": ""}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge", "title": "<code>Merge</code>", "text": "<p>             Bases: <code>Manual</code></p> <p>Adds funcs to support standard Merge table operations.</p> <p>Many methods have the @classmethod decorator to permit MergeTable.method() symtax. This makes access to instance attributes (e.g., (MergeTable &amp; \"example='restriction'\").restriction) harder, but these attributes have limited utility when the user wants to, for example, restrict the merged view rather than the master table itself.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>class Merge(dj.Manual):\n    \"\"\"Adds funcs to support standard Merge table operations.\n\n    Many methods have the @classmethod decorator to permit MergeTable.method()\n    symtax. This makes access to instance attributes (e.g., (MergeTable &amp;\n    \"example='restriction'\").restriction) harder, but these attributes have\n    limited utility when the user wants to, for example, restrict the merged\n    view rather than the master table itself.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._reserved_pk = RESERVED_PRIMARY_KEY\n        self._reserved_sk = RESERVED_SECONDARY_KEY\n        merge_def = (\n            f\"\\n    {self._reserved_pk}: uuid\\n    ---\\n\"\n            + f\"    {self._reserved_sk}: varchar({RESERVED_SK_LENGTH})\\n    \"\n        )\n        # TODO: Change warnings to logger. Throw error? - CBroz1\n        if not self.is_declared:\n            if self.definition != merge_def:\n                print(\n                    \"WARNING: merge table with non-default definition\\n\\t\"\n                    + f\"Expected: {merge_def.strip()}\\n\\t\"\n                    + f\"Actual  : {self.definition.strip()}\"\n                )\n            for part in self.parts(as_objects=True):\n                if part.primary_key != self.primary_key:\n                    print(\n                        f\"WARNING: unexpected primary key in {part.table_name}\"\n                        + f\"\\n\\tExpected: {self.primary_key}\"\n                        + f\"\\n\\tActual  : {part.primary_key}\"\n                    )\n\n    @classmethod\n    def _merge_restrict_parts(\n        cls,\n        restriction: str = True,\n        as_objects: bool = True,\n        return_empties: bool = True,\n        add_invalid_restrict: bool = True,\n    ) -&gt; list:\n        \"\"\"Returns a list of parts with restrictions applied.\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the parts. Default True, no restrictions.\n        as_objects: bool, optional\n            Default True. Return part tables as objects\n        return_empties: bool, optional\n            Default True. Return empty part tables\n        add_invalid_restrict: bool, optional\n            Default True. Include part for which the restriction is invalid.\n\n        Returns\n        ------\n        list\n            list of datajoint tables, parts of Merge Table\n        \"\"\"\n\n        cls._ensure_dependencies_loaded()\n\n        if not restriction:\n            restriction = True\n\n        # Normalize restriction to sql string\n        restr_str = make_condition(cls(), restriction, set())\n\n        parts_all = cls.parts(as_objects=True)\n        # If the restriction makes ref to a source, we only want that part\n        if (\n            not return_empties\n            and isinstance(restr_str, str)\n            and f\"`{cls()._reserved_sk}`\" in restr_str\n        ):\n            parts_all = [\n                part\n                for part in parts_all\n                if from_camel_case(\n                    restr_str.split(f'`{cls()._reserved_sk}`=\"')[-1].split('\"')[\n                        0\n                    ]\n                )  # Only look at source part table\n                in part.full_table_name\n            ]\n        if isinstance(restriction, dict):  # restr by source already done above\n            _ = restriction.pop(cls()._reserved_sk, None)  # won't work for str\n            # If a dict restriction has all invalid keys, it is treated as True\n            if not add_invalid_restrict:\n                parts_all = [  # so exclude tables w/ nonmatching attrs\n                    p\n                    for p in parts_all\n                    if all([k in p.heading.names for k in restriction.keys()])\n                ]\n\n        parts = []\n        for part in parts_all:\n            try:\n                parts.append(part.restrict(restriction))\n            except DataJointError:  # If restriction not valid on given part\n                if add_invalid_restrict:\n                    parts.append(part)\n\n        if not return_empties:\n            parts = [p for p in parts if len(p)]\n        if not as_objects:\n            parts = [p.full_table_name for p in parts]\n\n        return parts\n\n    @classmethod\n    def _merge_restrict_parents(\n        cls,\n        restriction: str = True,\n        parent_name: str = None,\n        as_objects: bool = True,\n        return_empties: bool = True,\n        add_invalid_restrict: bool = True,\n    ) -&gt; list:\n        \"\"\"Returns a list of part parents with restrictions applied.\n\n        Rather than part tables, we look at parents of those parts, the source\n        of the data.\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the returned parent. Default True, no\n            restrictions.\n        parent_name: str, optional\n            CamelCase name of the parent.\n        as_objects: bool, optional\n            Default True. Return part tables as objects\n        return_empties: bool, optional\n            Default True. Return empty part tables\n        add_invalid_restrict: bool, optional\n            Default True. Include part for which the restriction is invalid.\n\n        Returns\n        ------\n        list\n            list of datajoint tables, parents of parts of Merge Table\n        \"\"\"\n        # .restrict(restriction) does not work on returned part FreeTable\n        # &amp; part.fetch below restricts parent to entries in merge table\n        part_parents = [\n            parent\n            &amp; part.fetch(*part.heading.secondary_attributes, as_dict=True)\n            for part in cls()._merge_restrict_parts(\n                restriction=restriction,\n                return_empties=return_empties,\n                add_invalid_restrict=add_invalid_restrict,\n            )\n            for parent in part.parents(as_objects=True)  # ID respective parents\n            if cls().table_name not in parent.full_table_name  # Not merge table\n        ]\n        if parent_name:\n            part_parents = [\n                p\n                for p in part_parents\n                if from_camel_case(parent_name) in p.full_table_name\n            ]\n        if not as_objects:\n            part_parents = [p.full_table_name for p in part_parents]\n\n        return part_parents\n\n    @classmethod\n    def _merge_repr(cls, restriction: str = True) -&gt; dj.expression.Union:\n        \"\"\"Merged view, including null entries for columns unique to one part.\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the merged view\n\n        Returns\n        ------\n        datajoint.expression.Union\n        \"\"\"\n\n        parts = [\n            cls() * p  # join with master to include sec key (i.e., 'source')\n            for p in cls._merge_restrict_parts(\n                restriction=restriction,\n                add_invalid_restrict=False,\n                return_empties=False,\n            )\n        ]\n\n        primary_attrs = list(\n            dict.fromkeys(  # get all columns from parts\n                iter_chain.from_iterable([p.heading.names for p in parts])\n            )\n        )\n        # primary_attrs.append(cls()._reserved_sk)\n        query = dj.U(*primary_attrs) * parts[0].proj(  # declare query\n            ...,  # include all attributes from part 0\n            **{\n                a: \"NULL\"  # add null value where part has no column\n                for a in primary_attrs\n                if a not in parts[0].heading.names\n            },\n        )\n        for part in parts[1:]:  # add to declared query for each part\n            query += dj.U(*primary_attrs) * part.proj(\n                ...,\n                **{\n                    a: \"NULL\"\n                    for a in primary_attrs\n                    if a not in part.heading.names\n                },\n            )\n        return query\n\n    @classmethod\n    def _merge_insert(\n        cls, rows: list, part_name: str = None, mutual_exclusvity=True, **kwargs\n    ) -&gt; None:\n        \"\"\"Insert rows into merge, ensuring db integrity and mutual exclusivity\n\n        Parameters\n        ---------\n        rows: List[dict]\n            An iterable where an element is a dictionary.\n        part: str, optional\n            CamelCase name of the part table\n\n        Raises\n        ------\n        TypeError\n            If rows is not a list of dicts\n        ValueError\n            If entry already exists, mutual exclusivity errors\n            If data doesn't exist in part parents, integrity error\n        \"\"\"\n        cls._ensure_dependencies_loaded()\n\n        try:\n            for r in iter(rows):\n                assert isinstance(\n                    r, dict\n                ), 'Input \"rows\" must be a list of dictionaries'\n        except TypeError:\n            raise TypeError('Input \"rows\" must be a list of dictionaries')\n\n        parts = cls._merge_restrict_parts(as_objects=True)\n        if part_name:\n            parts = [\n                p\n                for p in parts\n                if from_camel_case(part_name) in p.full_table_name\n            ]\n\n        master_entries = []\n        parts_entries = {p: [] for p in parts}\n        for row in rows:\n            keys = []  # empty to-be-inserted key\n            for part in parts:  # check each part\n                part_parent = part.parents(as_objects=True)[-1]\n                part_name = to_camel_case(part.table_name.split(\"__\")[-1])\n                if part_parent &amp; row:  # if row is in part parent\n                    if keys and mutual_exclusvity:  # if key from other part\n                        raise ValueError(\n                            \"Mutual Exclusivity Error! Entry exists in more \"\n                            + f\"than one table - Entry: {row}\"\n                        )\n\n                    keys = (part_parent &amp; row).fetch(\"KEY\")  # get pk\n                    if len(keys) &gt; 1:\n                        raise ValueError(\n                            \"Ambiguous entry. Data has mult rows in \"\n                            + f\"{part_name}:\\n\\tData:{row}\\n\\t{keys}\"\n                        )\n                    master_pk = {  # make uuid\n                        cls()._reserved_pk: dj.hash.key_hash(keys[0]),\n                    }\n                    parts_entries[part].append({**master_pk, **keys[0]})\n                    master_entries.append(\n                        {**master_pk, cls()._reserved_sk: part_name}\n                    )\n\n            if not keys:\n                raise ValueError(\n                    \"Non-existing entry in any of the parent tables - Entry: \"\n                    + f\"{row}\"\n                )\n\n        with cls._safe_context():\n            super().insert(cls(), master_entries, **kwargs)\n            for part, part_entries in parts_entries.items():\n                part.insert(part_entries, **kwargs)\n\n    @classmethod\n    def _safe_context(cls):\n        \"\"\"Return transaction if not already in one.\"\"\"\n        return (\n            cls.connection.transaction\n            if not cls.connection.in_transaction\n            else nullcontext()\n        )\n\n    @classmethod\n    def _ensure_dependencies_loaded(cls) -&gt; None:\n        \"\"\"Ensure connection dependencies loaded.\n\n        Otherwise parts returns none\n        \"\"\"\n        if not dj.conn.connection.dependencies._loaded:\n            dj.conn.connection.dependencies.load()\n\n    def insert(self, rows: list, mutual_exclusvity=True, **kwargs):\n        \"\"\"Merges table specific insert\n\n        Ensuring db integrity and mutual exclusivity\n\n        Parameters\n        ---------\n        rows: List[dict]\n            An iterable where an element is a dictionary.\n        mutual_exclusvity: bool\n            Check for mutual exclusivity before insert. Default True.\n\n        Raises\n        ------\n        TypeError\n            If rows is not a list of dicts\n        ValueError\n            If entry already exists, mutual exclusivity errors\n            If data doesn't exist in part parents, integrity error\n        \"\"\"\n        self._merge_insert(rows, mutual_exclusvity=mutual_exclusvity, **kwargs)\n\n    @classmethod\n    def merge_view(cls, restriction: str = True):\n        \"\"\"Prints merged view, including null entries for unique columns.\n\n        Note: To handle this Union as a table-like object, use `merge_resrict`\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the merged view\n        \"\"\"\n\n        # If we overwrite `preview`, we then encounter issues with operators\n        # getting passed a `Union`, which doesn't have a method we can\n        # intercept to manage master/parts\n\n        return pprint(cls._merge_repr(restriction=restriction))\n\n    @classmethod\n    def merge_html(cls, restriction: str = True):\n        \"\"\"Displays HTML in notebooks.\"\"\"\n\n        return HTML(repr_html(cls._merge_repr(restriction=restriction)))\n\n    @classmethod\n    def merge_restrict(cls, restriction: str = True) -&gt; dj.U:\n        \"\"\"Given a restriction, return a merged view with restriction applied.\n\n        Example\n        -------\n            &gt;&gt;&gt; MergeTable.merge_restrict(\"field = 1\")\n\n        Parameters\n        ----------\n        restriction: str\n            Restriction one would apply if `merge_view` was a real table.\n\n        Returns\n        -------\n        datajoint.Union\n            Merged view with restriction applied.\n        \"\"\"\n        return cls._merge_repr(restriction=restriction)\n\n    @classmethod\n    def merge_delete(cls, restriction: str = True, **kwargs):\n        \"\"\"Given a restriction string, delete corresponding entries.\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before deletion from master/part\n            tables. If not provided, delete all entries.\n        kwargs: dict\n            Additional keyword arguments for DataJoint delete.\n\n        Example\n        -------\n            &gt;&gt;&gt; MergeTable.merge_delete(\"field = 1\")\n        \"\"\"\n        uuids = [\n            {k: v}\n            for entry in cls.merge_restrict(restriction).fetch(\"KEY\")\n            for k, v in entry.items()\n            if k == cls()._reserved_pk\n        ]\n        (cls() &amp; uuids).delete(**kwargs)\n\n    @classmethod\n    def merge_delete_parent(\n        cls, restriction: str = True, dry_run=True, **kwargs\n    ) -&gt; list:\n        \"\"\"Delete entries from merge master, part, and respective part parents\n\n        Note: Clears merge entries from their respective parents.\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before deletion from parents. If not\n            provided, delete all entries present in Merge Table.\n        dry_run: bool\n            Default True. If true, return list of tables with entries that would\n            be deleted. Otherwise, table entries.\n        kwargs: dict\n            Additional keyword arguments for DataJoint delete.\n        \"\"\"\n        part_parents = cls._merge_restrict_parents(\n            restriction=restriction, as_objects=True, return_empties=False\n        )\n\n        if dry_run:\n            return part_parents\n\n        merge_ids = cls.merge_restrict(restriction).fetch(\n            RESERVED_PRIMARY_KEY, as_dict=True\n        )\n\n        # CB: Removed transaction protection here bc 'no' confirmation resp\n        # still resulted in deletes. If re-add, consider transaction=False\n        super().delete((cls &amp; merge_ids), **kwargs)\n\n        if cls &amp; merge_ids:  # If 'no' on del prompt from above, skip below\n            return  # User can still abort del below, but yes/no is unlikly\n\n        for part_parent in part_parents:\n            super().delete(part_parent, **kwargs)  # add safemode=False?\n\n    @classmethod\n    def fetch_nwb(\n        cls, restriction: str = True, multi_source=False, *attrs, **kwargs\n    ):\n        \"\"\"Return the AnalysisNwbfile file linked in the source.\n\n        Parameters\n        ----------\n        restriction: str, optional\n            Restriction to apply to parents before running fetch. Default none.\n        multi_source: bool\n            Return from multiple parents. Default False.\n        \"\"\"\n        part_parents = cls._merge_restrict_parents(\n            restriction=restriction,\n            return_empties=False,\n            add_invalid_restrict=False,\n        )\n\n        if not multi_source and len(part_parents) != 1:\n            raise ValueError(\n                f\"{len(part_parents)} possible sources found in Merge Table:\"\n                + \" and \".join([p.full_table_name for p in part_parents])\n            )\n\n        nwbs = []\n        for part_parent in part_parents:\n            nwbs.extend(\n                fetch_nwb(\n                    part_parent,\n                    (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                    *attrs,\n                    **kwargs,\n                )\n            )\n        return nwbs\n\n    @classmethod\n    def merge_get_part(\n        cls,\n        restriction: str = True,\n        join_master: bool = False,\n        restrict_part=True,\n        multi_source=False,\n    ) -&gt; dj.Table:\n        \"\"\"Retrieve part table from a restricted Merge table.\n\n        Note: unlike other Merge Table methods, returns the native table, not\n        a FreeTable\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before determining part to return.\n            Default True.\n        join_master: bool\n            Join part with Merge master to show source field. Default False.\n        restrict_part: bool\n            Apply restriction to part. Default True. If False, return the\n            native part table.\n        multi_source: bool\n            Return multiple parts. Default False.\n\n        Returns\n        ------\n        Union[dj.Table, List[dj.Table]]\n            Native part table(s) of Merge. If `multi_source`, returns list.\n\n        Example\n        -------\n            &gt;&gt;&gt; (MergeTable &amp; restriction).get_part_table()\n            &gt;&gt;&gt; MergeTable().merge_get_part(restriction, join_master=True)\n\n        Raises\n        ------\n        ValueError\n            If multiple sources are found, but not expected lists and suggests\n            restricting\n        \"\"\"\n        sources = [\n            to_camel_case(n.split(\"__\")[-1].strip(\"`\"))  # friendly part name\n            for n in cls._merge_restrict_parts(\n                restriction=restriction,\n                as_objects=False,\n                return_empties=False,\n                add_invalid_restrict=False,\n            )\n        ]\n\n        if not multi_source and len(sources) != 1:\n            raise ValueError(\n                f\"Found {len(sources)} potential parts: {sources}\\n\\t\"\n                + \"Try adding a restriction before invoking `get_part`.\\n\\t\"\n                + \"Or permitting multiple sources with `multi_source=True`.\"\n            )\n\n        parts = [\n            getattr(cls, source)().restrict(restriction)\n            if restrict_part  # Re-apply restriction or don't\n            else getattr(cls, source)()\n            for source in sources\n        ]\n        if join_master:\n            parts = [cls * part for part in parts]\n\n        return parts if multi_source else parts[0]\n\n    @classmethod\n    def merge_get_parent(\n        cls,\n        restriction: str = True,\n        join_master: bool = False,\n        multi_source=False,\n    ) -&gt; dj.FreeTable:\n        \"\"\"Returns a list of part parents with restrictions applied.\n\n        Rather than part tables, we look at parents of those parts, the source\n        of the data, and only the rows that have keys inserted in the merge\n        table.\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before determining parent to return.\n            Default True.\n        join_master: bool\n            Default False. Join part with Merge master to show uuid and source\n\n        Returns\n        ------\n        dj.FreeTable\n            Parent of parts of Merge Table as FreeTable.\n        \"\"\"\n\n        part_parents = cls._merge_restrict_parents(\n            restriction=restriction,\n            as_objects=True,\n            return_empties=False,\n            add_invalid_restrict=False,\n        )\n\n        if not multi_source and len(part_parents) != 1:\n            raise ValueError(\n                f\"Found  {len(part_parents)} potential parents: {part_parents}\"\n                + \"\\n\\tTry adding a string restriction when invoking \"\n                + \"`get_parent`. Or permitting multiple sources with \"\n                + \"`multi_source=True`.\"\n            )\n\n        if join_master:\n            part_parents = [cls * part for part in part_parents]\n\n        return part_parents if multi_source else part_parents[0]\n\n    @classmethod\n    def merge_fetch(self, restriction: str = True, *attrs, **kwargs) -&gt; list:\n        \"\"\"Perform a fetch across all parts. If &gt;1 result, return as a list.\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before determining parent to return.\n            Default True.\n        attrs, kwargs\n            arguments passed to DataJoint `fetch` call\n\n        Returns\n        -------\n        Union[ List[np.array], List[dict], List[pd.DataFrame] ]\n            Table contents, with type determined by kwargs\n        \"\"\"\n        results = []\n        parts = self()._merge_restrict_parts(\n            restriction=restriction,\n            as_objects=True,\n            return_empties=False,\n            add_invalid_restrict=False,\n        )\n\n        for part in parts:\n            try:\n                results.extend(part.fetch(*attrs, **kwargs))\n            except DataJointError as e:\n                print(\n                    f\"WARNING: {e.args[0]} Skipping \"\n                    + to_camel_case(part.table_name.split(\"__\")[-1])\n                )\n\n        # Note: this could collapse results like merge_view, but user may call\n        # for recarray, pd.DataFrame, or dict, and fetched contents differ if\n        # attrs or \"KEY\" called. Intercept format, merge, and then transform?\n\n        if not results:\n            print(\n                \"No merge_fetch results.\\n\\t\"\n                + \"If not restricting, try: `M.merge_fetch(True,'attr')\\n\\t\"\n                + \"If restricting by source, use dict: \"\n                + \"`M.merge_fetch({'source':'X'})\"\n            )\n        return results[0] if len(results) == 1 else results\n\n    @classmethod\n    def merge_populate(source: str, key=None):\n        raise NotImplementedError(\n            \"CBroz: In the future, this command will support executing \"\n            + \"part_parent `make` and then inserting all entries into Merge\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.insert", "title": "<code>insert(rows, mutual_exclusvity=True, **kwargs)</code>", "text": "<p>Merges table specific insert</p> <p>Ensuring db integrity and mutual exclusivity</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>list</code> <p>An iterable where an element is a dictionary.</p> required <code>mutual_exclusvity</code> <p>Check for mutual exclusivity before insert. Default True.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If rows is not a list of dicts</p> <code>ValueError</code> <p>If entry already exists, mutual exclusivity errors If data doesn't exist in part parents, integrity error</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>def insert(self, rows: list, mutual_exclusvity=True, **kwargs):\n    \"\"\"Merges table specific insert\n\n    Ensuring db integrity and mutual exclusivity\n\n    Parameters\n    ---------\n    rows: List[dict]\n        An iterable where an element is a dictionary.\n    mutual_exclusvity: bool\n        Check for mutual exclusivity before insert. Default True.\n\n    Raises\n    ------\n    TypeError\n        If rows is not a list of dicts\n    ValueError\n        If entry already exists, mutual exclusivity errors\n        If data doesn't exist in part parents, integrity error\n    \"\"\"\n    self._merge_insert(rows, mutual_exclusvity=mutual_exclusvity, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_view", "title": "<code>merge_view(restriction=True)</code>  <code>classmethod</code>", "text": "<p>Prints merged view, including null entries for unique columns.</p> <p>Note: To handle this Union as a table-like object, use <code>merge_resrict</code></p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Restriction to apply to the merged view</p> <code>True</code> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_view(cls, restriction: str = True):\n    \"\"\"Prints merged view, including null entries for unique columns.\n\n    Note: To handle this Union as a table-like object, use `merge_resrict`\n\n    Parameters\n    ---------\n    restriction: str, optional\n        Restriction to apply to the merged view\n    \"\"\"\n\n    # If we overwrite `preview`, we then encounter issues with operators\n    # getting passed a `Union`, which doesn't have a method we can\n    # intercept to manage master/parts\n\n    return pprint(cls._merge_repr(restriction=restriction))\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_html", "title": "<code>merge_html(restriction=True)</code>  <code>classmethod</code>", "text": "<p>Displays HTML in notebooks.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_html(cls, restriction: str = True):\n    \"\"\"Displays HTML in notebooks.\"\"\"\n\n    return HTML(repr_html(cls._merge_repr(restriction=restriction)))\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_restrict", "title": "<code>merge_restrict(restriction=True)</code>  <code>classmethod</code>", "text": "<p>Given a restriction, return a merged view with restriction applied.</p>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_restrict--example", "title": "Example", "text": "<pre><code>&gt;&gt;&gt; MergeTable.merge_restrict(\"field = 1\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Restriction one would apply if <code>merge_view</code> was a real table.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union</code> <p>Merged view with restriction applied.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_restrict(cls, restriction: str = True) -&gt; dj.U:\n    \"\"\"Given a restriction, return a merged view with restriction applied.\n\n    Example\n    -------\n        &gt;&gt;&gt; MergeTable.merge_restrict(\"field = 1\")\n\n    Parameters\n    ----------\n    restriction: str\n        Restriction one would apply if `merge_view` was a real table.\n\n    Returns\n    -------\n    datajoint.Union\n        Merged view with restriction applied.\n    \"\"\"\n    return cls._merge_repr(restriction=restriction)\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_delete", "title": "<code>merge_delete(restriction=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Given a restriction string, delete corresponding entries.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before deletion from master/part tables. If not provided, delete all entries.</p> <code>True</code> <code>kwargs</code> <p>Additional keyword arguments for DataJoint delete.</p> <code>{}</code>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_delete--example", "title": "Example", "text": "<pre><code>&gt;&gt;&gt; MergeTable.merge_delete(\"field = 1\")\n</code></pre> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_delete(cls, restriction: str = True, **kwargs):\n    \"\"\"Given a restriction string, delete corresponding entries.\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before deletion from master/part\n        tables. If not provided, delete all entries.\n    kwargs: dict\n        Additional keyword arguments for DataJoint delete.\n\n    Example\n    -------\n        &gt;&gt;&gt; MergeTable.merge_delete(\"field = 1\")\n    \"\"\"\n    uuids = [\n        {k: v}\n        for entry in cls.merge_restrict(restriction).fetch(\"KEY\")\n        for k, v in entry.items()\n        if k == cls()._reserved_pk\n    ]\n    (cls() &amp; uuids).delete(**kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_delete_parent", "title": "<code>merge_delete_parent(restriction=True, dry_run=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Delete entries from merge master, part, and respective part parents</p> <p>Note: Clears merge entries from their respective parents.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before deletion from parents. If not provided, delete all entries present in Merge Table.</p> <code>True</code> <code>dry_run</code> <p>Default True. If true, return list of tables with entries that would be deleted. Otherwise, table entries.</p> <code>True</code> <code>kwargs</code> <p>Additional keyword arguments for DataJoint delete.</p> <code>{}</code> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_delete_parent(\n    cls, restriction: str = True, dry_run=True, **kwargs\n) -&gt; list:\n    \"\"\"Delete entries from merge master, part, and respective part parents\n\n    Note: Clears merge entries from their respective parents.\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before deletion from parents. If not\n        provided, delete all entries present in Merge Table.\n    dry_run: bool\n        Default True. If true, return list of tables with entries that would\n        be deleted. Otherwise, table entries.\n    kwargs: dict\n        Additional keyword arguments for DataJoint delete.\n    \"\"\"\n    part_parents = cls._merge_restrict_parents(\n        restriction=restriction, as_objects=True, return_empties=False\n    )\n\n    if dry_run:\n        return part_parents\n\n    merge_ids = cls.merge_restrict(restriction).fetch(\n        RESERVED_PRIMARY_KEY, as_dict=True\n    )\n\n    # CB: Removed transaction protection here bc 'no' confirmation resp\n    # still resulted in deletes. If re-add, consider transaction=False\n    super().delete((cls &amp; merge_ids), **kwargs)\n\n    if cls &amp; merge_ids:  # If 'no' on del prompt from above, skip below\n        return  # User can still abort del below, but yes/no is unlikly\n\n    for part_parent in part_parents:\n        super().delete(part_parent, **kwargs)  # add safemode=False?\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.fetch_nwb", "title": "<code>fetch_nwb(restriction=True, multi_source=False, *attrs, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Return the AnalysisNwbfile file linked in the source.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Restriction to apply to parents before running fetch. Default none.</p> <code>True</code> <code>multi_source</code> <p>Return from multiple parents. Default False.</p> <code>False</code> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef fetch_nwb(\n    cls, restriction: str = True, multi_source=False, *attrs, **kwargs\n):\n    \"\"\"Return the AnalysisNwbfile file linked in the source.\n\n    Parameters\n    ----------\n    restriction: str, optional\n        Restriction to apply to parents before running fetch. Default none.\n    multi_source: bool\n        Return from multiple parents. Default False.\n    \"\"\"\n    part_parents = cls._merge_restrict_parents(\n        restriction=restriction,\n        return_empties=False,\n        add_invalid_restrict=False,\n    )\n\n    if not multi_source and len(part_parents) != 1:\n        raise ValueError(\n            f\"{len(part_parents)} possible sources found in Merge Table:\"\n            + \" and \".join([p.full_table_name for p in part_parents])\n        )\n\n    nwbs = []\n    for part_parent in part_parents:\n        nwbs.extend(\n            fetch_nwb(\n                part_parent,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n        )\n    return nwbs\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_get_part", "title": "<code>merge_get_part(restriction=True, join_master=False, restrict_part=True, multi_source=False)</code>  <code>classmethod</code>", "text": "<p>Retrieve part table from a restricted Merge table.</p> <p>Note: unlike other Merge Table methods, returns the native table, not a FreeTable</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before determining part to return. Default True.</p> <code>True</code> <code>join_master</code> <code>bool</code> <p>Join part with Merge master to show source field. Default False.</p> <code>False</code> <code>restrict_part</code> <p>Apply restriction to part. Default True. If False, return the native part table.</p> <code>True</code> <code>multi_source</code> <p>Return multiple parts. Default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Table, List[Table]]</code> <p>Native part table(s) of Merge. If <code>multi_source</code>, returns list.</p>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_get_part--example", "title": "Example", "text": "<pre><code>&gt;&gt;&gt; (MergeTable &amp; restriction).get_part_table()\n&gt;&gt;&gt; MergeTable().merge_get_part(restriction, join_master=True)\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiple sources are found, but not expected lists and suggests restricting</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_get_part(\n    cls,\n    restriction: str = True,\n    join_master: bool = False,\n    restrict_part=True,\n    multi_source=False,\n) -&gt; dj.Table:\n    \"\"\"Retrieve part table from a restricted Merge table.\n\n    Note: unlike other Merge Table methods, returns the native table, not\n    a FreeTable\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before determining part to return.\n        Default True.\n    join_master: bool\n        Join part with Merge master to show source field. Default False.\n    restrict_part: bool\n        Apply restriction to part. Default True. If False, return the\n        native part table.\n    multi_source: bool\n        Return multiple parts. Default False.\n\n    Returns\n    ------\n    Union[dj.Table, List[dj.Table]]\n        Native part table(s) of Merge. If `multi_source`, returns list.\n\n    Example\n    -------\n        &gt;&gt;&gt; (MergeTable &amp; restriction).get_part_table()\n        &gt;&gt;&gt; MergeTable().merge_get_part(restriction, join_master=True)\n\n    Raises\n    ------\n    ValueError\n        If multiple sources are found, but not expected lists and suggests\n        restricting\n    \"\"\"\n    sources = [\n        to_camel_case(n.split(\"__\")[-1].strip(\"`\"))  # friendly part name\n        for n in cls._merge_restrict_parts(\n            restriction=restriction,\n            as_objects=False,\n            return_empties=False,\n            add_invalid_restrict=False,\n        )\n    ]\n\n    if not multi_source and len(sources) != 1:\n        raise ValueError(\n            f\"Found {len(sources)} potential parts: {sources}\\n\\t\"\n            + \"Try adding a restriction before invoking `get_part`.\\n\\t\"\n            + \"Or permitting multiple sources with `multi_source=True`.\"\n        )\n\n    parts = [\n        getattr(cls, source)().restrict(restriction)\n        if restrict_part  # Re-apply restriction or don't\n        else getattr(cls, source)()\n        for source in sources\n    ]\n    if join_master:\n        parts = [cls * part for part in parts]\n\n    return parts if multi_source else parts[0]\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_get_parent", "title": "<code>merge_get_parent(restriction=True, join_master=False, multi_source=False)</code>  <code>classmethod</code>", "text": "<p>Returns a list of part parents with restrictions applied.</p> <p>Rather than part tables, we look at parents of those parts, the source of the data, and only the rows that have keys inserted in the merge table.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before determining parent to return. Default True.</p> <code>True</code> <code>join_master</code> <code>bool</code> <p>Default False. Join part with Merge master to show uuid and source</p> <code>False</code> <p>Returns:</p> Type Description <code>FreeTable</code> <p>Parent of parts of Merge Table as FreeTable.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_get_parent(\n    cls,\n    restriction: str = True,\n    join_master: bool = False,\n    multi_source=False,\n) -&gt; dj.FreeTable:\n    \"\"\"Returns a list of part parents with restrictions applied.\n\n    Rather than part tables, we look at parents of those parts, the source\n    of the data, and only the rows that have keys inserted in the merge\n    table.\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before determining parent to return.\n        Default True.\n    join_master: bool\n        Default False. Join part with Merge master to show uuid and source\n\n    Returns\n    ------\n    dj.FreeTable\n        Parent of parts of Merge Table as FreeTable.\n    \"\"\"\n\n    part_parents = cls._merge_restrict_parents(\n        restriction=restriction,\n        as_objects=True,\n        return_empties=False,\n        add_invalid_restrict=False,\n    )\n\n    if not multi_source and len(part_parents) != 1:\n        raise ValueError(\n            f\"Found  {len(part_parents)} potential parents: {part_parents}\"\n            + \"\\n\\tTry adding a string restriction when invoking \"\n            + \"`get_parent`. Or permitting multiple sources with \"\n            + \"`multi_source=True`.\"\n        )\n\n    if join_master:\n        part_parents = [cls * part for part in part_parents]\n\n    return part_parents if multi_source else part_parents[0]\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_fetch", "title": "<code>merge_fetch(restriction=True, *attrs, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Perform a fetch across all parts. If &gt;1 result, return as a list.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before determining parent to return. Default True.</p> <code>True</code> <code>attrs</code> <p>arguments passed to DataJoint <code>fetch</code> call</p> <code>()</code> <code>kwargs</code> <p>arguments passed to DataJoint <code>fetch</code> call</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[List[array], List[dict], List[DataFrame]]</code> <p>Table contents, with type determined by kwargs</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_fetch(self, restriction: str = True, *attrs, **kwargs) -&gt; list:\n    \"\"\"Perform a fetch across all parts. If &gt;1 result, return as a list.\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before determining parent to return.\n        Default True.\n    attrs, kwargs\n        arguments passed to DataJoint `fetch` call\n\n    Returns\n    -------\n    Union[ List[np.array], List[dict], List[pd.DataFrame] ]\n        Table contents, with type determined by kwargs\n    \"\"\"\n    results = []\n    parts = self()._merge_restrict_parts(\n        restriction=restriction,\n        as_objects=True,\n        return_empties=False,\n        add_invalid_restrict=False,\n    )\n\n    for part in parts:\n        try:\n            results.extend(part.fetch(*attrs, **kwargs))\n        except DataJointError as e:\n            print(\n                f\"WARNING: {e.args[0]} Skipping \"\n                + to_camel_case(part.table_name.split(\"__\")[-1])\n            )\n\n    # Note: this could collapse results like merge_view, but user may call\n    # for recarray, pd.DataFrame, or dict, and fetched contents differ if\n    # attrs or \"KEY\" called. Intercept format, merge, and then transform?\n\n    if not results:\n        print(\n            \"No merge_fetch results.\\n\\t\"\n            + \"If not restricting, try: `M.merge_fetch(True,'attr')\\n\\t\"\n            + \"If restricting by source, use dict: \"\n            + \"`M.merge_fetch({'source':'X'})\"\n        )\n    return results[0] if len(results) == 1 else results\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.delete_downstream_merge", "title": "<code>delete_downstream_merge(table, restriction=True, dry_run=True, recurse_level=2, **kwargs)</code>", "text": "<p>Given a table/restriction, id or delete relevant downstream merge entries</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>DataJoint table or restriction thereof</p> required <code>restriction</code> <code>str</code> <p>Optional restriction to apply before deletion from merge/part tables. If not provided, delete all downstream entries.</p> <code>True</code> <code>dry_run</code> <p>Default True. If true, return list of tuples, merge/part tables downstream of table input. Otherwise, delete merge/part table entries.</p> <code>True</code> <code>recurse_level</code> <p>Default 2. Depth to recurse into table descendants.</p> <code>2</code> <code>kwargs</code> <p>Additional keyword arguments for DataJoint delete.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Tuple[Table, Table]]</code> <p>Entries in merge/part tables downstream of table input.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>def delete_downstream_merge(\n    table: dj.Table,\n    restriction: str = True,\n    dry_run=True,\n    recurse_level=2,\n    **kwargs,\n) -&gt; list:\n    \"\"\"Given a table/restriction, id or delete relevant downstream merge entries\n\n    Parameters\n    ----------\n    table: dj.Table\n        DataJoint table or restriction thereof\n    restriction: str\n        Optional restriction to apply before deletion from merge/part\n        tables. If not provided, delete all downstream entries.\n    dry_run: bool\n        Default True. If true, return list of tuples, merge/part tables\n        downstream of table input. Otherwise, delete merge/part table entries.\n    recurse_level: int\n        Default 2. Depth to recurse into table descendants.\n    kwargs: dict\n        Additional keyword arguments for DataJoint delete.\n\n    Returns\n    -------\n    List[Tuple[dj.Table, dj.Table]]\n        Entries in merge/part tables downstream of table input.\n    \"\"\"\n    if table.restriction:\n        print(\n            f\"Warning: ignoring table restriction: {table.restriction}.\\n\\t\"\n            + \"Please pass restrictions as an arg\"\n        )\n\n    descendants = _unique_descendants(table, recurse_level)\n    merge_table_pairs = _master_table_pairs(descendants, restriction)\n\n    # restrict the merge table based on uuids in part\n    merge_pairs = [\n        (merge &amp; uuids, part)  # don't need part for del, but show on dry_run\n        for merge, part in merge_table_pairs\n        for uuids in part.fetch(RESERVED_PRIMARY_KEY, as_dict=True)\n    ]\n\n    if dry_run:\n        return merge_pairs\n\n    for merge_table, _ in merge_pairs:\n        merge_table.delete(**kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/", "title": "nwb_helper_fn.py", "text": "<p>NWB helper functions for finding processing modules and data interfaces.</p>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.</p> <p>If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file or NWB file name. If it does not start with a \"/\", get path with Nwbfile.get_abs_path</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n    \"\"\"Return an NWBFile object with the given file path in read mode.\n\n    If the file is not found locally, this will check if it has been shared\n    with kachery and if so, download it and open it.\n\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file or NWB file name. If it does not start with a \"/\",\n        get path with Nwbfile.get_abs_path\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    if not nwb_file_path.startswith(\"/\"):\n        from ..common import Nwbfile\n\n        nwb_file_path = Nwbfile.get_abs_path(nwb_file_path)\n\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                \"NWB file not found locally; checking kachery for \"\n                + f\"{nwb_file_path}\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to\n            # get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_config", "title": "<code>get_config(nwb_file_path)</code>", "text": "<p>Return a dictionary of config settings for the given NWB file. If the file does not exist, return an empty dict.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Absolute path to the NWB file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of configuration settings loaded from the corresponding YAML file</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_config(nwb_file_path):\n    \"\"\"Return a dictionary of config settings for the given NWB file.\n    If the file does not exist, return an empty dict.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Absolute path to the NWB file.\n\n    Returns\n    -------\n    dict\n        Dictionary of configuration settings loaded from the corresponding YAML file\n    \"\"\"\n    if nwb_file_path in __configs:  # load from cache if exists\n        return __configs[nwb_file_path]\n\n    p = Path(nwb_file_path)\n    # NOTE use p.stem[:-1] to remove the underscore that was added to the file\n    config_path = p.parent / (p.stem[:-1] + \"_spyglass_config.yaml\")\n    if not os.path.exists(config_path):\n        print(f\"No config found at file path {config_path}\")\n        return dict()\n    with open(config_path, \"r\") as stream:\n        d = yaml.safe_load(stream)\n\n    # TODO write a JSON schema for the yaml file and validate the yaml file\n    __configs[nwb_file_path] = d  # store in cache\n    return d\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_data_interface", "title": "<code>get_data_interface(nwbfile, data_interface_name, data_interface_class=None)</code>", "text": "<p>Search for NWBDataInterface or DynamicTable in processing modules of an NWB.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>NWBFile</code> <p>The NWB file object to search in.</p> required <code>data_interface_name</code> <code>str</code> <p>The name of the NWBDataInterface or DynamicTable to search for.</p> required <code>data_interface_class</code> <code>type</code> <p>The class (or superclass) to search for. This argument helps to prevent accessing an object with the same name but the incorrect type. Default: no restriction.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If multiple NWBDataInterface and DynamicTable objects with the matching name are found.</p> <p>Returns:</p> Name Type Description <code>data_interface</code> <code>NWBDataInterface</code> <p>The data interface object with the given name, or None if not found.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_data_interface(nwbfile, data_interface_name, data_interface_class=None):\n    \"\"\"\n    Search for NWBDataInterface or DynamicTable in processing modules of an NWB.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n    data_interface_name : str\n        The name of the NWBDataInterface or DynamicTable to search for.\n    data_interface_class : type, optional\n        The class (or superclass) to search for. This argument helps to prevent\n        accessing an object with the same name but the incorrect type. Default:\n        no restriction.\n\n    Warns\n    -----\n    UserWarning\n        If multiple NWBDataInterface and DynamicTable objects with the matching\n        name are found.\n\n    Returns\n    -------\n    data_interface : NWBDataInterface\n        The data interface object with the given name, or None if not found.\n    \"\"\"\n    ret = []\n    for module in nwbfile.processing.values():\n        match = module.data_interfaces.get(data_interface_name, None)\n        if match is not None:\n            if data_interface_class is not None and not isinstance(\n                match, data_interface_class\n            ):\n                continue\n            ret.append(match)\n    if len(ret) &gt; 1:\n        warnings.warn(\n            f\"Multiple data interfaces with name '{data_interface_name}' \"\n            f\"found in NWBFile with identifier {nwbfile.identifier}. \"\n            + \"Using the first one found. \"\n            \"Use the data_interface_class argument to restrict the search.\"\n        )\n    if len(ret) &gt;= 1:\n        return ret[0]\n\n    return None\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_raw_eseries", "title": "<code>get_raw_eseries(nwbfile)</code>", "text": "<p>Return all ElectricalSeries in the acquisition group of an NWB file.</p> <p>ElectricalSeries found within LFP objects in the acquisition will also be returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>NWBFile</code> <p>The NWB file object to search in.</p> required <p>Returns:</p> Name Type Description <code>ret</code> <code>list</code> <p>A list of all ElectricalSeries in the acquisition group of an NWB file</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_raw_eseries(nwbfile):\n    \"\"\"Return all ElectricalSeries in the acquisition group of an NWB file.\n\n    ElectricalSeries found within LFP objects in the acquisition will also be\n    returned.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n\n    Returns\n    -------\n    ret : list\n        A list of all ElectricalSeries in the acquisition group of an NWB file\n    \"\"\"\n    ret = []\n    for nwb_object in nwbfile.acquisition.values():\n        if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n            ret.append(nwb_object)\n        elif isinstance(nwb_object, pynwb.ecephys.LFP):\n            ret.extend(nwb_object.electrical_series.values())\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.estimate_sampling_rate", "title": "<code>estimate_sampling_rate(timestamps, multiplier=1.75, verbose=False, filename='file')</code>", "text": "<p>Estimate the sampling rate given a list of timestamps.</p> <p>Assumes that the most common temporal differences between timestamps approximate the sampling rate. Note that this can fail for very high sampling rates and irregular timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>1D numpy array of timestamp values.</p> required <code>multiplier</code> <code>float or int</code> <p>Deft</p> <code>1.75</code> <code>verbose</code> <code>bool</code> <p>Print sampling rate to stdout. Default, False</p> <code>False</code> <code>filename</code> <code>str</code> <p>Filename to reference when printing or err. Default, \"file\"</p> <code>'file'</code> <p>Returns:</p> Name Type Description <code>estimated_rate</code> <code>float</code> <p>The estimated sampling rate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If estimated rate is less than 0.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def estimate_sampling_rate(\n    timestamps, multiplier=1.75, verbose=False, filename=\"file\"\n):\n    \"\"\"Estimate the sampling rate given a list of timestamps.\n\n    Assumes that the most common temporal differences between timestamps\n    approximate the sampling rate. Note that this can fail for very high\n    sampling rates and irregular timestamps.\n\n    Parameters\n    ----------\n    timestamps : numpy.ndarray\n        1D numpy array of timestamp values.\n    multiplier : float or int, optional\n        Deft\n    verbose : bool, optional\n        Print sampling rate to stdout. Default, False\n    filename : str, optional\n        Filename to reference when printing or err. Default, \"file\"\n\n    Returns\n    -------\n    estimated_rate : float\n        The estimated sampling rate.\n\n    Raises\n    ------\n    ValueError\n        If estimated rate is less than 0.\n    \"\"\"\n\n    # approach:\n    # 1. use a box car smoother and a histogram to get the modal value\n    # 2. identify adjacent samples as those that have a\n    #    time difference &lt; the multiplier * the modal value\n    # 3. average the time differences between adjacent samples\n\n    sample_diff = np.diff(timestamps[~np.isnan(timestamps)])\n\n    if len(sample_diff) &lt; 10:\n        raise ValueError(\n            f\"Only {len(sample_diff)} timestamps are valid. Check the data.\"\n        )\n\n    smooth_diff = np.convolve(sample_diff, np.ones(10) / 10, mode=\"same\")\n\n    # we histogram with 100 bins out to 3 * mean, which should be fine for any\n    # reasonable number of samples\n    hist, bins = np.histogram(\n        smooth_diff, bins=100, range=[0, 3 * np.mean(smooth_diff)]\n    )\n    mode = bins[np.where(hist == np.max(hist))][0]\n\n    adjacent = sample_diff &lt; mode * multiplier\n\n    sampling_rate = np.round(1.0 / np.mean(sample_diff[adjacent]))\n\n    if sampling_rate &lt; 0:\n        raise ValueError(f\"Error calculating sampling rate. For {filename}\")\n    if verbose:\n        print(f\"Estimated sampling rate for {filename}: {sampling_rate} Hz\")\n\n    return sampling_rate\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_valid_intervals", "title": "<code>get_valid_intervals(timestamps, sampling_rate, gap_proportion=2.5, min_valid_len=0)</code>", "text": "<p>Finds the set of all valid intervals in a list of timestamps. Valid interval: (start time, stop time) during which there are no gaps (i.e. missing samples).</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>1D numpy array of timestamp values.</p> required <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data.</p> required <code>gap_proportion</code> <code>float</code> <p>Threshold for detecting a gap; i.e. if the difference (in samples) between consecutive timestamps exceeds gap_proportion, it is considered a gap. Must be &gt; 1. Default to 2.5</p> <code>2.5</code> <code>min_valid_len</code> <code>float</code> <p>Length of smallest valid interval. Default to 0. If greater than interval duration, print warning and use half the total time.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>valid_times</code> <code>ndarray</code> <p>Array of start and stop times of shape (N, 2) for valid data.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_valid_intervals(\n    timestamps, sampling_rate, gap_proportion=2.5, min_valid_len=0\n):\n    \"\"\"Finds the set of all valid intervals in a list of timestamps.\n    Valid interval: (start time, stop time) during which there are\n    no gaps (i.e. missing samples).\n\n    Parameters\n    ----------\n    timestamps : numpy.ndarray\n        1D numpy array of timestamp values.\n    sampling_rate : float\n        Sampling rate of the data.\n    gap_proportion : float, optional\n        Threshold for detecting a gap; i.e. if the difference (in samples)\n        between consecutive timestamps exceeds gap_proportion, it is considered\n        a gap. Must be &gt; 1. Default to 2.5\n    min_valid_len : float, optional\n        Length of smallest valid interval. Default to 0. If greater\n        than interval duration, print warning and use half the total time.\n\n    Returns\n    -------\n    valid_times : np.ndarray\n        Array of start and stop times of shape (N, 2) for valid data.\n    \"\"\"\n\n    eps = 0.0000001\n\n    total_time = timestamps[-1] - timestamps[0]\n\n    if total_time &lt; min_valid_len:\n        half_total_time = total_time / 2\n        print(f\"WARNING: Setting minimum valid interval to {half_total_time}\")\n        min_valid_len = half_total_time\n\n    # get rid of NaN elements\n    timestamps = timestamps[~np.isnan(timestamps)]\n    # find gaps\n    gap = np.diff(timestamps) &gt; 1.0 / sampling_rate * gap_proportion\n\n    # all true entries of gap represent gaps. Get the times bounding these intervals.\n    gapind = np.asarray(np.where(gap))\n    # The end of each valid interval are the indices of the gaps and the final value\n    valid_end = np.append(gapind, np.asarray(len(timestamps) - 1))\n\n    # the beginning of the gaps are the first element and gapind+1\n    valid_start = np.insert(gapind + 1, 0, 0)\n\n    valid_indices = np.vstack([valid_start, valid_end]).transpose()\n\n    valid_times = timestamps[valid_indices]\n    # adjust the times to deal with single valid samples\n    valid_times[:, 0] = valid_times[:, 0] - eps\n    valid_times[:, 1] = valid_times[:, 1] + eps\n\n    valid_intervals = (valid_times[:, 1] - valid_times[:, 0]) &gt; min_valid_len\n\n    return valid_times[valid_intervals, :]\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_electrode_indices", "title": "<code>get_electrode_indices(nwb_object, electrode_ids)</code>", "text": "<p>Return indices of the specified electrode_ids given an NWB file.</p> <p>Also accepts electrical series object. If an ElectricalSeries is given, then the indices returned are relative to the selected rows in ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the index of the matching electrode in electricalseries.electrodes.</p> <p>Indices for electrode_ids that are not in the electrical series are returned as np.nan</p> <p>If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_object</code> <code>NWBFile or ElectricalSeries</code> <p>The NWB file object or NWB electrical series object.</p> required <code>electrode_ids</code> <code>ndarray or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>list</code> <p>Array of indices of the specified electrode IDs.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_electrode_indices(nwb_object, electrode_ids):\n    \"\"\"Return indices of the specified electrode_ids given an NWB file.\n\n    Also accepts electrical series object. If an ElectricalSeries is given,\n    then the indices returned are relative to the selected rows in\n    ElectricalSeries.electrodes. For example, if electricalseries.electrodes =\n    [5], and row index 5 of nwbfile.electrodes has ID 10, then calling\n    get_electrode_indices(electricalseries, 10) will return 0, the index of the\n    matching electrode in electricalseries.electrodes.\n\n    Indices for electrode_ids that are not in the electrical series are\n    returned as np.nan\n\n    If an NWBFile is given, then the row indices with the matching IDs in the\n    file's electrodes table are returned.\n\n    Parameters\n    ----------\n    nwb_object : pynwb.NWBFile or pynwb.ecephys.ElectricalSeries\n        The NWB file object or NWB electrical series object.\n    electrode_ids : np.ndarray or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : list\n        Array of indices of the specified electrode IDs.\n    \"\"\"\n    if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n        # electrodes is a DynamicTableRegion which may contain a subset of the\n        # rows in NWBFile.electrodes match against only the subset of\n        # electrodes referenced by this ElectricalSeries\n        electrode_table_indices = nwb_object.electrodes.data[:]\n        selected_elect_ids = [\n            nwb_object.electrodes.table.id[x] for x in electrode_table_indices\n        ]\n    elif isinstance(nwb_object, pynwb.NWBFile):\n        # electrodes is a DynamicTable that contains all electrodes\n        selected_elect_ids = list(nwb_object.electrodes.id[:])\n    else:\n        raise ValueError(\n            \"nwb_object must be of type ElectricalSeries or NWBFile\"\n        )\n\n    # for each electrode_id, find its index in selected_elect_ids and return\n    # that if it's there and invalid_electrode_index if not.\n\n    return [\n        selected_elect_ids.index(elect_id)\n        if elect_id in selected_elect_ids\n        else invalid_electrode_index\n        for elect_id in electrode_ids\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_all_spatial_series", "title": "<code>get_all_spatial_series(nwbf, verbose=False, incl_times=True)</code>", "text": "<p>Given an NWB, get the spatial series and return a dictionary by epoch.</p> <p>If incl_times is True, then the valid intervals are included in the output.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>NWBFile</code> <p>The source NWB file object.</p> required <code>verbose</code> <code>bool</code> <p>Flag representing whether to print the sampling rate.</p> <code>False</code> <code>incl_times</code> <code>bool</code> <p>Include valid times in the output. Default, True. Set to False for only spatial series object IDs.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>pos_data_dict</code> <code>dict</code> <p>Dict mapping indices to a dict with keys 'valid_times' and 'raw_position_object_id'. Returns None if there is no position data in the file. The 'raw_position_object_id' is the object ID of the SpatialSeries object.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_all_spatial_series(nwbf, verbose=False, incl_times=True) -&gt; dict:\n    \"\"\"\n    Given an NWB, get the spatial series and return a dictionary by epoch.\n\n    If incl_times is True, then the valid intervals are included in the output.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    verbose : bool\n        Flag representing whether to print the sampling rate.\n    incl_times : bool\n        Include valid times in the output. Default, True. Set to False for only\n        spatial series object IDs.\n\n    Returns\n    -------\n    pos_data_dict : dict\n        Dict mapping indices to a dict with keys 'valid_times' and\n        'raw_position_object_id'. Returns None if there is no position data in\n        the file. The 'raw_position_object_id' is the object ID of the\n        SpatialSeries object.\n    \"\"\"\n    pos_interface = get_data_interface(\n        nwbf, \"position\", pynwb.behavior.Position\n    )\n\n    if pos_interface is None:\n        return None\n\n    return _get_pos_dict(\n        position=pos_interface.spatial_series,\n        epoch_groups=_get_epoch_groups(pos_interface),\n        session_id=nwbf.session_id,\n        verbose=verbose,\n        incl_times=incl_times,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_nwb_copy_filename", "title": "<code>get_nwb_copy_filename(nwb_file_name)</code>", "text": "<p>Get file name of copy of nwb file without the electrophys data</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_copy_filename(nwb_file_name):\n    \"\"\"Get file name of copy of nwb file without the electrophys data\"\"\"\n\n    filename, file_extension = os.path.splitext(nwb_file_name)\n\n    if filename.endswith(\"_\"):\n        print(f\"WARNING: File may already be a copy: {nwb_file_name}\")\n\n    return f\"{filename}_{file_extension}\"\n</code></pre>"}, {"location": "misc/figurl_views/", "title": "Creating figurl views", "text": ""}, {"location": "misc/figurl_views/#spike-sorting-recording-view", "title": "Spike sorting recording view", "text": "<pre><code>import spyglass.common as ndc\nimport spyglass.figurl_views as ndf\n\nquery = ...\n\n# To replace:\n# (ndf.SpikeSortingRecordingView &amp; query).delete()\n\nndf.SpikeSortingRecordingView.populate([(ndc.SpikeSortingRecording &amp; query).proj()])\n</code></pre>"}, {"location": "misc/figurl_views/#spike-sorting-view", "title": "Spike sorting view", "text": "<pre><code>import spyglass.common as ndc\nimport spyglass.figurl_views as ndf\n\nquery = ...\n\n# To replace:\n# (ndf.SpikeSortingView &amp; query).delete()\n\nndf.SpikeSortingView.populate([(ndc.SpikeSorting &amp; query).proj()])\n</code></pre>"}, {"location": "misc/insert_data/", "title": "How to insert data into <code>spyglass</code>", "text": "<p>In <code>spyglass</code>, every table corresponds to an object. An experimental session is defined as a collection of such objects. When an NWB file is ingested into <code>spyglass</code>, the information about these objects is first read and inserted into tables in the <code>common</code> module (e.g. <code>Institution</code>, <code>Lab</code>, <code>Electrode</code>, etc). However, not every NWB file has all the information required by <code>spyglass</code>. For example, many NWB files do not contain any information about the <code>DataAcquisitionDevice</code> or <code>Probe</code> because NWB does not yet have an official standard for specifying them. In addition, one might find that the information contained in the NWB file is incorrect and would like to modify it before inserting it into <code>spyglass</code> without having to go through the time-consuming process of re-generating the NWB file. For these cases, we provide an alternative approach to inserting data to <code>spyglass</code>.</p> <p>This alternate approach consists of two steps. First, the user must identify entries that they would like to add to the <code>spyglass</code> database that exist independently of any particular NWB file. For example, information about a particular probe is stored in the <code>ProbeType</code> and <code>Probe</code> tables of <code>spyglass.common</code>. The user can either:</p> <ol> <li> <p>create these entries programmatically using DataJoint <code>insert</code> commands, for    example:</p> <pre><code>sgc.ProbeType.insert1({\n\"probe_type\": \"128c-4s6mm6cm-15um-26um-sl\",\n\"probe_description\": \"A Livermore flexible probe with 128 channels, 4 shanks, 6 mm shank length, 6 cm ribbon length. 15 um contact diameter, 26 um center-to-center distance (pitch), single-line configuration.\",\n\"manufacturer\": \"Lawrence Livermore National Lab\",\n\"num_shanks\": 4,\n}, skip_duplicates=True)\n</code></pre> </li> <li> <p>define these entries in a special YAML file called <code>entries.yaml</code> that is    processed when <code>spyglass</code> is imported. One can think of <code>entries.yaml</code> as a    place to define information that the database should come pre-equipped prior    to ingesting any NWB files. The <code>entries.yaml</code> file should be placed in the    <code>spyglass</code> base directory. An example can be found in    <code>examples/config_yaml/entries.yaml</code>. It has the following structure:</p> <pre><code>TableName:\n- TableEntry1Field1: Value\nTableEntry1Field2: Value\n- TableEntry2Field1: Value\nTableEntry2Field2: Value\n</code></pre> <p>For example,</p> <pre><code>ProbeType:\n- probe_type: 128c-4s6mm6cm-15um-26um-sl\nprobe_description: A Livermore flexible probe with 128 channels, 4 shanks, 6 mm shank length, 6 cm ribbon length. 15 um contact diameter, 26 um center-to-center distance (pitch), single-line configuration.\nmanufacturer: Lawrence Livermore National Lab\nnum_shanks: 4\n</code></pre> </li> </ol> <p>Using a YAML file over programmatically creating these entries in a notebook or script has the advantages that the YAML file maintains a record of what entries have been added that is easy to access, and the file is portable and can be shared alongside an NWB file or set of NWB files from a given experiment.</p> <p>Next, the user must associate the NWB file with entries defined in the database. This is done by cresqating a configuration file, which must: be in the same directory as the NWB file that it configures be in YAML format have the following naming convention: <code>&lt;name_of_nwb_file&gt;_spyglass_config.yaml</code>.</p> <p>Users can programmatically generate this configuration file. It is then read by spyglass when calling <code>insert_session</code> on the associated NWB file.</p> <p>An example of this can be found at <code>examples/config_yaml/\u200b\u200bsub-AppleBottom_ses-AppleBottom-DY20-g3_behavior+ecephys_spyglass_config.yaml</code>. This file is associated with the NWB file <code>sub-AppleBottom_ses-AppleBottom-DY20-g3_behavior+ecephys.nwb</code>.</p> <p>This is the general format for the config entry:</p> <pre><code>TableName:\n- primary_key1: value1\n</code></pre> <p>For example:</p> <pre><code>DataAcquisitionDevice:\n- data_acquisition_device_name: Neuropixels Recording Device\n</code></pre> <p>In this example, the NWB file that corresponds to this config YAML will become associated with the DataAcquisitionDevice with primary key data_acquisition_device_name: Neuropixels Recording Device. This entry must exist.</p>"}, {"location": "misc/merge_tables/", "title": "Merge Tables", "text": ""}, {"location": "misc/merge_tables/#why", "title": "Why", "text": "<p>A pipeline may diverge when we want to process the same data in different ways. Merge Tables allow us to join divergent pipelines together, and unify downstream processing steps. For a more in depth discussion, please refer to this notebook and related discussions here and here.</p> <p>Note: Deleting entries upstream of Merge Tables will throw errors related to deleting a part entry before the master. To circumvent this, you can add <code>force_parts=True</code> to the <code>delete</code> function call, but this will leave and orphaned primary key in the master. Instead, use <code>spyglass.utils.dj_merge_tables.delete_downstream_merge</code> to delete master/part pairs.</p>"}, {"location": "misc/merge_tables/#what", "title": "What", "text": "<p>A Merge Table is fundamentally a master table with one part for each divergent pipeline. By convention...</p> <ol> <li> <p>The master table has one primary key, <code>merge_id</code>, a    UUID, and one    secondary attribute, <code>source</code>, which gives the part table name. Both are    managed with the custom <code>insert</code> function of this class.</p> </li> <li> <p>Each part table has inherits the final table in its respective pipeline, and    shares the same name as this table.</p> </li> </ol> <pre><code>from spyglass.utils.dj_merge_tables import _Merge\n\n@schema\nclass MergeOutput(_Merge):\n    definition = \"\"\"\n    merge_id: uuid\n    ---\n    source: varchar(32)\n    \"\"\"\n\n    class One(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; One\n        \"\"\"\n\n    class Two(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; Two\n        \"\"\"\n</code></pre> <p></p> <p>By convention, Merge Tables have been named with the pipeline name plus <code>Output</code> (e.g., <code>LFPOutput</code>, <code>PositionOutput</code>). Using the underscore alias for this class allows us to circumvent a DataJoint protection that interprets the class as a table itself.</p>"}, {"location": "misc/merge_tables/#how", "title": "How", "text": ""}, {"location": "misc/merge_tables/#merging", "title": "Merging", "text": "<p>The Merge class in Spyglass's utils is a subclass of DataJoint's Manual Table and adds functions to make the awkwardness of part tables more manageable. These functions are described in the API section, under <code>utils.dj_merge_tables</code>.</p>"}, {"location": "misc/merge_tables/#restricting", "title": "Restricting", "text": "<p>In short: restrict Merge Tables with arguments, not the <code>&amp;</code> operator.</p> <ul> <li>Normally: <code>Table &amp; \"field='value'\"</code></li> <li>Instead: <code>MergeTable.merge_view(restriction=\"field='value'\"</code>).</li> </ul> <p>Caution. The <code>&amp;</code> operator may look like it's working when using <code>dict</code>, but this is because invalid keys will be ignored. <code>Master &amp; {'part_field':'value'}</code> is equivalent to <code>Master</code> alone (source).</p> <p>When provided as arguments, methods like <code>merge_get_part</code> and <code>merge_get_parent</code> will override the permissive treatment of mappings described above to only return relevant tables.</p>"}, {"location": "misc/merge_tables/#building-downstream", "title": "Building Downstream", "text": "<p>A downstream analysis will ideally be able to use all diverget pipelines interchangeably. If there are parameters that may be required for downstream processing, they should be included in the final table of the pipeline. In the example above, both <code>One</code> and <code>Two</code> might have a secondary key <code>params</code>. A downstream Computed table could do the following:</p> <pre><code>def make(self, key):\n    try:\n        params = MergeTable.merge_get_parent(restriction=key).fetch('params')\n    except DataJointError:\n        params = default_params\n    processed_data = self.processing_func(key, params)\n</code></pre> <p>Note that the <code>try/except</code> above catches a possible error in the event <code>params</code> is not present in the parent.</p>"}, {"location": "misc/merge_tables/#example", "title": "Example", "text": "<p>For example usage, see our Merge Table notebook.</p>"}, {"location": "misc/session_groups/", "title": "Session groups", "text": "<p>A session group is a collection of sessions. Each group has a name (primary key) and a description.</p> <pre><code>from spyglass.common import SessionGroup\n\n# Create a new session group\nSessionGroup.add_group('test_group_1', 'Description of test group 1')\n\n# Get the table of session groups\nSessionGroup()\n\n# Add a session to the group\nSessionGroup.add_session_to_group('RN2_20191110_.nwb', 'test_group_1')\n\n# Remove a session from a group\n# SessionGroup.remove_session_from_group('RN2_20191110_.nwb', 'test_group_1')\n\n# Get all sessions in group\nSessionGroup.get_group_sessions('test_group_1')\n\n# Update the description of a session group\nSessionGroup.update_session_group_description('test_group_1', 'Test description')\n</code></pre>"}, {"location": "notebooks/00_Setup/", "title": "Setup", "text": "<p>Welcome to Spyglass, a DataJoint pipeline maintained by the Frank Lab at UCSF.</p> <p>Spyglass will help you take an NWB file from raw data to analysis-ready preprocessed formats using DataJoint to (a) connect to a relational database (here, MySQL), and (b) automate processing steps. To use Spyglass, you'll need to ...</p> <ol> <li>Set up your local environment</li> <li>Connect to a database</li> </ol> <p>You have a few options for databases.</p> <ol> <li>Connect to an existing database.</li> <li>Use GitHub Codespaces (coming soon...)</li> <li>Run your own database with Docker</li> </ol> <p>Once your database is set up, be sure to configure the connection with your <code>dj_local_conf.json</code> file.</p> <p>Members of the Frank Lab can run the <code>dj_config.py</code> helper script to generate a config like the one below. Outside users should copy/paste <code>dj_local_conf_example</code> and adjust values accordingly.</p> <pre>cd spyglass\npython config/dj_config.py &lt;username&gt; &lt;base_path&gt; &lt;output_filename&gt;\n</pre> <p>The base path (formerly <code>SPYGLASS_BASE_DIR</code>) is the directory where all data will be saved. See also docs for more information on subdirectories.</p> <p>A different <code>output_filename</code> will save different files:</p> <ul> <li><code>dj_local_conf.json</code>: Recommended. Used for tutorials. A file in the current directory DataJoint will automatically recognize when a Python session is launched from this directory.</li> <li><code>.datajoint_config.json</code> or no input: A file in the user's home directory that will be referenced whenever no local version (see above) is present.</li> <li>Anything else: A custom name that will need to be loaded (e.g., <code>dj.load('x')</code>) for each python session.</li> </ul> <p>The config will be a <code>json</code> file like the following.</p> <pre>{\n    \"database.host\": \"lmf-db.cin.ucsf.edu\",\n    \"database.user\": \"&lt;username&gt;\",\n    \"database.password\": \"Not recommended for shared machines\",\n    \"database.port\": 3306,\n    \"database.use_tls\": true,\n    \"enable_python_native_blobs\": true,\n    \"filepath_checksum_size_limit\": 1 * 1024**3,\n    \"stores\": {\n        \"raw\": {\n            \"protocol\": \"file\",\n            \"location\": \"/stelmo/nwb/raw\",\n            \"stage\": \"/stelmo/nwb/raw\"\n        },\n        \"analysis\": {\n            \"protocol\": \"file\",\n            \"location\": \"/stelmo/nwb/analysis\",\n            \"stage\": \"/stelmo/nwb/analysis\"\n        }\n    },\n    \"custom\": {\n        \"spyglass_dirs\": {\n            \"base\": \"/stelmo/nwb/\"\n        }\n    }\n}\n</pre> In\u00a0[1]: Copied! <pre>import os\nimport datajoint as dj\n\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")\n\nfrom spyglass.settings import config\n\nconfig\n</pre> import os import datajoint as dj  if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  from spyglass.settings import config  config Out[1]: <pre>{'debug_mode': True,\n 'prepopulate': True,\n 'SPYGLASS_BASE_DIR': '/stelmo/nwb',\n 'SPYGLASS_RAW_DIR': '/stelmo/nwb/raw',\n 'SPYGLASS_ANALYSIS_DIR': '/stelmo/nwb/analysis',\n 'SPYGLASS_RECORDING_DIR': '/stelmo/nwb/recording',\n 'SPYGLASS_SORTING_DIR': '/stelmo/nwb/spikesorting',\n 'SPYGLASS_WAVEFORMS_DIR': '/stelmo/nwb/waveforms',\n 'SPYGLASS_TEMP_DIR': '/stelmo/nwb/tmp',\n 'SPYGLASS_VIDEO_DIR': '/stelmo/nwb/video',\n 'KACHERY_CLOUD_DIR': '/stelmo/nwb/kachery_storage',\n 'KACHERY_STORAGE_DIR': '/stelmo/nwb/kachery_storage',\n 'KACHERY_TEMP_DIR': '/stelmo/nwb/tmp',\n 'KACHERY_ZONE': 'franklab.default',\n 'FIGURL_CHANNEL': 'franklab2',\n 'DJ_SUPPORT_FILEPATH_MANAGEMENT': 'TRUE',\n 'KACHERY_CLOUD_EPHEMERAL': 'TRUE'}</pre> In\u00a0[2]: Copied! <pre>from spyglass.common import Nwbfile\n\nNwbfile()\n</pre> from spyglass.common import Nwbfile  Nwbfile() <pre>[2023-09-28 08:07:06,176][INFO]: Connecting root@localhost:3307\n[2023-09-28 08:07:06,254][INFO]: Connected root@localhost:3307\n</pre> <pre>Populate: Populating table DataAcquisitionDeviceSystem with data {'data_acquisition_device_system': 'SpikeGadgets'} using insert1.\nPopulate: Populating table DataAcquisitionDeviceAmplifier with data {'data_acquisition_device_amplifier': 'Intan'} using insert1.\n</pre> Out[2]: Table for holding the NWB files. <p>nwb_file_name</p> name of the NWB file <p>nwb_file_abs_path</p> CH101_20210711_.nwb =BLOB=CH73_20211206_.nwb =BLOB=CH65_20211212_.nwb =BLOB=J1620210620_.nwb =BLOB=montague20200802_.nwb =BLOB=chimi20200304_.nwb =BLOB=Wallie20220913_.nwb =BLOB=mango20211203_.nwb =BLOB=peanut20201108_.nwb =BLOB=wilbur20210406_.nwb =BLOB=eliot20221022_.nwb =BLOB=Dan20211109_.nwb =BLOB= <p>...</p> <p>Total: 817</p> <p>Next, we'll try inserting data</p>"}, {"location": "notebooks/00_Setup/#setup", "title": "Setup\u00b6", "text": ""}, {"location": "notebooks/00_Setup/#local-environment", "title": "Local environment\u00b6", "text": "<p>Codespace users can skip this step. Frank Lab members should first follow 'rec to nwb overview' steps on Google Drive to set up an ssh connection.</p> <p>For local use, download and install ...</p> <ol> <li>Python 3.9.</li> <li>mamba as a replacement for conda. Spyglass installation is significantly faster with mamba.</li> <li>VS Code with relevant python extensions, including Jupyter. Hold off on selecting your interpreter until after you make the environment with <code>mamba</code>.</li> <li>git for downloading the repository, including notebooks.</li> </ol> <p>In a terminal, ...</p> <ol> <li>navigate to your project directory.</li> <li>use <code>git</code> to download the Spyglass repository.</li> <li>navigate to the newly downloaded directory.</li> <li>create a <code>mamba</code> environment with either the standard <code>environment.yml</code> or the <code>environment_position.yml</code>, if you intend to use the full position pipeline. The latter will take longer to install.</li> <li>open this notebook with VSCode</li> </ol> <p>Commands for the steps above ...</p> <pre>cd /your/project/directory/ # 1\ngit clone https://github.com/LorenFrankLab/spyglass/ # 2\ncd spyglass # 3\nmamba env create -f environment.yml # 4\ncode notebooks/00_Setup.ipynb # 5\n</pre> <p>Note: Spyglass is also installable via pip and pypi with <code>pip install spyglass-neuro</code>, but downloading from GitHub will also other files accessible.</p> <p>Next, within VSCode, select the kernel that matches your spyglass environment created with <code>mamba</code>. To use other Python interfaces, be sure to activate the environment: <code>conda activate spyglass</code></p> <p>See this guide for additional details on each of these programs and the role they play in using the pipeline.</p>"}, {"location": "notebooks/00_Setup/#database-connection", "title": "Database Connection\u00b6", "text": ""}, {"location": "notebooks/00_Setup/#existing-database", "title": "Existing Database\u00b6", "text": ""}, {"location": "notebooks/00_Setup/#running-your-own-database", "title": "Running your own database\u00b6", "text": ""}, {"location": "notebooks/00_Setup/#setup-docker", "title": "Setup Docker\u00b6", "text": "<ul> <li><p>First, install Docker.</p> </li> <li><p>Add yourself to the <code>docker</code> group so that you don't have to be sudo to run docker.</p> </li> <li><p>Download the docker image for datajoint/mysql</p> <pre>docker pull datajoint/mysql\n</pre> </li> <li><p>When run, this is referred to as a 'Docker container'</p> </li> <li><p>Next start the container with a couple additional pieces of info...</p> <ul> <li>Root password. We use <code>tutorial</code>.</li> <li>Database name. Here, we use <code>spyglass-db</code>.</li> <li>Port mapping. Here, we map 3306 across the local machine and container.</li> </ul> <pre>docker run --name spyglass-db -p 3306:3306 -e MYSQL_ROOT_PASSWORD=tutorial datajoint/mysql\n</pre> </li> <li><p>For data to persist after terminating the container, attach a volume when running:</p> <pre>docker volume create dj-vol\ndocker run --name spyglass-db -v dj-vol:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=tutorial datajoint/mysql\n</pre> </li> </ul>"}, {"location": "notebooks/00_Setup/#configure", "title": "Configure\u00b6", "text": "<p>The <code>dj_local_conf_example.json</code> contains all the defaults for a Docker connection. Simply rename to <code>dj_local_conf.json</code> and modify the contents accordingly. This includes the host, password and user. For Spyglass, you'll want to set your base path under <code>custom</code>:</p> <pre>{\n  \"database.host\": \"localhost\",\n  \"database.password\": \"tutorial\",\n  \"database.user\": \"root\",\n  \"custom\": {\n    \"database.prefix\": \"username_\",\n    \"spyglass_dirs\": {\n      \"base\": \"/your/base/path\"\n    }\n  }\n}\n</pre>"}, {"location": "notebooks/00_Setup/#loading-the-config", "title": "Loading the config\u00b6", "text": "<p>We can check that the paths are correctly set up by loading the config from the main Spyglass directory.</p>"}, {"location": "notebooks/00_Setup/#connect", "title": "Connect\u00b6", "text": "<p>Now, you should be able to connect to the database you set up.</p> <p>Let's demonstrate with an example table:</p>"}, {"location": "notebooks/00_Setup/#up-next", "title": "Up Next\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/", "title": "Insert Data", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see these additional tutorials</li> </ul> <p>Let's start by importing the <code>spyglass</code> package, along with a few others.</p> In\u00a0[1]: Copied! <pre>import os\nimport datajoint as dj\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\nwarnings.simplefilter(\"ignore\", category=UserWarning)\n\n# spyglass.common has the most frequently used tables\nimport spyglass.common as sgc\n\n# spyglass.data_import has tools for inserting NWB files into the database\nimport spyglass.data_import as sgi\n</pre> import os import datajoint as dj  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) warnings.simplefilter(\"ignore\", category=UserWarning)  # spyglass.common has the most frequently used tables import spyglass.common as sgc  # spyglass.data_import has tools for inserting NWB files into the database import spyglass.data_import as sgi <pre>[2023-10-05 11:48:12,292][INFO]: Connecting root@localhost:3306\n[2023-10-05 11:48:12,302][INFO]: Connected root@localhost:3306\n</pre> <p>Datajoint enables users to use Python to build and interact with a Relational Database. In a Relational Data Model, each table is an object that can reference information in other tables to avoid redundancy.</p> <p>DataJoint has built-in tools for generating/saving a Diagram of the relationships between tables. This page describes the notation used.</p> <p>Polygons are tables, colors reference table type:</p> <ul> <li>Green rectangle: tables whose entries are entered manually.</li> <li>Blue oval: tables whose entries are imported from external files (e.g. NWB file).</li> <li>Red circle: tables whose entries are computed from entries of other tables.</li> <li>No shape (only text): tables whose entries are part of the table upstream</li> </ul> <p>Lines are dependencies between tables. An upstream table is connected to a downstream table via inheritance of the primary key. This is the set of attributes (i.e., column names) used to uniquely define an entry (i.e., a row)</p> <ul> <li>Bold lines: the upstream primary key is the sole downstream primary key</li> <li>Solid lines: the upstream table as part of the downstream primary key</li> <li>Dashed lines: the primary key of upstream table as non-primary key</li> </ul> In\u00a0[2]: Copied! <pre># Draw tables that are two levels below and one level above Session\ndj.Diagram(sgc.Session) - 1 + 2\n</pre> # Draw tables that are two levels below and one level above Session dj.Diagram(sgc.Session) - 1 + 2 Out[2]: <p>By adding diagrams together, of adding and subtracting levels, we can visualize key parts of Spyglass.</p> <p>Note: Notice the Selection tables. This is a design pattern that selects a subset of upstream items for further processing. In some cases, these also pair the selected data with processing parameters.</p> <p>After exploring the pipeline's structure, we'll now grab some example data. Spyglass will assume that the data is a neural recording with relevant auxiliary in NWB.</p> <p>We offer a few examples:</p> <ul> <li><code>minirec20230622.nwb</code>, .3 GB: minimal recording, Link</li> <li><code>mediumnwb20230802.nwb</code>, 32 GB: full-featured dataset, Link</li> <li><code>montague20200802.nwb</code>, 8 GB: full experimental recording, Link</li> <li>For those in the UCSF network, these and many others on <code>/stelmo/nwb/raw</code></li> </ul> <p>If you are connected to the Frank lab database, please rename any downloaded files (e.g., <code>example20200101_yourname.nwb</code>) to avoid naming collisions, as the file name acts as the primary key across key tables.</p> In\u00a0[2]: Copied! <pre>from spyglass.utils.nwb_helper_fn import get_nwb_copy_filename\n\n# Define the name of the file that you copied and renamed\nnwb_file_name = \"minirec20230622.nwb\"\nnwb_copy_file_name = get_nwb_copy_filename(nwb_file_name)\n</pre> from spyglass.utils.nwb_helper_fn import get_nwb_copy_filename  # Define the name of the file that you copied and renamed nwb_file_name = \"minirec20230622.nwb\" nwb_copy_file_name = get_nwb_copy_filename(nwb_file_name) <p>Spyglass will create a copy with this name.</p> In\u00a0[4]: Copied! <pre>nwb_copy_file_name\n</pre> nwb_copy_file_name Out[4]: <pre>'minirec20230622_.nwb'</pre> <p>Let's start small by inserting personnel information.</p> <p>The <code>LabMember</code> table lists all lab members, with an additional part table for <code>LabMemberInfo</code>. This holds Google account and DataJoint username info for each member, for authentication purposes.</p> <p>We can insert lab member information using the NWB file <code>experimenter</code> field as follows...</p> In\u00a0[5]: Copied! <pre># take a look at the lab members\nsgc.LabMember.insert_from_nwbfile(nwb_file_name)\n</pre> # take a look at the lab members sgc.LabMember.insert_from_nwbfile(nwb_file_name) <pre>Please add the Google user ID for Firstname Lastname in the LabMember.LabMemberInfo table to help manage permissions.\nPlease add the Google user ID for Firstname2 Lastname2 in the LabMember.LabMemberInfo table to help manage permissions.\n</pre> <p>We can insert into <code>LabMemberInfo</code> directly with a list of lists that reflect the order of the fields present in the table. See this notebook for examples of inserting with <code>dicts</code>.</p> In\u00a0[6]: Copied! <pre>sgc.LabMember.LabMemberInfo.insert(\n    [  # Full name, Google email address, DataJoint username\n        [\"Firstname Lastname\", \"example1@gmail.com\", \"example1\"],\n        [\"Firstname2 Lastname2\", \"example2@gmail.com\", \"example2\"],\n    ],\n    skip_duplicates=True,\n)\nsgc.LabMember.LabMemberInfo()\n</pre> sgc.LabMember.LabMemberInfo.insert(     [  # Full name, Google email address, DataJoint username         [\"Firstname Lastname\", \"example1@gmail.com\", \"example1\"],         [\"Firstname2 Lastname2\", \"example2@gmail.com\", \"example2\"],     ],     skip_duplicates=True, ) sgc.LabMember.LabMemberInfo() Out[6]: Information about lab member in the context of Frank lab network <p>lab_member_name</p> <p>google_user_name</p> used for permission to curate <p>datajoint_user_name</p> used for permission to delete entries Firstname Lastname example1@gmail.com example1Firstname2 Lastname2 example2@gmail.com example2 <p>Total: 2</p> <p>A <code>LabTeam</code> is a set of lab members who own a set of NWB files and the associated information in the database. This is often a subgroup that collaborates on the same projects. Data is associated with a given team, granting members analysis (e.g., curation) and deletion (coming soon) privileges.</p> In\u00a0[7]: Copied! <pre>sgc.LabTeam().create_new_team(\n    team_name=\"My Team\",  # Should be unique\n    team_members=[\"Firstname Lastname\", \"Firstname2 Lastname2\"],\n    team_description=\"test\",  # Optional\n)\n</pre> sgc.LabTeam().create_new_team(     team_name=\"My Team\",  # Should be unique     team_members=[\"Firstname Lastname\", \"Firstname2 Lastname2\"],     team_description=\"test\",  # Optional ) <p>By default, each member is part of their own team. We can see all teams and members by looking at the part table <code>LabTeam.LabTeamMember</code>.</p> In\u00a0[8]: Copied! <pre>sgc.LabTeam.LabTeamMember()\n</pre> sgc.LabTeam.LabTeamMember() Out[8]: <p>team_name</p> <p>lab_member_name</p> Firstname Lastname Firstname LastnameMy Team Firstname LastnameFirstname2 Lastname2 Firstname2 Lastname2My Team Firstname2 Lastname2 <p>Total: 4</p> <p><code>spyglass.data_import.insert_sessions</code> helps take the many fields of data present in an NWB file and insert them into various tables across Spyglass. If the NWB file is properly composed, this includes...</p> <ul> <li>the experimenter (replicating part of the process above)</li> <li>animal behavior (e.g. video recording of position)</li> <li>neural activity (extracellular recording of multiple brain areas)</li> <li>etc.</li> </ul> <p>Note: this may take time as Spyglass creates the copy. You may see a prompt about inserting device information.</p> In\u00a0[3]: Copied! <pre>sgi.insert_sessions(nwb_file_name)\n</pre> sgi.insert_sessions(nwb_file_name) <pre>Creating a copy of NWB file minirec20230622.nwb with link to raw ephys data: minirec20230622_.nwb\nPopulate Session...\nNo config found at file path /home/cb/wrk/data/raw/minirec20230622_spyglass_config.yaml\nInstitution...\nLab...\nLabMember...\nPlease add the Google user ID for Firstname2 Lastname2 in the LabMember.LabMemberInfo table to help manage permissions.\nSubject...\nPopulate CameraDevice...\nInserted camera devices ['test camera 1']\n\nPopulate Probe...\nProbe ID '128c-4s6mm6cm-15um-26um-sl' already exists in the database. Spyglass will use that and not create a new Probe, Shanks, or Electrodes.\nInserted probes {'128c-4s6mm6cm-15um-26um-sl'}\n\nSkipping Apparatus for now...\nIntervalList...\nLabMember with name lastname, firstname does not exist. Cannot link Session with LabMember in Session.Experimenter.\nLabMember with name lastname2, firstname2 does not exist. Cannot link Session with LabMember in Session.Experimenter.\nPopulate ElectrodeGroup...\nPopulate Electrode...\nNo config found at file path /home/cb/wrk/data/raw/minirec20230622_spyglass_config.yaml\nPopulate Raw...\nEstimating sampling rate...\nEstimated sampling rate for file: 30000.0 Hz\nImporting raw data: Sampling rate:\t30000.0 Hz\nNumber of valid intervals:\t2\nPopulate SampleCount...\nPopulate DIOEvents...\nPopulate TaskEpochs\nPopulate StateScriptFile\nPopulate VideoFile\nNo video found corresponding to file minirec20230622_.nwb, epoch 01_s1\nNo video found corresponding to file minirec20230622_.nwb, epoch 02_s2\nRawPosition...\nEstimated sampling rate for 12345: 30.0 Hz\nWARNING: Setting minimum valid interval to 5.1912336349487305\nEstimated sampling rate for 12345: 30.0 Hz\nWARNING: Setting minimum valid interval to 5.1912336349487305\nEstimated sampling rate for 12345: 30.0 Hz\nWARNING: Setting minimum valid interval to 5.339195609092712\nEstimated sampling rate for 12345: 30.0 Hz\nWARNING: Setting minimum valid interval to 5.339195609092712\nPopulated PosIntervalMap for minirec20230622_.nwb, 01_s1\nPopulated PosIntervalMap for minirec20230622_.nwb, 02_s2\n</pre> <p>To look at data, we can query a table with <code>Table()</code> syntax.</p> In\u00a0[11]: Copied! <pre>sgc.Lab()\n</pre> sgc.Lab() Out[11]: <p>lab_name</p> Loren Frank Lab <p>Total: 1</p> <p>The <code>Session</code> table has considerably more fields</p> In\u00a0[12]: Copied! <pre>sgc.Session.heading.names\n</pre> sgc.Session.heading.names Out[12]: <pre>['nwb_file_name',\n 'subject_id',\n 'institution_name',\n 'lab_name',\n 'session_id',\n 'session_description',\n 'session_start_time',\n 'timestamps_reference_time',\n 'experiment_description']</pre> <p>But a short primary key</p> In\u00a0[13]: Copied! <pre>sgc.Session.heading.primary_key\n</pre> sgc.Session.heading.primary_key Out[13]: <pre>['nwb_file_name']</pre> <p>The primary key is shown in bold in the html</p> In\u00a0[14]: Copied! <pre>sgc.Session()\n</pre> sgc.Session() Out[14]: Table for holding experimental sessions. <p>nwb_file_name</p> name of the NWB file <p>subject_id</p> <p>institution_name</p> <p>lab_name</p> <p>session_id</p> <p>session_description</p> <p>session_start_time</p> <p>timestamps_reference_time</p> <p>experiment_description</p> minirec20230622_.nwb 54321 UCSF Loren Frank Lab 12345 test yaml insertion 2023-06-22 15:59:58 1970-01-01 00:00:00 Test Conversion <p>Total: 1</p> <p>Text only interfaces designate the primary key fields with <code>*</code></p> In\u00a0[15]: Copied! <pre>print(sgc.Session())\n</pre> print(sgc.Session()) <pre>*nwb_file_name subject_id     institution_na lab_name       session_id     session_descri session_start_ timestamps_ref experiment_des\n+------------+ +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ +------------+\nminirec2023062 54321          UCSF           Loren Frank La 12345          test yaml inse 2023-06-22 15: 1970-01-01 00: Test Conversio\n (Total: 1)\n\n</pre> <p>To see a the table definition, including data types, use <code>describe</code>.</p> <ul> <li><code>---</code> separates the primary key</li> <li><code>:</code> are used to separate field name from data type</li> <li><code>#</code> can be used to add comments to a field</li> </ul> In\u00a0[16]: Copied! <pre>from pprint import pprint  # adds line breaks\n\npprint(sgc.Session.describe())\n</pre> from pprint import pprint  # adds line breaks  pprint(sgc.Session.describe()) <pre>('# Table for holding experimental sessions.\\n'\n '-&gt; sgc.Nwbfile\\n'\n '---\\n'\n '-&gt; [nullable] sgc.Subject\\n'\n '-&gt; [nullable] sgc.Institution\\n'\n '-&gt; [nullable] sgc.Lab\\n'\n 'session_id=null      : varchar(200)                 \\n'\n 'session_description  : varchar(2000)                \\n'\n 'session_start_time   : datetime                     \\n'\n 'timestamps_reference_time : datetime                     \\n'\n 'experiment_description=null : varchar(2000)                \\n')\n</pre> <p>To look at specific entries in a table, we can use the <code>&amp;</code> operator. Below, we restrict based on a <code>dict</code>, but you can also use a string.</p> In\u00a0[17]: Copied! <pre>sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[17]: Table for holding experimental sessions. <p>nwb_file_name</p> name of the NWB file <p>subject_id</p> <p>institution_name</p> <p>lab_name</p> <p>session_id</p> <p>session_description</p> <p>session_start_time</p> <p>timestamps_reference_time</p> <p>experiment_description</p> minirec20230622_.nwb 54321 UCSF Loren Frank Lab 12345 test yaml insertion 2023-06-22 15:59:58 1970-01-01 00:00:00 Test Conversion <p>Total: 1</p> <p><code>Raw</code> is connected to <code>Session</code> with a bold line, so it has the same primary key.</p> In\u00a0[18]: Copied! <pre>dj.Diagram(sgc.Session) + dj.Diagram(sgc.Raw)\n</pre> dj.Diagram(sgc.Session) + dj.Diagram(sgc.Raw) Out[18]: In\u00a0[19]: Copied! <pre>sgc.Raw &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.Raw &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[19]: Raw voltage timeseries data, ElectricalSeries in NWB. <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>raw_object_id</p> the NWB object ID for loading this object from the file <p>sampling_rate</p> Sampling rate calculated from data, in Hz <p>comments</p> <p>description</p> minirec20230622_.nwb raw data valid times 4e756642-9203-4f00-b9d0-0e9747c14493 30000.0 No comment Recording of extracellular voltage <p>Total: 1</p> <p><code>IntervalList</code> is connected to <code>Session</code> with a solid line because it has additional primary key attributes. Here, you need to know both <code>nwb_file_name</code> and <code>interval_list_name</code> to uniquely identify an entry.</p> In\u00a0[20]: Copied! <pre># join/split condenses long spaces before field comments\npprint(\"\".join(sgc.IntervalList.describe().split(\"  \")))\n</pre> # join/split condenses long spaces before field comments pprint(\"\".join(sgc.IntervalList.describe().split(\"  \"))) <pre>('# Time intervals used for analysis\\n'\n '-&gt; sgc.Session\\n'\n 'interval_list_name : varchar(200) # descriptive name of this interval list\\n'\n '---\\n'\n 'valid_times: longblob # numpy array with start and end times for each '\n 'interval\\n')\n</pre> In\u00a0[21]: Copied! <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[21]: Time intervals used for analysis <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>valid_times</p> numpy array with start and end times for each interval minirec20230622_.nwb 01_s1 =BLOB=minirec20230622_.nwb 02_s2 =BLOB=minirec20230622_.nwb pos 0 valid times =BLOB=minirec20230622_.nwb pos 1 valid times =BLOB=minirec20230622_.nwb raw data valid times =BLOB= <p>Total: 5</p> <p>Raw data types like <code>valid_times</code> are shown as <code>=BLOB=</code>. We can inspect these with <code>fetch</code></p> <p>Note: like <code>insert</code>/<code>insert1</code>, <code>fetch</code> can be uses as <code>fetch1</code> to raise an error when many (or no) entries are retrieved. To limit to one entry when there may be many, use <code>query.fetch(limit=1)[0]</code></p> In\u00a0[22]: Copied! <pre>(\n    sgc.IntervalList\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n    }\n).fetch1(\"valid_times\")\n</pre> (     sgc.IntervalList     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",     } ).fetch1(\"valid_times\") Out[22]: <pre>array([[1.68747483e+09, 1.68747484e+09]])</pre> <p>In DataJoint operators, <code>&amp;</code> selects by a condition and <code>-</code> removes a condition.</p> In\u00a0[23]: Copied! <pre>(\n    (\n        (sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name})\n        - {\"interval_list_name\": \"pos 1 valid times\"}\n    )\n    - {\"interval_list_name\": \"pos 2 valid times\"}\n).fetch(\"interval_list_name\")\n</pre> (     (         (sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name})         - {\"interval_list_name\": \"pos 1 valid times\"}     )     - {\"interval_list_name\": \"pos 2 valid times\"} ).fetch(\"interval_list_name\") Out[23]: <pre>array(['01_s1', '02_s2', 'pos 0 valid times', 'raw data valid times'],\n      dtype=object)</pre> <p>Another neat feature of DataJoint is that it automatically maintains data integrity with cascading deletes. For example, if we delete our <code>Session</code> entry, all associated downstream entries are also deleted (e.g. <code>Raw</code>, <code>IntervalList</code>).</p> <p>Note: The deletion process can be complicated by Merge Tables when the entry is referenced by a part table. To demo deletion in these cases, run the hidden code below.</p> Quick Merge Insert <pre>import spyglass.lfp as lfp\n\nsgc.FirFilterParameters().create_standard_filters()\nlfp.lfp_electrode.LFPElectrodeGroup.create_lfp_electrode_group(\n    nwb_file_name=nwb_copy_file_name,\n    group_name=\"test\",\n    electrode_list=[0],\n)\nlfp_key = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"lfp_electrode_group_name\": \"test\",\n    \"target_interval_list_name\": \"01_s1\",\n    \"filter_name\": \"LFP 0-400 Hz\",\n    \"filter_sampling_rate\": 30_000,\n}\nlfp.v1.LFPSelection.insert1(lfp_key, skip_duplicates=True)\nlfp.v1.LFPV1().populate(lfp_key)\n</pre> Deleting Merge Entries <pre>from spyglass.utils.dj_merge_tables import delete_downstream_merge\n\ndelete_downstream_merge(\n    sgc.Nwbfile(),\n    restriction={\"nwb_file_name\": nwb_copy_file_name},\n    dry_run=False, # True will show Merge Table entries that would be deleted\n)\n</pre> In\u00a0[24]: Copied! <pre>session_entry = sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}\nsession_entry\n</pre> session_entry = sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name} session_entry Out[24]: Table for holding experimental sessions. <p>nwb_file_name</p> name of the NWB file <p>subject_id</p> <p>institution_name</p> <p>lab_name</p> <p>session_id</p> <p>session_description</p> <p>session_start_time</p> <p>timestamps_reference_time</p> <p>experiment_description</p> minirec20230622_.nwb 54321 UCSF Loren Frank Lab 12345 test yaml insertion 2023-06-22 15:59:58 1970-01-01 00:00:00 Test Conversion <p>Total: 1</p> <p>By default, DataJoint is cautious about deletes and will prompt before deleting. To delete, uncomment the cell below and respond <code>yes</code> in the prompt.</p> In\u00a0[26]: Copied! <pre>session_entry.delete()\n</pre> session_entry.delete() <pre>[2023-09-28 08:29:15,814][INFO]: Deleting 4 rows from `common_behav`.`_raw_position__pos_object`\nINFO:datajoint:Deleting 4 rows from `common_behav`.`_raw_position__pos_object`\n[2023-09-28 08:29:15,822][INFO]: Deleting 2 rows from `common_behav`.`_raw_position`\nINFO:datajoint:Deleting 2 rows from `common_behav`.`_raw_position`\n[2023-09-28 08:29:15,834][INFO]: Deleting 4 rows from `common_behav`.`position_source__spatial_series`\nINFO:datajoint:Deleting 4 rows from `common_behav`.`position_source__spatial_series`\n[2023-09-28 08:29:15,841][INFO]: Deleting 2 rows from `common_behav`.`position_source`\nINFO:datajoint:Deleting 2 rows from `common_behav`.`position_source`\n[2023-09-28 08:29:15,851][INFO]: Deleting 7 rows from `common_dio`.`_d_i_o_events`\nINFO:datajoint:Deleting 7 rows from `common_dio`.`_d_i_o_events`\n[2023-09-28 08:29:15,871][INFO]: Deleting 128 rows from `common_ephys`.`_electrode`\nINFO:datajoint:Deleting 128 rows from `common_ephys`.`_electrode`\n[2023-09-28 08:29:15,879][INFO]: Deleting 1 rows from `common_ephys`.`_electrode_group`\nINFO:datajoint:Deleting 1 rows from `common_ephys`.`_electrode_group`\n[2023-09-28 08:29:15,887][INFO]: Deleting 1 rows from `common_ephys`.`_raw`\nINFO:datajoint:Deleting 1 rows from `common_ephys`.`_raw`\n[2023-09-28 08:29:15,896][INFO]: Deleting 1 rows from `common_ephys`.`_sample_count`\nINFO:datajoint:Deleting 1 rows from `common_ephys`.`_sample_count`\n[2023-09-28 08:29:15,908][INFO]: Deleting 2 rows from `common_behav`.`__position_interval_map`\nINFO:datajoint:Deleting 2 rows from `common_behav`.`__position_interval_map`\n[2023-09-28 08:29:15,918][INFO]: Deleting 2 rows from `common_task`.`_task_epoch`\nINFO:datajoint:Deleting 2 rows from `common_task`.`_task_epoch`\n[2023-09-28 08:29:15,924][INFO]: Deleting 5 rows from `common_interval`.`interval_list`\nINFO:datajoint:Deleting 5 rows from `common_interval`.`interval_list`\n[2023-09-28 08:29:15,931][INFO]: Deleting 1 rows from `common_session`.`_session`\nINFO:datajoint:Deleting 1 rows from `common_session`.`_session`\n[2023-09-28 08:29:18,765][INFO]: Deletes committed.\nINFO:datajoint:Deletes committed.\n</pre> Out[26]: <pre>1</pre> <p>We can check that delete worked, both for <code>Session</code> and <code>IntervalList</code></p> In\u00a0[27]: Copied! <pre>sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[27]: Table for holding experimental sessions. <p>nwb_file_name</p> name of the NWB file <p>subject_id</p> <p>institution_name</p> <p>lab_name</p> <p>session_id</p> <p>session_description</p> <p>session_start_time</p> <p>timestamps_reference_time</p> <p>experiment_description</p> <p>Total: 0</p> In\u00a0[28]: Copied! <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[28]: Time intervals used for analysis <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>valid_times</p> numpy array with start and end times for each interval <p>Total: 0</p> <p><code>delete</code> is useful for re-running something. Editing entries is possible, but discouraged because it can lead to integrity issues. Instead, re-enter and let the automation handle the rest.</p> <p>Spyglass falls short, however, in that deleting from <code>Session</code> doesn't also delete the associated entry in <code>Nwbfile</code>, which has to be removed separately (for now). This table offers a <code>cleanup</code> method to remove the added files (with the <code>delete_files</code> argument as <code>True</code>).</p> <p>Note: this also applies to deleting files from <code>AnalysisNwbfile</code> table.</p> In\u00a0[53]: Copied! <pre># Uncomment to delete\n# (sgc.Nwbfile &amp; {\"nwb_file_name\": nwb_copy_file_name}).delete()\n</pre> # Uncomment to delete # (sgc.Nwbfile &amp; {\"nwb_file_name\": nwb_copy_file_name}).delete() <pre>[2023-07-18 19:01:15,343][INFO]: Deleting 1 rows from `common_nwbfile`.`nwbfile`\nINFO:datajoint:Deleting 1 rows from `common_nwbfile`.`nwbfile`\n[2023-07-18 19:01:17,130][INFO]: Deletes committed.\nINFO:datajoint:Deletes committed.\n</pre> Out[53]: <pre>1</pre> <p>Note that the file (ends with <code>_.nwb</code>) has not been deleted, even if the entry was deleted above.</p> In\u00a0[\u00a0]: Copied! <pre>!ls $SPYGLASS_BASE_DIR/raw\n</pre> !ls $SPYGLASS_BASE_DIR/raw <pre>minirec20230622.nwb\nminirec20230622_.nwb\nmontague20200802_tutorial.nwb\nmontague20200802_tutorial_.nwb\nmontague20200802_tutorial__.nwb\nsub-despereaux_ses-despereaux-08_behavior+ecephys_trim.nwb\nsub-despereaux_ses-despereaux-08_behavior+ecephys_trim_.nwb\ntonks20211103_.nwb\n</pre> <p>We can clean these files with the <code>cleanup</code> method</p> In\u00a0[55]: Copied! <pre>sgc.Nwbfile().cleanup(delete_files=True)\n</pre> sgc.Nwbfile().cleanup(delete_files=True) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 304.24it/s]\n</pre> In\u00a0[56]: Copied! <pre>!ls $SPYGLASS_BASE_DIR/raw\n</pre> !ls $SPYGLASS_BASE_DIR/raw <pre>minirec20230622.nwb\nmontague20200802_tutorial.nwb\nmontague20200802_tutorial_.nwb\nmontague20200802_tutorial__.nwb\nsub-despereaux_ses-despereaux-08_behavior+ecephys_trim.nwb\nsub-despereaux_ses-despereaux-08_behavior+ecephys_trim_.nwb\ntonks20211103_.nwb\n</pre> <p>In the next notebook, we'll explore tools for syncing.</p>"}, {"location": "notebooks/01_Insert_Data/#insert-data", "title": "Insert Data\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#intro", "title": "Intro\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#visualizing-the-database", "title": "Visualizing the database\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#example-data", "title": "Example data\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#basic-inserts-lab-team", "title": "Basic Inserts: Lab Team\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#inserting-from-nwb", "title": "Inserting from NWB\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#inspecting-the-data", "title": "Inspecting the data\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#deleting-data", "title": "Deleting data\u00b6", "text": ""}, {"location": "notebooks/01_Insert_Data/#up-next", "title": "Up Next\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/", "title": "Data Sync", "text": "<p>This notebook will cover ...</p> <ol> <li>General Kachery information</li> <li>Setting up Kachery as a host. If you'll use an existing host, skip this.</li> <li>Setting up Kachery in your database. If you're using an existing database, skip this.</li> <li>Adding Kachery data.</li> </ol> <p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>To fully demonstrate syncing features, we'll need to run some basic analyses. This can either be done with code in this notebook or by running another notebook (e.g., LFP)</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see these additional tutorials</li> </ul> <p>Let's start by importing the <code>spyglass</code> package.</p> In\u00a0[3]: Copied! <pre>import os\nimport datajoint as dj\nimport pandas as pd\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection\n\nimport spyglass.common as sgc\nimport spyglass.sharing as sgs\nfrom spyglass.settings import config\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import os import datajoint as dj import pandas as pd  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection  import spyglass.common as sgc import spyglass.sharing as sgs from spyglass.settings import config  import warnings  warnings.filterwarnings(\"ignore\") <pre>[2023-09-28 09:39:48,974][INFO]: Connecting root@localhost:3307\n[2023-09-28 09:39:49,050][INFO]: Connected root@localhost:3307\n</pre> <p>For example analysis files, run the code hidden below.</p> Quick Analysis <pre>from spyglass.utils.nwb_helper_fn import get_nwb_copy_filename\nimport spyglass.data_import as sgi\nimport spyglass.lfp as lfp\n\nnwb_file_name = \"minirec20230622.nwb\"\nnwb_copy_file_name = get_nwb_copy_filename(nwb_file_name)\n\nsgi.insert_sessions(nwb_file_name)\nsgc.FirFilterParameters().create_standard_filters()\nlfp.lfp_electrode.LFPElectrodeGroup.create_lfp_electrode_group(\n    nwb_file_name=nwb_copy_file_name,\n    group_name=\"test\",\n    electrode_list=[0],\n)\nlfp.v1.LFPSelection.insert1(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"lfp_electrode_group_name\": \"test\",\n        \"target_interval_list_name\": \"01_s1\",\n        \"filter_name\": \"LFP 0-400 Hz\",\n        \"filter_sampling_rate\": 30_000,\n    },\n    skip_duplicates=True,\n)\nlfp.v1.LFPV1().populate()\n</pre> <p>This notebook contains instructions for setting up data sharing/syncing through Kachery Cloud, which makes it possible to share analysis results, stored in NWB files. When a user tries to access a file, Spyglass does the following:</p> <ol> <li>Try to load from the local file system/store.</li> <li>If unavailable, check if it is in the relevant sharing table (i.e., <code>NwbKachery</code> or <code>AnalysisNWBKachery</code>).</li> <li>If present, attempt to download from the associated Kachery Resource.</li> </ol> <p>Note: large file downloads may take a long time, so downloading raw data is not supported. We suggest direct transfer with globus or a similar service.</p> <p>A Kachery Zone is a cloud storage host. The Frank laboratory has three separate Kachery zones:</p> <ol> <li><code>franklab.default</code>: Internal file sharing, including figurls</li> <li><code>franklab.collaborator</code>: File sharing with collaborating labs.</li> <li><code>franklab.public</code>: Public file sharing (not yet active)</li> </ol> <p>Setting your zone can either be done as as an environment variable or an item in a DataJoint config.</p> <ul> <li><p>Environment variable:</p> <pre>export KACHERY_ZONE=franklab.default\nexport KACHERY_CLOUD_DIR=/stelmo/nwb/.kachery_cloud\n</pre> </li> <li><p>DataJoint Config:</p> <pre>\"custom\": {\n   \"kachery_zone\": \"franklab.default\",\n   \"kachery_dirs\": {\n      \"cloud\": \"/your/base/path/.kachery_cloud\"\n   }\n}\n</pre> </li> </ul> <p>See instructions for setting up new Kachery Zones, including creating a cloud bucket and registering it with the Kachery team.</p> <p>Notes:</p> <ul> <li>Bucket names cannot include periods, so we substitute a dash, as in <code>franklab-default</code>.</li> <li>You only need to create an API token for your first zone.</li> </ul> <p>See instructions for setting up zone resources. This allows for sharing files on demand. We suggest using the same name for the zone and resource.</p> <p>Note: For each zone, you need to run the local daemon that listens for requests from that zone. An example of the bash script we use is</p> <pre>export KACHERY_ZONE=franklab.collaborators\n    export KACHERY_CLOUD_DIR=/stelmo/nwb/.kachery_cloud\n    cd /stelmo/nwb/franklab_collaborators_resource\n    npx kachery-resource@latest share\n</pre> <p>We'll add zones/resources to the Spyglass database. First, we'll check existing Zones.</p> In\u00a0[3]: Copied! <pre>sgs.KacheryZone()\n</pre> sgs.KacheryZone() Out[3]: <p>kachery_zone_name</p> the name of the kachery zone. Note that this is the same as the name of the kachery resource. <p>description</p> description of this zone <p>kachery_cloud_dir</p> kachery cloud directory on local machine where files are linked <p>kachery_proxy</p> kachery sharing proxy <p>lab_name</p> <p>Total: 0</p> <p>Check existing file list:</p> In\u00a0[4]: Copied! <pre>sgs.AnalysisNwbfileKachery()\n</pre> sgs.AnalysisNwbfileKachery() Out[4]: <p>kachery_zone_name</p> the name of the kachery zone. Note that this is the same as the name of the kachery resource. <p>analysis_file_name</p> name of the file <p>analysis_file_uri</p> the uri of the file <p>Total: 0</p> <p>Prepare an entry for the <code>KacheryZone</code> table:</p> In\u00a0[38]: Copied! <pre>zone_name = config.get(\"KACHERY_ZONE\")\ncloud_dir = config.get(\"KACHERY_CLOUD_DIR\")\n\nzone_key = {\n    \"kachery_zone_name\": zone_name,\n    \"description\": \" \".join(zone_name.split(\".\")) + \" zone\",\n    \"kachery_cloud_dir\": cloud_dir,\n    \"kachery_proxy\": \"https://kachery-resource-proxy.herokuapp.com\",\n    \"lab_name\": sgc.Lab.fetch(\"lab_name\", limit=1)[0],\n}\n</pre> zone_name = config.get(\"KACHERY_ZONE\") cloud_dir = config.get(\"KACHERY_CLOUD_DIR\")  zone_key = {     \"kachery_zone_name\": zone_name,     \"description\": \" \".join(zone_name.split(\".\")) + \" zone\",     \"kachery_cloud_dir\": cloud_dir,     \"kachery_proxy\": \"https://kachery-resource-proxy.herokuapp.com\",     \"lab_name\": sgc.Lab.fetch(\"lab_name\", limit=1)[0], } <p>Use caution when inserting into an active database, as it could interfere with ongoing work.</p> In\u00a0[39]: Copied! <pre>sgs.KacheryZone().insert1(zone_key)\n</pre> sgs.KacheryZone().insert1(zone_key) <p>Once the zone exists, we can add <code>AnalysisNWB</code> files we want to share by adding entries to the <code>AnalysisNwbfileKacherySelection</code> table.</p> <p>Note: This step depends on having previously run an analysis on the example file.</p> In\u00a0[40]: Copied! <pre>nwb_copy_filename = \"minirec20230622_.nwb\"\n\nanalysis_file_list = (  # Grab all analysis files for this nwb file\n    sgc.AnalysisNwbfile() &amp; {\"nwb_file_name\": nwb_copy_filename}\n).fetch(\"analysis_file_name\")\n\nkachery_selection_key = {\"kachery_zone_name\": zone_name}\n\nfor file in analysis_file_list:  # Add all analysis to shared list\n    kachery_selection_key[\"analysis_file_name\"] = file\n    sgs.AnalysisNwbfileKacherySelection.insert1(\n        kachery_selection_key, skip_duplicates=True\n    )\n</pre> nwb_copy_filename = \"minirec20230622_.nwb\"  analysis_file_list = (  # Grab all analysis files for this nwb file     sgc.AnalysisNwbfile() &amp; {\"nwb_file_name\": nwb_copy_filename} ).fetch(\"analysis_file_name\")  kachery_selection_key = {\"kachery_zone_name\": zone_name}  for file in analysis_file_list:  # Add all analysis to shared list     kachery_selection_key[\"analysis_file_name\"] = file     sgs.AnalysisNwbfileKacherySelection.insert1(         kachery_selection_key, skip_duplicates=True     ) <p>With those files in the selection table, we can add them as links to the zone by populating the <code>AnalysisNwbfileKachery</code> table:</p> In\u00a0[\u00a0]: Copied! <pre>sgs.AnalysisNwbfileKachery.populate()\n</pre> sgs.AnalysisNwbfileKachery.populate() <p>If all of that worked,</p> <ol> <li>Go to https://kachery-gateway.figurl.org/admin?zone=your_zone (changing your_zone to the name of your zone)</li> <li>Go to the Admin/Authorization Settings tab</li> <li>Add the GitHub login names and permissions for the users you want to share with.</li> </ol> <p>If those users can connect to your database, they should now be able to use the <code>.fetch_nwb()</code> method to download any <code>AnalysisNwbfiles</code> that have been shared through Kachery.</p> <p>For example:</p> <pre>from spyglass.spikesorting import CuratedSpikeSorting\n\ntest_sort = (\n    CuratedSpikeSorting &amp; {\"nwb_file_name\": \"minirec20230622_.nwb\"}\n).fetch()[0]\nsort = (CuratedSpikeSorting &amp; test_sort).fetch_nwb()\n</pre> <p>In the next notebook, we'll explore the details of a table tier unique to Spyglass, Merge Tables.</p>"}, {"location": "notebooks/02_Data_Sync/#sync-data", "title": "Sync Data\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#kachery", "title": "Kachery\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#cloud", "title": "Cloud\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#zone", "title": "Zone\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#host-setup", "title": "Host Setup\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#zones", "title": "Zones\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#database-setup", "title": "Database Setup\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#data-setup", "title": "Data Setup\u00b6", "text": ""}, {"location": "notebooks/02_Data_Sync/#up-next", "title": "Up Next\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/", "title": "03 Merge Tables", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>To insert data, see the Insert Data notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see these additional tutorials</li> <li>For information on why we use merge tables, and how to make one, see our documentation</li> </ul> <p>Let's start by importing the <code>spyglass</code> package, along with a few others.</p> In\u00a0[1]: Copied! <pre>import os\nimport datajoint as dj\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\nwarnings.simplefilter(\"ignore\", category=UserWarning)\n\nimport spyglass.common as sgc\nimport spyglass.lfp as lfp\nfrom spyglass.utils.nwb_helper_fn import get_nwb_copy_filename\nfrom spyglass.utils.dj_merge_tables import delete_downstream_merge, Merge\nfrom spyglass.common.common_ephys import LFP as CommonLFP  # Upstream 1\nfrom spyglass.lfp.lfp_merge import LFPOutput  # Merge Table\nfrom spyglass.lfp.v1.lfp import LFPV1  # Upstream 2\n</pre> import os import datajoint as dj  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) warnings.simplefilter(\"ignore\", category=UserWarning)  import spyglass.common as sgc import spyglass.lfp as lfp from spyglass.utils.nwb_helper_fn import get_nwb_copy_filename from spyglass.utils.dj_merge_tables import delete_downstream_merge, Merge from spyglass.common.common_ephys import LFP as CommonLFP  # Upstream 1 from spyglass.lfp.lfp_merge import LFPOutput  # Merge Table from spyglass.lfp.v1.lfp import LFPV1  # Upstream 2 <pre>[2023-10-12 11:15:17,864][INFO]: Connecting root@localhost:3306\n[2023-10-12 11:15:17,873][INFO]: Connected root@localhost:3306\n</pre> <p>Check to make sure the data inserted in the previour notebook is still there.</p> In\u00a0[2]: Copied! <pre>nwb_file_name = \"minirec20230622.nwb\"\nnwb_copy_file_name = get_nwb_copy_filename(nwb_file_name)\nnwb_file_dict = {\"nwb_file_name\": nwb_copy_file_name}\nsgc.Session &amp; nwb_file_dict\n</pre> nwb_file_name = \"minirec20230622.nwb\" nwb_copy_file_name = get_nwb_copy_filename(nwb_file_name) nwb_file_dict = {\"nwb_file_name\": nwb_copy_file_name} sgc.Session &amp; nwb_file_dict Out[2]: Table for holding experimental sessions. <p>nwb_file_name</p> name of the NWB file <p>subject_id</p> <p>institution_name</p> <p>lab_name</p> <p>session_id</p> <p>session_description</p> <p>session_start_time</p> <p>timestamps_reference_time</p> <p>experiment_description</p> minirec20230622_.nwb 54321 UCSF Loren Frank Lab 12345 test yaml insertion 2023-06-22 15:59:58 1970-01-01 00:00:00 Test Conversion <p>Total: 1</p> <p>If you haven't already done so, insert data into a Merge Table.</p> <p>Note: Some existing parents of Merge Tables perform the Merge Table insert as part of the populate methods. This practice will be revised in the future.</p> In\u00a0[3]: Copied! <pre>sgc.FirFilterParameters().create_standard_filters()\nlfp.lfp_electrode.LFPElectrodeGroup.create_lfp_electrode_group(\n    nwb_file_name=nwb_copy_file_name,\n    group_name=\"test\",\n    electrode_list=[0],\n)\nlfp_key = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"lfp_electrode_group_name\": \"test\",\n    \"target_interval_list_name\": \"01_s1\",\n    \"filter_name\": \"LFP 0-400 Hz\",\n    \"filter_sampling_rate\": 30_000,\n}\nlfp.v1.LFPSelection.insert1(lfp_key, skip_duplicates=True)\nlfp.v1.LFPV1().populate(lfp_key)\nLFPOutput.insert([lfp_key], skip_duplicates=True)\n</pre> sgc.FirFilterParameters().create_standard_filters() lfp.lfp_electrode.LFPElectrodeGroup.create_lfp_electrode_group(     nwb_file_name=nwb_copy_file_name,     group_name=\"test\",     electrode_list=[0], ) lfp_key = {     \"nwb_file_name\": nwb_copy_file_name,     \"lfp_electrode_group_name\": \"test\",     \"target_interval_list_name\": \"01_s1\",     \"filter_name\": \"LFP 0-400 Hz\",     \"filter_sampling_rate\": 30_000, } lfp.v1.LFPSelection.insert1(lfp_key, skip_duplicates=True) lfp.v1.LFPV1().populate(lfp_key) LFPOutput.insert([lfp_key], skip_duplicates=True) <p>Merge Tables have multiple custom methods that begin with <code>merge</code>.</p> <p><code>help</code> can show us the docstring of each</p> In\u00a0[4]: Copied! <pre>merge_methods = [d for d in dir(Merge) if d.startswith(\"merge\")]\nprint(merge_methods)\n</pre> merge_methods = [d for d in dir(Merge) if d.startswith(\"merge\")] print(merge_methods) <pre>['merge_delete', 'merge_delete_parent', 'merge_fetch', 'merge_get_parent', 'merge_get_part', 'merge_html', 'merge_populate', 'merge_restrict', 'merge_view']\n</pre> In\u00a0[5]: Copied! <pre>help(getattr(Merge, merge_methods[-1]))\n</pre> help(getattr(Merge, merge_methods[-1])) <pre>Help on method merge_view in module spyglass.utils.dj_merge_tables:\n\nmerge_view(restriction: str = True) method of datajoint.user_tables.TableMeta instance\n    Prints merged view, including null entries for unique columns.\n    \n    Note: To handle this Union as a table-like object, use `merge_resrict`\n    \n    Parameters\n    ---------\n    restriction: str, optional\n        Restriction to apply to the merged view\n\n</pre> <p><code>merge_view</code> shows a union of the master and all part tables.</p> <p>Note: Restrict Merge Tables with arguments, not the <code>&amp;</code> operator.</p> <ul> <li>Normally: <code>Table &amp; \"field='value'\"</code></li> <li>Instead: <code>MergeTable.merge_view(restriction=\"field='value'\"</code>).</li> </ul> In\u00a0[7]: Copied! <pre>LFPOutput.merge_view()\n</pre> LFPOutput.merge_view() <pre>*merge_id      *source    *nwb_file_name *lfp_electrode *target_interv *filter_name   *filter_sampli\n+------------+ +--------+ +------------+ +------------+ +------------+ +------------+ +------------+\nc34f98c5-7de7- LFPV1      minirec2023062 test           01_s1          LFP 0-400 Hz   30000         \n (Total: 1)\n\n</pre> <p>UUIDs help retain unique entries across all part tables. We can fetch NWB file by referencing this or other features.</p> In\u00a0[8]: Copied! <pre>uuid_key = LFPOutput.fetch(limit=1, as_dict=True)[-1]\nrestrict = LFPOutput &amp; uuid_key\nrestrict\n</pre> uuid_key = LFPOutput.fetch(limit=1, as_dict=True)[-1] restrict = LFPOutput &amp; uuid_key restrict Out[8]: <p>merge_id</p> <p>source</p> c34f98c5-7de7-1daf-6eaf-1e15981def44 LFPV1 <p>Total: 1</p> In\u00a0[9]: Copied! <pre>result1 = restrict.fetch_nwb()\nresult1\n</pre> result1 = restrict.fetch_nwb() result1 Out[9]: <pre>[{'nwb_file_name': 'minirec20230622_.nwb',\n  'lfp_electrode_group_name': 'test',\n  'target_interval_list_name': '01_s1',\n  'filter_name': 'LFP 0-400 Hz',\n  'filter_sampling_rate': 30000,\n  'analysis_file_name': 'minirec20230622_JOV02AWW09.nwb',\n  'interval_list_name': 'lfp_test_01_s1_valid times',\n  'lfp_object_id': '340b9a0b-626b-40ca-8b48-e033be72570a',\n  'lfp_sampling_rate': 1000.0,\n  'lfp': filtered data pynwb.ecephys.ElectricalSeries at 0x139910624563552\n  Fields:\n    comments: no comments\n    conversion: 1.0\n    data: &lt;HDF5 dataset \"data\": shape (10476, 1), type \"&lt;i2\"&gt;\n    description: filtered data\n    electrodes: electrodes &lt;class 'hdmf.common.table.DynamicTableRegion'&gt;\n    interval: 1\n    offset: 0.0\n    resolution: -1.0\n    timestamps: &lt;HDF5 dataset \"timestamps\": shape (10476,), type \"&lt;f8\"&gt;\n    timestamps_unit: seconds\n    unit: volts}]</pre> In\u00a0[10]: Copied! <pre>nwb_key = LFPOutput.merge_restrict(nwb_file_dict).fetch(as_dict=True)[0]\nnwb_key\n</pre> nwb_key = LFPOutput.merge_restrict(nwb_file_dict).fetch(as_dict=True)[0] nwb_key Out[10]: <pre>{'merge_id': UUID('c34f98c5-7de7-1daf-6eaf-1e15981def44'),\n 'source': 'LFPV1',\n 'nwb_file_name': 'minirec20230622_.nwb',\n 'lfp_electrode_group_name': 'test',\n 'target_interval_list_name': '01_s1',\n 'filter_name': 'LFP 0-400 Hz',\n 'filter_sampling_rate': 30000}</pre> In\u00a0[12]: Copied! <pre>result2 = (LFPOutput &amp; nwb_key).fetch_nwb()\nresult2 == result1\n</pre> result2 = (LFPOutput &amp; nwb_key).fetch_nwb() result2 == result1 Out[12]: <pre>True</pre> <p>There are also functions for retrieving part/parent table(s) and fetching data.</p> <p>These <code>get</code> functions will either return the part table of the Merge table or the parent table with the source information for that part.</p> In\u00a0[14]: Copied! <pre>result4 = LFPOutput.merge_get_part(restriction=nwb_file_dict, join_master=True)\nresult4\n</pre> result4 = LFPOutput.merge_get_part(restriction=nwb_file_dict, join_master=True) result4 Out[14]: <p>merge_id</p> <p>source</p> <p>nwb_file_name</p> name of the NWB file <p>lfp_electrode_group_name</p> the name of this group of electrodes <p>target_interval_list_name</p> descriptive name of this interval list <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter c34f98c5-7de7-1daf-6eaf-1e15981def44 LFPV1 minirec20230622_.nwb test 01_s1 LFP 0-400 Hz 30000 <p>Total: 1</p> In\u00a0[15]: Copied! <pre>result5 = LFPOutput.merge_get_parent(restriction='nwb_file_name LIKE \"mini%\"')\nresult5\n</pre> result5 = LFPOutput.merge_get_parent(restriction='nwb_file_name LIKE \"mini%\"') result5 Out[15]: <p>nwb_file_name</p> name of the NWB file <p>lfp_electrode_group_name</p> the name of this group of electrodes <p>target_interval_list_name</p> descriptive name of this interval list <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>analysis_file_name</p> name of the file <p>interval_list_name</p> descriptive name of this interval list <p>lfp_object_id</p> the NWB object ID for loading this object from the file <p>lfp_sampling_rate</p> the sampling rate, in HZ minirec20230622_.nwb test 01_s1 LFP 0-400 Hz 30000 minirec20230622_JOV02AWW09.nwb lfp_test_01_s1_valid times 340b9a0b-626b-40ca-8b48-e033be72570a 1000.0 <p>Total: 1</p> <p><code>fetch</code> will collect all relevant entries and return them as a list in the format specified by keyword arguments and one's DataJoint config.</p> In\u00a0[16]: Copied! <pre>result6 = result5.fetch(\"lfp_sampling_rate\")  # Sample rate for all mini* files\nresult6\n</pre> result6 = result5.fetch(\"lfp_sampling_rate\")  # Sample rate for all mini* files result6 Out[16]: <pre>array([1000.])</pre> <p><code>merge_fetch</code> requires a restriction as the first argument. For no restriction, use <code>True</code>.</p> In\u00a0[19]: Copied! <pre>result7 = LFPOutput.merge_fetch(True, \"filter_name\", \"nwb_file_name\")\nresult7\n</pre> result7 = LFPOutput.merge_fetch(True, \"filter_name\", \"nwb_file_name\") result7 Out[19]: <pre>[array(['LFP 0-400 Hz'], dtype=object),\n array(['minirec20230622_.nwb'], dtype=object)]</pre> In\u00a0[20]: Copied! <pre>result8 = LFPOutput.merge_fetch(as_dict=True)\nresult8\n</pre> result8 = LFPOutput.merge_fetch(as_dict=True) result8 Out[20]: <pre>{'merge_id': UUID('c34f98c5-7de7-1daf-6eaf-1e15981def44'),\n 'nwb_file_name': 'minirec20230622_.nwb',\n 'lfp_electrode_group_name': 'test',\n 'target_interval_list_name': '01_s1',\n 'filter_name': 'LFP 0-400 Hz',\n 'filter_sampling_rate': 30000}</pre> <p>When deleting from Merge Tables, we can either...</p> <ol> <li><p>delete from the Merge Table itself with <code>merge_delete</code>, deleting both the master and part.</p> </li> <li><p>use <code>merge_delete_parent</code> to delete from the parent sources, getting rid of the entries in the source table they came from.</p> </li> <li><p>use <code>delete_downstream_merge</code> to find Merge Tables downstream and get rid full entries, avoiding orphaned master table entries.</p> </li> </ol> <p>The two latter cases can be destructive, so we include an extra layer of protection with <code>dry_run</code>. When true (by default), these functions return a list of tables with the entries that would otherwise be deleted.</p> In\u00a0[\u00a0]: Copied! <pre>LFPOutput.merge_delete(nwb_file_dict)  # Delete from merge table\nLFPOutput.merge_delete_parent(restriction=nwb_file_dict, dry_run=True)\ndelete_downstream_merge(\n    table=LFPV1,\n    restriction=nwb_file_dict,\n    dry_run=True,\n)\n</pre> LFPOutput.merge_delete(nwb_file_dict)  # Delete from merge table LFPOutput.merge_delete_parent(restriction=nwb_file_dict, dry_run=True) delete_downstream_merge(     table=LFPV1,     restriction=nwb_file_dict,     dry_run=True, ) <p>To delete all merge table entries associated with an NWB file, use <code>delete_downstream_merge</code> with the <code>Nwbfile</code> table.</p> In\u00a0[\u00a0]: Copied! <pre>delete_downstream_merge(\n    table=sgc.Nwbfile,\n    restriction={\"nwb_file_name\": nwb_copy_file_name},\n    dry_run=True,\n    recurse_level=3,  # for long pipelines with many tables\n)\n</pre> delete_downstream_merge(     table=sgc.Nwbfile,     restriction={\"nwb_file_name\": nwb_copy_file_name},     dry_run=True,     recurse_level=3,  # for long pipelines with many tables ) <p>In the next notebook, we'll start working with ephys data with spike sorting.</p>"}, {"location": "notebooks/03_Merge_Tables/#merge-tables", "title": "Merge Tables\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#intro", "title": "Intro\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#example-data", "title": "Example data\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#helper-functions", "title": "Helper functions\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#showing-data", "title": "Showing data\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#selecting-data", "title": "Selecting data\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#deletion-from-merge-tables", "title": "Deletion from Merge Tables\u00b6", "text": ""}, {"location": "notebooks/03_Merge_Tables/#up-next", "title": "Up Next\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/", "title": "Spike Sorting", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> Extract the recording <ol> <li>Specifying your NWB file.</li> <li>Specifying which electrodes involved in the recording to sort data from. - <code>SortGroup</code></li> <li>Specifying the time segment of the recording we want to sort. - <code>IntervalList</code>, <code>SortInterval</code></li> <li>Specifying the parameters to use for filtering the recording. - <code>SpikeSortingPreprocessingParameters</code></li> <li>Combining these parameters. - <code>SpikeSortingRecordingSelection</code></li> <li>Extracting the recording. - <code>SpikeSortingRecording</code></li> <li>Specifying the parameters to apply for artifact detection/removal. -<code>ArtifactDetectionParameters</code></li> </ol> Spike sorting the recording <ol> <li>Specify the spike sorter and parameters to use. - <code>SpikeSorterParameters</code></li> <li>Combine these parameters. - <code>SpikeSortingSelection</code></li> <li>Spike sort the extracted recording according to chose parameter set. - <code>SpikeSorting</code></li> </ol> <p> </p> In\u00a0[13]: Copied! <pre>import os\nimport datajoint as dj\nimport numpy as np\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.spikesorting as sgs\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import datajoint as dj import numpy as np  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.spikesorting as sgs  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <p>If you haven't already done so, add yourself to <code>LabTeam</code></p> In\u00a0[14]: Copied! <pre>name, email, dj_user = \"Firstname Lastname\", \"example@gmail.com\", \"user\"\nsgc.LabMember.insert_from_name(name)\nsgc.LabMember.LabMemberInfo.insert1(\n    [name, email, dj_user], skip_duplicates=True\n)\nsgc.LabTeam.LabTeamMember.insert1(\n    {\"team_name\": \"My Team\", \"lab_member_name\": name},\n    skip_duplicates=True,\n)\n</pre> name, email, dj_user = \"Firstname Lastname\", \"example@gmail.com\", \"user\" sgc.LabMember.insert_from_name(name) sgc.LabMember.LabMemberInfo.insert1(     [name, email, dj_user], skip_duplicates=True ) sgc.LabTeam.LabTeamMember.insert1(     {\"team_name\": \"My Team\", \"lab_member_name\": name},     skip_duplicates=True, ) <p>We can try <code>fetch</code> to confirm.</p> <p>Exercise: Try to write a fer lines to generate a dictionary with team names as keys and lists of members as values. It may be helpful to add more data with the code above and use <code>fetch(as_dict=True)</code>.</p> In\u00a0[15]: Copied! <pre>my_team_members = (\n    (sgc.LabTeam.LabTeamMember &amp; {\"team_name\": \"My Team\"})\n    .fetch(\"lab_member_name\")\n    .tolist()\n)\nif name in my_team_members:\n    print(\"You made it in!\")\n</pre> my_team_members = (     (sgc.LabTeam.LabTeamMember &amp; {\"team_name\": \"My Team\"})     .fetch(\"lab_member_name\")     .tolist() ) if name in my_team_members:     print(\"You made it in!\") <pre>You made it in!\n</pre> Code hidden here <pre>members = sgc.LabTeam.LabTeamMember.fetch(as_dict=True)\nteams_dict = {member[\"team_name\"]: [] for member in members}\nfor member in members:\n    teams_dict[member[\"team_name\"]].append(member[\"lab_member_name\"])\nprint(teams_dict)\n</pre> <p>If you haven't already, load an NWB file. For more details on downloading and importing data, see this notebook.</p> In\u00a0[16]: Copied! <pre>import spyglass.data_import as sdi\n\nsdi.insert_sessions(\"minirec20230622.nwb\")\nnwb_file_name = \"minirec20230622_.nwb\"\n</pre> import spyglass.data_import as sdi  sdi.insert_sessions(\"minirec20230622.nwb\") nwb_file_name = \"minirec20230622_.nwb\" <pre>/home/cb/wrk/spyglass/src/spyglass/data_import/insert_sessions.py:41: UserWarning: Cannot insert data from minirec20230622.nwb: minirec20230622_.nwbis already in Nwbfile table.\n  warnings.warn(\n</pre> <p>Each NWB file will have multiple electrodes we can use for spike sorting. We commonly use multiple electrodes in a <code>SortGroup</code> selected by what tetrode or shank of a probe they were on.</p> <p>Note: This will delete any existing entries. Answer 'yes' when prompted.</p> In\u00a0[17]: Copied! <pre>sgs.SortGroup().set_group_by_shank(nwb_file_name)\n</pre> sgs.SortGroup().set_group_by_shank(nwb_file_name) <pre>[2023-07-21 13:56:24,232][INFO]: Deleting 128 rows from `spikesorting_recording`.`sort_group__sort_group_electrode`\n[2023-07-21 13:56:24,234][INFO]: Deleting 4 rows from `spikesorting_recording`.`sort_group`\n[2023-07-21 13:56:27,358][INFO]: Deletes committed.\n</pre> <p>Each electrode has an <code>electrode_id</code> and is associated with an <code>electrode_group_name</code>, which corresponds with a <code>sort_group_id</code>.</p> <p>For example, data recorded from a 32 tetrode (128 channel) drive results in 128 unique <code>electrode_id</code>. This could result in 32 unique <code>electrode_group_name</code> and 32 unique <code>sort_group_id</code>.</p> In\u00a0[18]: Copied! <pre>sgs.SortGroup.SortGroupElectrode &amp; {\"nwb_file_name\": nwb_file_name}\n</pre> sgs.SortGroup.SortGroupElectrode &amp; {\"nwb_file_name\": nwb_file_name} Out[18]: <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>electrode_group_name</p> electrode group name from NWBFile <p>electrode_id</p> the unique number for this electrode minirec20230622_.nwb 0 0 0minirec20230622_.nwb 0 0 1minirec20230622_.nwb 0 0 2minirec20230622_.nwb 0 0 3minirec20230622_.nwb 0 0 4minirec20230622_.nwb 0 0 5minirec20230622_.nwb 0 0 6minirec20230622_.nwb 0 0 7minirec20230622_.nwb 0 0 8minirec20230622_.nwb 0 0 9minirec20230622_.nwb 0 0 10minirec20230622_.nwb 0 0 11 <p>...</p> <p>Total: 128</p> In\u00a0[19]: Copied! <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_file_name}\n</pre> sgc.IntervalList &amp; {\"nwb_file_name\": nwb_file_name} Out[19]: Time intervals used for analysis <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>valid_times</p> numpy array with start and end times for each interval minirec20230622_.nwb 01_s1 =BLOB=minirec20230622_.nwb 02_s2 =BLOB=minirec20230622_.nwb pos 0 valid times =BLOB=minirec20230622_.nwb pos 1 valid times =BLOB=minirec20230622_.nwb pos 2 valid times =BLOB=minirec20230622_.nwb pos 3 valid times =BLOB=minirec20230622_.nwb raw data valid times =BLOB= <p>Total: 7</p> <p>Let's start with the first run interval (<code>01_s1</code>) and fetch corresponding <code>valid_times</code>. For the <code>minirec</code> example, this is relatively short.</p> In\u00a0[20]: Copied! <pre>interval_list_name = \"01_s1\"\ninterval_list = (\n    sgc.IntervalList\n    &amp; {\"nwb_file_name\": nwb_file_name, \"interval_list_name\": interval_list_name}\n).fetch1(\"valid_times\")[0]\n\n\ndef print_interval_duration(interval_list: np.ndarray):\n    duration = np.round((interval_list[1] - interval_list[0]))\n    print(f\"This interval list is {duration:g} seconds long\")\n\n\nprint_interval_duration(interval_list)\n</pre> interval_list_name = \"01_s1\" interval_list = (     sgc.IntervalList     &amp; {\"nwb_file_name\": nwb_file_name, \"interval_list_name\": interval_list_name} ).fetch1(\"valid_times\")[0]   def print_interval_duration(interval_list: np.ndarray):     duration = np.round((interval_list[1] - interval_list[0]))     print(f\"This interval list is {duration:g} seconds long\")   print_interval_duration(interval_list) <pre>This interval list is 10 seconds long\n</pre> In\u00a0[21]: Copied! <pre>n = 9\nsort_interval_name = interval_list_name + f\"_first{n}\"\nsort_interval = np.array([interval_list[0], interval_list[0] + n])\n</pre> n = 9 sort_interval_name = interval_list_name + f\"_first{n}\" sort_interval = np.array([interval_list[0], interval_list[0] + n]) <p>With the above, we can insert into <code>SortInterval</code></p> In\u00a0[22]: Copied! <pre>sgs.SortInterval.insert1(\n    {\n        \"nwb_file_name\": nwb_file_name,\n        \"sort_interval_name\": sort_interval_name,\n        \"sort_interval\": sort_interval,\n    },\n    skip_duplicates=True,\n)\n</pre> sgs.SortInterval.insert1(     {         \"nwb_file_name\": nwb_file_name,         \"sort_interval_name\": sort_interval_name,         \"sort_interval\": sort_interval,     },     skip_duplicates=True, ) <p>And verify the entry</p> In\u00a0[23]: Copied! <pre>print_interval_duration(\n    (\n        sgs.SortInterval\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"sort_interval_name\": sort_interval_name,\n        }\n    ).fetch1(\"sort_interval\")\n)\n</pre> print_interval_duration(     (         sgs.SortInterval         &amp; {             \"nwb_file_name\": nwb_file_name,             \"sort_interval_name\": sort_interval_name,         }     ).fetch1(\"sort_interval\") ) <pre>This interval list is 9 seconds long\n</pre> <p><code>SpikeSortingPreprocessingParameters</code> contains the parameters used to filter the recorded data in the spike band prior to sorting.</p> In\u00a0[24]: Copied! <pre>sgs.SpikeSortingPreprocessingParameters()\n</pre> sgs.SpikeSortingPreprocessingParameters() Out[24]: <p>preproc_params_name</p> <p>preproc_params</p> default =BLOB= <p>Total: 1</p> <p>Here, we insert the default parameters and then fetch them.</p> In\u00a0[25]: Copied! <pre>sgs.SpikeSortingPreprocessingParameters().insert_default()\npreproc_params = (\n    sgs.SpikeSortingPreprocessingParameters()\n    &amp; {\"preproc_params_name\": \"default\"}\n).fetch1(\"preproc_params\")\nprint(preproc_params)\n</pre> sgs.SpikeSortingPreprocessingParameters().insert_default() preproc_params = (     sgs.SpikeSortingPreprocessingParameters()     &amp; {\"preproc_params_name\": \"default\"} ).fetch1(\"preproc_params\") print(preproc_params) <pre>{'frequency_min': 300, 'frequency_max': 6000, 'margin_ms': 5, 'seed': 0}\n</pre> <p>Let's adjust the <code>frequency_min</code> to 600, the preference for hippocampal data, and insert that into the table as a new set of parameters for hippocampal data.</p> In\u00a0[26]: Copied! <pre>preproc_params[\"frequency_min\"] = 600\nsgs.SpikeSortingPreprocessingParameters().insert1(\n    {\n        \"preproc_params_name\": \"default_hippocampus\",\n        \"preproc_params\": preproc_params,\n    },\n    skip_duplicates=True,\n)\n</pre> preproc_params[\"frequency_min\"] = 600 sgs.SpikeSortingPreprocessingParameters().insert1(     {         \"preproc_params_name\": \"default_hippocampus\",         \"preproc_params\": preproc_params,     },     skip_duplicates=True, ) In\u00a0[33]: Copied! <pre>interval_list_name\n</pre> interval_list_name Out[33]: <pre>'01_s1'</pre> In\u00a0[34]: Copied! <pre>ssr_key = dict(\n    nwb_file_name=nwb_file_name,\n    sort_group_id=0,  # See SortGroup\n    sort_interval_name=sort_interval_name,  # First N seconds above\n    preproc_params_name=\"default_hippocampus\",  # See preproc_params\n    interval_list_name=interval_list_name,\n    team_name=\"My Team\",\n)\n</pre> ssr_key = dict(     nwb_file_name=nwb_file_name,     sort_group_id=0,  # See SortGroup     sort_interval_name=sort_interval_name,  # First N seconds above     preproc_params_name=\"default_hippocampus\",  # See preproc_params     interval_list_name=interval_list_name,     team_name=\"My Team\", ) In\u00a0[35]: Copied! <pre>sgs.SpikeSortingRecordingSelection.insert1(ssr_key, skip_duplicates=True)\nsgs.SpikeSortingRecordingSelection() &amp; ssr_key\n</pre> sgs.SpikeSortingRecordingSelection.insert1(ssr_key, skip_duplicates=True) sgs.SpikeSortingRecordingSelection() &amp; ssr_key Out[35]: Defines recordings to be sorted <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>interval_list_name</p> descriptive name of this interval list minirec20230622_.nwb 0 01_s1_first9 default_hippocampus My Team 01_s1 <p>Total: 1</p> In\u00a0[38]: Copied! <pre>ssr_pk = (sgs.SpikeSortingRecordingSelection &amp; ssr_key).proj()\nsgs.SpikeSortingRecording.populate([ssr_pk])\n</pre> ssr_pk = (sgs.SpikeSortingRecordingSelection &amp; ssr_key).proj() sgs.SpikeSortingRecording.populate([ssr_pk]) <pre>write_binary_recording with n_jobs = 8 and chunk_size = 299593\n</pre> <pre>write_binary_recording:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <p>Now we can see our recording in the table. E x c i t i n g !</p> In\u00a0[39]: Copied! <pre>sgs.SpikeSortingRecording() &amp; ssr_key\n</pre> sgs.SpikeSortingRecording() &amp; ssr_key Out[39]: <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>recording_path</p> <p>sort_interval_list_name</p> descriptive name of this interval list minirec20230622_.nwb 0 01_s1_first9 default_hippocampus My Team /home/cb/wrk/zOther/data/recording/minirec20230622_.nwb_01_s1_first9_0_default_hippocampus minirec20230622_.nwb_01_s1_first9_0_default_hippocampus <p>Total: 1</p> In\u00a0[41]: Copied! <pre>sgs.ArtifactDetectionParameters().insert_default()\nartifact_key = (sgs.SpikeSortingRecording() &amp; ssr_key).fetch1(\"KEY\")\nartifact_key[\"artifact_params_name\"] = \"none\"\n</pre> sgs.ArtifactDetectionParameters().insert_default() artifact_key = (sgs.SpikeSortingRecording() &amp; ssr_key).fetch1(\"KEY\") artifact_key[\"artifact_params_name\"] = \"none\" <p>We then pair artifact detection parameters in <code>ArtifactParameters</code> with a recording extracted through population of <code>SpikeSortingRecording</code> and insert into <code>ArtifactDetectionSelection</code>.</p> In\u00a0[42]: Copied! <pre>sgs.ArtifactDetectionSelection().insert1(artifact_key)\nsgs.ArtifactDetectionSelection() &amp; artifact_key\n</pre> sgs.ArtifactDetectionSelection().insert1(artifact_key) sgs.ArtifactDetectionSelection() &amp; artifact_key Out[42]: Specifies artifact detection parameters to apply to a sort group's recording. <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>artifact_params_name</p> <p>custom_artifact_detection</p> minirec20230622_.nwb 0 01_s1_first9 default_hippocampus My Team none 0 <p>Total: 1</p> <p>Then, we can populate <code>ArtifactDetection</code>, which will find periods where there are artifacts, as specified by the parameters.</p> In\u00a0[43]: Copied! <pre>sgs.ArtifactDetection.populate(artifact_key)\n</pre> sgs.ArtifactDetection.populate(artifact_key) <pre>Amplitude and zscore thresholds are both None, skipping artifact detection\n</pre> <p>Populating <code>ArtifactDetection</code> also inserts an entry into <code>ArtifactRemovedIntervalList</code>, which stores the interval without detected artifacts.</p> In\u00a0[44]: Copied! <pre>sgs.ArtifactRemovedIntervalList() &amp; artifact_key\n</pre> sgs.ArtifactRemovedIntervalList() &amp; artifact_key Out[44]: Stores intervals without detected artifacts. <p>artifact_removed_interval_list_name</p> <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>artifact_params_name</p> <p>artifact_removed_valid_times</p> <p>artifact_times</p> np array of artifact intervals minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times minirec20230622_.nwb 0 01_s1_first9 default_hippocampus My Team none =BLOB= =BLOB= <p>Total: 1</p> In\u00a0[45]: Copied! <pre>sgs.SpikeSorterParameters().insert_default()\n\n# Let's look at the default params\nsorter_name = \"mountainsort4\"\nms4_default_params = (\n    sgs.SpikeSorterParameters\n    &amp; {\"sorter\": sorter_name, \"sorter_params_name\": \"default\"}\n).fetch1()\nprint(ms4_default_params)\n</pre> sgs.SpikeSorterParameters().insert_default()  # Let's look at the default params sorter_name = \"mountainsort4\" ms4_default_params = (     sgs.SpikeSorterParameters     &amp; {\"sorter\": sorter_name, \"sorter_params_name\": \"default\"} ).fetch1() print(ms4_default_params) <pre>{'sorter': 'mountainsort4', 'sorter_params_name': 'default', 'sorter_params': {'detect_sign': -1, 'adjacency_radius': -1, 'freq_min': 300, 'freq_max': 6000, 'filter': True, 'whiten': True, 'num_workers': 1, 'clip_size': 50, 'detect_threshold': 3, 'detect_interval': 10, 'tempdir': None}}\n</pre> <p>Now we can change these default parameters to line up more closely with our preferences.</p> In\u00a0[52]: Copied! <pre>sorter_params = {\n    **ms4_default_params[\"sorter_params\"],  # start with defaults\n    \"detect_sign\": -1,  # downward going spikes (1 for upward, 0 for both)\n    \"adjacency_radius\": 100,  # Sort electrodes together within 100 microns\n    \"filter\": False,  # No filter, since we filter prior to starting sort\n    \"freq_min\": 0,\n    \"freq_max\": 0,\n    \"whiten\": False,  # Turn whiten, since we whiten it prior to starting sort\n    \"num_workers\": 4,  #  same number as number of electrodes\n    \"verbose\": True,\n    \"clip_size\": np.int64(\n        1.33e-3  # same as # of samples for 1.33 ms based on the sampling rate\n        * (sgc.Raw &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\"sampling_rate\")\n    ),\n}\nfrom pprint import pprint\n\npprint(sorter_params)\n</pre> sorter_params = {     **ms4_default_params[\"sorter_params\"],  # start with defaults     \"detect_sign\": -1,  # downward going spikes (1 for upward, 0 for both)     \"adjacency_radius\": 100,  # Sort electrodes together within 100 microns     \"filter\": False,  # No filter, since we filter prior to starting sort     \"freq_min\": 0,     \"freq_max\": 0,     \"whiten\": False,  # Turn whiten, since we whiten it prior to starting sort     \"num_workers\": 4,  #  same number as number of electrodes     \"verbose\": True,     \"clip_size\": np.int64(         1.33e-3  # same as # of samples for 1.33 ms based on the sampling rate         * (sgc.Raw &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\"sampling_rate\")     ), } from pprint import pprint  pprint(sorter_params) <pre>{'adjacency_radius': 100,\n 'clip_size': 39,\n 'detect_interval': 10,\n 'detect_sign': -1,\n 'detect_threshold': 3,\n 'filter': False,\n 'freq_max': 0,\n 'freq_min': 0,\n 'num_workers': 4,\n 'tempdir': None,\n 'verbose': True,\n 'whiten': False}\n</pre> <p>We can give these <code>sorter_params</code> a <code>sorter_params_name</code> and insert into <code>SpikeSorterParameters</code>.</p> In\u00a0[53]: Copied! <pre>sorter_params_name = \"hippocampus_tutorial\"\nsgs.SpikeSorterParameters.insert1(\n    {\n        \"sorter\": sorter_name,\n        \"sorter_params_name\": sorter_params_name,\n        \"sorter_params\": sorter_params,\n    },\n    skip_duplicates=True,\n)\n(\n    sgs.SpikeSorterParameters\n    &amp; {\"sorter\": sorter_name, \"sorter_params_name\": sorter_params_name}\n).fetch1()\n</pre> sorter_params_name = \"hippocampus_tutorial\" sgs.SpikeSorterParameters.insert1(     {         \"sorter\": sorter_name,         \"sorter_params_name\": sorter_params_name,         \"sorter_params\": sorter_params,     },     skip_duplicates=True, ) (     sgs.SpikeSorterParameters     &amp; {\"sorter\": sorter_name, \"sorter_params_name\": sorter_params_name} ).fetch1() Out[53]: <pre>{'sorter': 'mountainsort4',\n 'sorter_params_name': 'hippocampus_tutorial',\n 'sorter_params': {'detect_sign': -1,\n  'adjacency_radius': 100,\n  'freq_min': 0,\n  'freq_max': 0,\n  'filter': False,\n  'whiten': False,\n  'num_workers': 4,\n  'clip_size': 39,\n  'detect_threshold': 3,\n  'detect_interval': 10,\n  'tempdir': None,\n  'verbose': True}}</pre> In\u00a0[59]: Copied! <pre>ss_key = dict(\n    **(sgs.ArtifactDetection &amp; ssr_key).fetch1(\"KEY\"),\n    **(sgs.ArtifactRemovedIntervalList() &amp; ssr_key).fetch1(\"KEY\"),\n    sorter=sorter_name,\n    sorter_params_name=sorter_params_name,\n)\nss_key.pop(\"artifact_params_name\")\nss_key\n</pre> ss_key = dict(     **(sgs.ArtifactDetection &amp; ssr_key).fetch1(\"KEY\"),     **(sgs.ArtifactRemovedIntervalList() &amp; ssr_key).fetch1(\"KEY\"),     sorter=sorter_name,     sorter_params_name=sorter_params_name, ) ss_key.pop(\"artifact_params_name\") ss_key Out[59]: <pre>{'nwb_file_name': 'minirec20230622_.nwb',\n 'sort_group_id': 0,\n 'sort_interval_name': '01_s1_first9',\n 'preproc_params_name': 'default_hippocampus',\n 'team_name': 'My Team',\n 'artifact_removed_interval_list_name': 'minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times',\n 'sorter': 'mountainsort4',\n 'sorter_params_name': 'hippocampus_tutorial'}</pre> In\u00a0[60]: Copied! <pre>sgs.SpikeSortingSelection.insert1(ss_key, skip_duplicates=True)\n(sgs.SpikeSortingSelection &amp; ss_key)\n</pre> sgs.SpikeSortingSelection.insert1(ss_key, skip_duplicates=True) (sgs.SpikeSortingSelection &amp; ss_key) Out[60]: Table for holding selection of recording and parameters for each spike sorting run <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>import_path</p> optional path to previous curated sorting output minirec20230622_.nwb 0 01_s1_first9 default_hippocampus My Team mountainsort4 hippocampus_tutorial minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times <p>Total: 1</p> In\u00a0[62]: Copied! <pre># [(sgs.SpikeSortingSelection &amp; ss_key).proj()]\nsgs.SpikeSorting.populate()\n</pre> # [(sgs.SpikeSortingSelection &amp; ss_key).proj()] sgs.SpikeSorting.populate() <pre>Running spike sorting on {'nwb_file_name': 'minirec20230622_.nwb', 'sort_group_id': 0, 'sort_interval_name': '01_s1_first9', 'preproc_params_name': 'default_hippocampus', 'team_name': 'My Team', 'sorter': 'mountainsort4', 'sorter_params_name': 'hippocampus_tutorial', 'artifact_removed_interval_list_name': 'minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times'}...\nMountainsort4 use the OLD spikeextractors mapped with NewToOldRecording\nUsing temporary directory /home/cb/wrk/zOther/data/tmp/tmpr9_xzjwk\nUsing 4 workers.\nUsing tempdir: /home/cb/wrk/zOther/data/tmp/tmpr9_xzjwk/tmpo_xved1i\nNum. workers = 4\nPreparing /home/cb/wrk/zOther/data/tmp/tmpr9_xzjwk/tmpo_xved1i/timeseries.hdf5...\nPreparing neighborhood sorters (M=31, N=269997)...\nNeighboorhood of channel 29 has 5 channels.Neighboorhood of channel 28 has 6 channels.\n\nDetecting events on channel 29 (phase1)...\nDetecting events on channel 30 (phase1)...\nNeighboorhood of channel 23 has 7 channels.\nDetecting events on channel 24 (phase1)...\nNeighboorhood of channel 7 has 7 channels.\nDetecting events on channel 8 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.022525\nNum events detected on channel 30 (phase1): 1\nComputing PCA features for channel 30 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.025604\nNum events detected on channel 29 (phase1): 6\nComputing PCA features for channel 29 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.028793\nNum events detected on channel 24 (phase1): 5\nComputing PCA features for channel 24 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.029814\nNum events detected on channel 8 (phase1): 5\nComputing PCA features for channel 8 (phase1)...\nClustering for channel 30 (phase1)...\nFound 1 clusters for channel 30 (phase1)...\nComputing templates for channel 30 (phase1)...\nRe-assigning events for channel 30 (phase1)...\nNeighboorhood of channel 17 has 7 channels.\nDetecting events on channel 18 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.096352\nNum events detected on channel 18 (phase1): 5\nComputing PCA features for channel 18 (phase1)...\nClustering for channel 24 (phase1)...\nFound 1 clusters for channel 24 (phase1)...Clustering for channel 29 (phase1)...\n\nComputing templates for channel 24 (phase1)...\nFound 1 clusters for channel 29 (phase1)...\nComputing templates for channel 29 (phase1)...\nClustering for channel 8 (phase1)...\nFound 1 clusters for channel 8 (phase1)...\nComputing templates for channel 8 (phase1)...\nRe-assigning events for channel 29 (phase1)...Re-assigning events for channel 24 (phase1)...\nNeighboorhood of channel 20 has 7 channels.\nDetecting events on channel 21 (phase1)...\n\nNeighboorhood of channel 25 has 7 channels.\nDetecting events on channel 26 (phase1)...\nClustering for channel 18 (phase1)...\nFound 1 clusters for channel 18 (phase1)...\nComputing templates for channel 18 (phase1)...\nRe-assigning events for channel 8 (phase1)...\nNeighboorhood of channel 26 has 7 channels.\nDetecting events on channel 27 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.039213\nNum events detected on channel 26 (phase1): 4\nComputing PCA features for channel 26 (phase1)...\nElapsed time for detect on neighborhood:Re-assigning events for channel 18 (phase1)...\n 0:00:00.055824\nNum events detected on channel 21 (phase1): 14\nNeighboorhood of channel 14 has 7 channels.Computing PCA features for channel 21 (phase1)...\n\nDetecting events on channel 15 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.029223\nNum events detected on channel 27 (phase1): 8\nComputing PCA features for channel 27 (phase1)...\nElapsed time for detect on neighborhood:Clustering for channel 27 (phase1)...\nFound 1 clusters for channel 27 (phase1)... 0:00:00.086340\nNum events detected on channel 15 (phase1): 2\nComputing PCA features for channel 15 (phase1)...\n\nComputing templates for channel 27 (phase1)...\nClustering for channel 26 (phase1)...\nFound 1 clusters for channel 26 (phase1)...\nComputing templates for channel 26 (phase1)...\nClustering for channel 21 (phase1)...\nFound 1 clusters for channel 21 (phase1)...\nComputing templates for channel 21 (phase1)...\nRe-assigning events for channel 26 (phase1)...Re-assigning events for channel 27 (phase1)...\nNeighboorhood of channel 4 has 7 channels.\nDetecting events on channel 5 (phase1)...\nRe-assigning events for channel 21 (phase1)...\n\nNeighboorhood of channel 15 has 7 channels.Neighboorhood of channel 16 has 7 channels.\nDetecting events on channel 16 (phase1)...\n\nDetecting events on channel 17 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.027093\nNum events detected on channel 5 (phase1): 18\nComputing PCA features for channel 5 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.035564\nNum events detected on channel 16 (phase1): 6\nComputing PCA features for channel 16 (phase1)...\nClustering for channel 5 (phase1)...\nFound 1 clusters for channel 5 (phase1)...\nComputing templates for channel 5 (phase1)...\nClustering for channel 16 (phase1)...\nFound 1 clusters for channel 16 (phase1)...\nComputing templates for channel 16 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.091005\nNum events detected on channel 17 (phase1): 17\nComputing PCA features for channel 17 (phase1)...\nClustering for channel 15 (phase1)...Re-assigning events for channel 5 (phase1)...\nFound 1 clusters for channel 15 (phase1)...\nComputing templates for channel 15 (phase1)...\n\nNeighboorhood of channel 11 has 7 channels.\nDetecting events on channel 12 (phase1)...\nRe-assigning events for channel 16 (phase1)...\nNeighboorhood of channel 12 has 7 channels.\nDetecting events on channel 13 (phase1)...\nElapsed time for detect on neighborhood:Re-assigning events for channel 15 (phase1)... 0:00:00.040278\nNum events detected on channel 12 (phase1): 7\nComputing PCA features for channel 12 (phase1)...\n\nNeighboorhood of channel 6 has 7 channels.\nDetecting events on channel 7 (phase1)...\nClustering for channel 17 (phase1)...\nFound 1 clusters for channel 17 (phase1)...\nComputing templates for channel 17 (phase1)...\nRe-assigning events for channel 17 (phase1)...\nNeighboorhood of channel 21 has 7 channels.\nDetecting events on channel 22 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.033808\nNum events detected on channel 7 (phase1): 10\nComputing PCA features for channel 7 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.076235\nNum events detected on channel 13 (phase1): 1\nComputing PCA features for channel 13 (phase1)...\nClustering for channel 12 (phase1)...\nFound 1 clusters for channel 12 (phase1)...\nComputing templates for channel 12 (phase1)...\nClustering for channel 13 (phase1)...Re-assigning events for channel 12 (phase1)...\nNeighboorhood of channel 0 has 4 channels.\nDetecting events on channel 1 (phase1)...\n\nFound 1 clusters for channel 13 (phase1)...\nComputing templates for channel 13 (phase1)...\nElapsed time for detect on neighborhood:Re-assigning events for channel 13 (phase1)...\nNeighboorhood of channel 5 has 7 channels.\nDetecting events on channel 6 (phase1)...\n 0:00:00.026181\nNum events detected on channel 1 (phase1): 3\nComputing PCA features for channel 1 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.133353\nNum events detected on channel 22 (phase1): 3\nComputing PCA features for channel 22 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.025383\nNum events detected on channel 6 (phase1): 2\nComputing PCA features for channel 6 (phase1)...\nClustering for channel 1 (phase1)...Clustering for channel 7 (phase1)...\nFound 1 clusters for channel 1 (phase1)...\nComputing templates for channel 1 (phase1)...\n\nFound 1 clusters for channel 7 (phase1)...\nComputing templates for channel 7 (phase1)...\nRe-assigning events for channel 1 (phase1)...\nNeighboorhood of channel 13 has 7 channels.Re-assigning events for channel 7 (phase1)...\nDetecting events on channel 14 (phase1)...\n\nNeighboorhood of channel 8 has 7 channels.\nDetecting events on channel 9 (phase1)...\nClustering for channel 6 (phase1)...\nFound 1 clusters for channel 6 (phase1)...\nComputing templates for channel 6 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.033709\nNum events detected on channel 9 (phase1): 6\nComputing PCA features for channel 9 (phase1)...\nElapsed time for detect on neighborhood:Re-assigning events for channel 6 (phase1)... 0:00:00.055517\nNum events detected on channel 14 (phase1): 4\nComputing PCA features for channel 14 (phase1)...\n\nNeighboorhood of channel 22 has 7 channels.\nDetecting events on channel 23 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.020576\nNum events detected on channel 23 (phase1): 17\nComputing PCA features for channel 23 (phase1)...\nClustering for channel 23 (phase1)...Clustering for channel 14 (phase1)...\nFound 1 clusters for channel 14 (phase1)...\nComputing templates for channel 14 (phase1)...\nClustering for channel 22 (phase1)...\nFound 1 clusters for channel 22 (phase1)...\nComputing templates for channel 22 (phase1)...\nClustering for channel 9 (phase1)...\nFound 1 clusters for channel 9 (phase1)...\nComputing templates for channel 9 (phase1)...\n\nFound 1 clusters for channel 23 (phase1)...\nComputing templates for channel 23 (phase1)...\nRe-assigning events for channel 14 (phase1)...\nRe-assigning events for channel 9 (phase1)...Neighboorhood of channel 18 has 7 channels.\nDetecting events on channel 19 (phase1)...\n\nNeighboorhood of channel 9 has 7 channels.\nDetecting events on channel 10 (phase1)...\nRe-assigning events for channel 23 (phase1)...\nNeighboorhood of channel 27 has 7 channels.\nDetecting events on channel 28 (phase1)...\nRe-assigning events for channel 22 (phase1)...\nNeighboorhood of channel 3 has 7 channels.\nDetecting events on channel 4 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.035256\nNum events detected on channel 19 (phase1): 11\nComputing PCA features for channel 19 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.040792\nNum events detected on channel 10 (phase1): 5\nComputing PCA features for channel 10 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.033487\nNum events detected on channel 28 (phase1): 4\nComputing PCA features for channel 28 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.029246\nNum events detected on channel 4 (phase1): 10\nComputing PCA features for channel 4 (phase1)...\nClustering for channel 28 (phase1)...\nFound 1 clusters for channel 28 (phase1)...\nComputing templates for channel 28 (phase1)...\nRe-assigning events for channel 28 (phase1)...\nClustering for channel 10 (phase1)...\nFound 1 clusters for channel 10 (phase1)...\nComputing templates for channel 10 (phase1)...\nRe-assigning events for channel 10 (phase1)...\nClustering for channel 19 (phase1)...\nFound 1 clusters for channel 19 (phase1)...\nComputing templates for channel 19 (phase1)...\nRe-assigning events for channel 19 (phase1)...\nNeighboorhood of channel 1 has 5 channels.\nDetecting events on channel 2 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.019415\nNum events detected on channel 2 (phase1): 3\nComputing PCA features for channel 2 (phase1)...\nClustering for channel 4 (phase1)...\nFound 1 clusters for channel 4 (phase1)...\nComputing templates for channel 4 (phase1)...\nClustering for channel 2 (phase1)...\nFound 1 clusters for channel 2 (phase1)...\nComputing templates for channel 2 (phase1)...\nRe-assigning events for channel 2 (phase1)...\nNeighboorhood of channel 30 has 4 channels.\nDetecting events on channel 31 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.012684\nNum events detected on channel 31 (phase1): 0\nComputing PCA features for channel 31 (phase1)...\nClustering for channel 31 (phase1)...\nFound 0 clusters for channel 31 (phase1)...\nComputing templates for channel 31 (phase1)...\nRe-assigning events for channel 31 (phase1)...\nNeighboorhood of channel 24 has 7 channels.\nDetecting events on channel 25 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.017826\nNum events detected on channel 25 (phase1): 14\nComputing PCA features for channel 25 (phase1)...\nRe-assigning events for channel 4 (phase1)...\nNeighboorhood of channel 19 has 7 channels.\nDetecting events on channel 20 (phase1)...\nClustering for channel 25 (phase1)...\nFound 1 clusters for channel 25 (phase1)...\nComputing templates for channel 25 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.022288\nNum events detected on channel 20 (phase1): 14\nComputing PCA features for channel 20 (phase1)...\nRe-assigning events for channel 25 (phase1)...\nNeighboorhood of channel 2 has 6 channels.\nDetecting events on channel 3 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.018741\nNum events detected on channel 3 (phase1): 11\nComputing PCA features for channel 3 (phase1)...\nClustering for channel 3 (phase1)...\nFound 1 clusters for channel 3 (phase1)...\nComputing templates for channel 3 (phase1)...\nRe-assigning events for channel 3 (phase1)...\nNeighboorhood of channel 10 has 7 channels.\nDetecting events on channel 11 (phase1)...\nClustering for channel 20 (phase1)...\nFound 1 clusters for channel 20 (phase1)...\nComputing templates for channel 20 (phase1)...\nRe-assigning events for channel 20 (phase1)...\nElapsed time for detect on neighborhood: 0:00:00.035092\nNum events detected on channel 11 (phase1): 6\nComputing PCA features for channel 11 (phase1)...\nClustering for channel 11 (phase1)...\nFound 1 clusters for channel 11 (phase1)...\nComputing templates for channel 11 (phase1)...\nRe-assigning events for channel 11 (phase1)...\nNeighboorhood of channel 17 has 7 channels.\nComputing PCA features for channel 18 (phase2)...\nNo duplicate events found for channel 17 in phase2\nNeighboorhood of channel 25 has 7 channels.\nComputing PCA features for channel 26 (phase2)...\nNo duplicate events found for channel 25 in phase2\nNeighboorhood of channel 4 has 7 channels.\nComputing PCA features for channel 5 (phase2)...\nNo duplicate events found for channel 4 in phase2\nNeighboorhood of channel 29 has 5 channels.\nComputing PCA features for channel 30 (phase2)...\nNo duplicate events found for channel 29 in phase2\nClustering for channel 30 (phase2)...\nFound 1 clusters for channel 30 (phase2)...\nNeighboorhood of channel 8 has 7 channels.\nComputing PCA features for channel 9 (phase2)...\nNo duplicate events found for channel 8 in phase2\nClustering for channel 18 (phase2)...\nFound 1 clusters for channel 18 (phase2)...\nNeighboorhood of channel 24 has 7 channels.\nComputing PCA features for channel 25 (phase2)...\nNo duplicate events found for channel 24 in phase2\nClustering for channel 5 (phase2)...\nFound 1 clusters for channel 5 (phase2)...\nNeighboorhood of channel 1 has 5 channels.\nComputing PCA features for channel 2 (phase2)...\nNo duplicate events found for channel 1 in phase2\nClustering for channel 9 (phase2)...\nFound 1 clusters for channel 9 (phase2)...\nNeighboorhood of channel 14 has 7 channels.\nComputing PCA features for channel 15 (phase2)...\nNo duplicate events found for channel 14 in phase2\nClustering for channel 2 (phase2)...\nFound 1 clusters for channel 2 (phase2)...\nNeighboorhood of channel 12 has 7 channels.\nComputing PCA features for channel 13 (phase2)...\nNo duplicate events found for channel 12 in phase2\nClustering for channel 26 (phase2)...\nFound 1 clusters for channel 26 (phase2)...\nNeighboorhood of channel 27 has 7 channels.\nComputing PCA features for channel 28 (phase2)...\nNo duplicate events found for channel 27 in phase2\nClustering for channel 15 (phase2)...\nFound 1 clusters for channel 15 (phase2)...\nNeighboorhood of channel 28 has 6 channels.\nComputing PCA features for channel 29 (phase2)...\nNo duplicate events found for channel 28 in phase2\nClustering for channel 13 (phase2)...\nFound 1 clusters for channel 13 (phase2)...\nClustering for channel 29 (phase2)...\nFound 1 clusters for channel 29 (phase2)...\nNeighboorhood of channel 16 has 7 channels.\nComputing PCA features for channel 17 (phase2)...\nNo duplicate events found for channel 16 in phase2\nNeighboorhood of channel 13 has 7 channels.\nComputing PCA features for channel 14 (phase2)...\nNo duplicate events found for channel 13 in phase2\nClustering for channel 17 (phase2)...\nFound 1 clusters for channel 17 (phase2)...\nNeighboorhood of channel 26 has 7 channels.\nComputing PCA features for channel 27 (phase2)...\nNo duplicate events found for channel 26 in phase2\nClustering for channel 27 (phase2)...Clustering for channel 14 (phase2)...\n\nFound 1 clusters for channel 14 (phase2)...\nFound 1 clusters for channel 27 (phase2)...\nNeighboorhood of channel 18 has 7 channels.\nComputing PCA features for channel 19 (phase2)...\nNo duplicate events found for channel 18 in phase2\nNeighboorhood of channel 11 has 7 channels.\nComputing PCA features for channel 12 (phase2)...Clustering for channel 28 (phase2)...\n\nNo duplicate events found for channel 11 in phase2\nFound 1 clusters for channel 28 (phase2)...\nNeighboorhood of channel 15 has 7 channels.\nComputing PCA features for channel 16 (phase2)...\nNo duplicate events found for channel 15 in phase2\nClustering for channel 25 (phase2)...\nFound 1 clusters for channel 25 (phase2)...\nNeighboorhood of channel 9 has 7 channels.\nComputing PCA features for channel 10 (phase2)...\nNo duplicate events found for channel 9 in phase2\nClustering for channel 12 (phase2)...\nFound 1 clusters for channel 12 (phase2)...\nNeighboorhood of channel 10 has 7 channels.\nComputing PCA features for channel 11 (phase2)...\nNo duplicate events found for channel 10 in phase2\nClustering for channel 19 (phase2)...\nFound 1 clusters for channel 19 (phase2)...\nNeighboorhood of channel 3 has 7 channels.\nComputing PCA features for channel 4 (phase2)...\nNo duplicate events found for channel 3 in phase2\nClustering for channel 11 (phase2)...\nFound 1 clusters for channel 11 (phase2)...\nNeighboorhood of channel 6 has 7 channels.\nComputing PCA features for channel 7 (phase2)...\nNo duplicate events found for channel 6 in phase2\nClustering for channel 4 (phase2)...\nFound 1 clusters for channel 4 (phase2)...\nNeighboorhood of channel 23 has 7 channels.\nComputing PCA features for channel 24 (phase2)...\nNo duplicate events found for channel 23 in phase2\nClustering for channel 7 (phase2)...\nFound 1 clusters for channel 7 (phase2)...\nNeighboorhood of channel 22 has 7 channels.\nComputing PCA features for channel 23 (phase2)...\nNo duplicate events found for channel 22 in phase2\nClustering for channel 23 (phase2)...Clustering for channel 16 (phase2)...\n\nFound 1 clusters for channel 16 (phase2)...\nFound 1 clusters for channel 23 (phase2)...\nNeighboorhood of channel 5 has 7 channels.\nComputing PCA features for channel 6 (phase2)...\nNo duplicate events found for channel 5 in phase2\nNeighboorhood of channel 20 has 7 channels.\nComputing PCA features for channel 21 (phase2)...\nNo duplicate events found for channel 20 in phase2\nClustering for channel 10 (phase2)...\nFound 1 clusters for channel 10 (phase2)...\nNeighboorhood of channel 2 has 6 channels.\nComputing PCA features for channel 3 (phase2)...\nNo duplicate events found for channel 2 in phase2\nClustering for channel 24 (phase2)...\nFound 1 clusters for channel 24 (phase2)...\nClustering for channel 21 (phase2)...\nFound 1 clusters for channel 21 (phase2)...\nNeighboorhood of channel 21 has 7 channels.\nComputing PCA features for channel 22 (phase2)...\nNo duplicate events found for channel 21 in phase2\nClustering for channel 6 (phase2)...\nFound 1 clusters for channel 6 (phase2)...\nNeighboorhood of channel 7 has 7 channels.\nClustering for channel 22 (phase2)...\nFound 1 clusters for channel 22 (phase2)...\nComputing PCA features for channel 8 (phase2)...\nNo duplicate events found for channel 7 in phase2\nClustering for channel 3 (phase2)...\nFound 1 clusters for channel 3 (phase2)...\nNeighboorhood of channel 0 has 4 channels.\nComputing PCA features for channel 1 (phase2)...\nNo duplicate events found for channel 0 in phase2\nClustering for channel 1 (phase2)...\nFound 1 clusters for channel 1 (phase2)...\nNeighboorhood of channel 30 has 4 channels.\nComputing PCA features for channel 31 (phase2)...\nNo duplicate events found for channel 30 in phase2\nClustering for channel 31 (phase2)...\nFound 0 clusters for channel 31 (phase2)...\nClustering for channel 8 (phase2)...\nFound 1 clusters for channel 8 (phase2)...\nNeighboorhood of channel 19 has 7 channels.\nComputing PCA features for channel 20 (phase2)...\nNo duplicate events found for channel 19 in phase2\nClustering for channel 20 (phase2)...\nFound 1 clusters for channel 20 (phase2)...\nPreparing output...\nDone with ms4alg.\nCleaning tempdir::::: /home/cb/wrk/zOther/data/tmp/tmpr9_xzjwk/tmpo_xved1i\nmountainsort4 run time 5.69s\nSaving sorting results...\n</pre> <pre>/home/cb/miniconda3/envs/spy/lib/python3.9/site-packages/spikeinterface/core/basesorting.py:212: UserWarning: The registered recording will not be persistent on disk, but only available in memory\n  warnings.warn(\"The registered recording will not be persistent on disk, but only available in memory\")\n/home/cb/miniconda3/envs/spy/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '/home/cb/wrk/zOther/data/tmp/tmpr9_xzjwk'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n</pre> In\u00a0[63]: Copied! <pre>sgs.SpikeSorting() &amp; ss_key\n</pre> sgs.SpikeSorting() &amp; ss_key Out[63]: <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>sorting_path</p> <p>time_of_sort</p> in Unix time, to the nearest second minirec20230622_.nwb 0 01_s1_first9 default_hippocampus My Team mountainsort4 hippocampus_tutorial minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times /home/cb/wrk/zOther/data/\"sorting\"/minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_3335c236_spikesorting 1689971050 <p>Total: 1</p>"}, {"location": "notebooks/10_Spike_Sorting/#spike-sorting", "title": "Spike Sorting\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#imports", "title": "Imports\u00b6", "text": "<p>Let's start by importing tables from Spyglass and quieting warnings caused by some dependencies.</p> <p>Note: It the imports below throw a <code>FileNotFoundError</code>, make a cell with <code>!env | grep X</code> where X is part of the problematic directory. This will show the variable causing issues. Make another cell that sets this variable elsewhere with <code>%env VAR=\"/your/path/\"</code></p>"}, {"location": "notebooks/10_Spike_Sorting/#fetch-exercise", "title": "Fetch Exercise\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#adding-an-nwb-file", "title": "Adding an NWB file\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#import-data", "title": "Import Data\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#extracting-the-recording", "title": "Extracting the recording\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#sortgroup", "title": "<code>SortGroup</code>\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#intervallist", "title": "<code>IntervalList</code>\u00b6", "text": "<p>Next, we make a decision about the time interval for our spike sorting using <code>IntervalList</code>.</p>"}, {"location": "notebooks/10_Spike_Sorting/#sortinterval", "title": "<code>SortInterval</code>\u00b6", "text": "<p>For longer recordings, Spyglass subsets this interval with <code>SortInterval</code>. Below, we select the first <code>n</code> seconds of this interval.</p>"}, {"location": "notebooks/10_Spike_Sorting/#preprocessing-parameters", "title": "Preprocessing Parameters\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#processing-a-key", "title": "Processing a key\u00b6", "text": "<p>key is often used to describe an entry we want to move through the pipeline, and keys are often managed as dictionaries. Here, we'll manage the spike sort recording key, <code>ssr_key</code>.</p>"}, {"location": "notebooks/10_Spike_Sorting/#recording-selection", "title": "Recording Selection\u00b6", "text": "<p>We now insert this key <code>SpikeSortingRecordingSelection</code> table to specify what time/tetrode/etc. of the recording we want to extract.</p>"}, {"location": "notebooks/10_Spike_Sorting/#spikesortingrecording", "title": "<code>SpikeSortingRecording</code>\u00b6", "text": "<p>And now we're ready to extract the recording! The <code>populate</code> command will automatically process data in Computed or Imported table tiers.</p> <p>If we only want to process certain entries, we can grab their primary key with the <code>.proj()</code> command and use a list of primary keys when calling <code>populate</code>.</p>"}, {"location": "notebooks/10_Spike_Sorting/#artifact-detection", "title": "Artifact Detection\u00b6", "text": "<p><code>ArtifactDetectionParameters</code> establishes the parameters for removing artifacts from the data. We may want to target artifact signal that is within the frequency band of our filter (600Hz-6KHz), and thus will not get removed by filtering.</p> <p>For this demo, we'll use a parameter set to skip this step.</p>"}, {"location": "notebooks/10_Spike_Sorting/#spike-sorting", "title": "Spike sorting\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#spikesorterparameters", "title": "<code>SpikeSorterParameters</code>\u00b6", "text": "<p>For our example, we will be using <code>mountainsort4</code>. There are already some default parameters in the <code>SpikeSorterParameters</code> table we'll <code>fetch</code>.</p>"}, {"location": "notebooks/10_Spike_Sorting/#spikesortingselection", "title": "<code>SpikeSortingSelection</code>\u00b6", "text": "<p>Gearing up to Spike Sort!</p> <p>We now collect our various keys to insert into <code>SpikeSortingSelection</code>, which is specific to this recording and eventual sorting segment.</p> <p>Note: the spike sorter parameters defined above are specific to <code>mountainsort4</code> and may not work for other sorters.</p>"}, {"location": "notebooks/10_Spike_Sorting/#spikesorting", "title": "<code>SpikeSorting</code>\u00b6", "text": "<p>After adding to <code>SpikeSortingSelection</code>, we can simply populate <code>SpikeSorting</code>.</p> <p>Note: This may take time with longer data sets. Be sure to <code>pip install mountainsort4</code> if this is your first time spike sorting.</p>"}, {"location": "notebooks/10_Spike_Sorting/#check-to-make-sure-the-table-populated", "title": "Check to make sure the table populated\u00b6", "text": ""}, {"location": "notebooks/10_Spike_Sorting/#next-steps", "title": "Next Steps\u00b6", "text": "<p>Congratulations, you've spike sorted! See our next notebook for curation steps.</p>"}, {"location": "notebooks/11_Curation/", "title": "Curation", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see this notebook</li> <li>For a more detailed introduction to DataJoint with inserts, see this notebook</li> <li>The Spike Sorting notebook is a mandatory prerequisite to Curation.</li> </ul> In\u00a0[2]: Copied! <pre>%env KACHERY_CLOUD_DIR=\"/home/cb/.kachery-cloud/\"\n</pre> %env KACHERY_CLOUD_DIR=\"/home/cb/.kachery-cloud/\" <pre>env: KACHERY_CLOUD_DIR=\"/home/cb/.kachery-cloud/\"\n</pre> In\u00a0[6]: Copied! <pre>import os\nimport warnings\nimport datajoint as dj\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nfrom spyglass.spikesorting import SpikeSorting\n</pre> import os import warnings import datajoint as dj  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning)  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  from spyglass.spikesorting import SpikeSorting <pre>[2023-07-21 13:55:48,211][INFO]: Connecting root@localhost:3306\n[2023-07-21 13:55:48,242][INFO]: Connected root@localhost:3306\n/home/cb/miniconda3/envs/spy/lib/python3.9/site-packages/spikeinterface/sortingcomponents/peak_detection.py:643: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit(parallel=False)\n/home/cb/miniconda3/envs/spy/lib/python3.9/site-packages/spikeinterface/sortingcomponents/peak_detection.py:668: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit(parallel=False)\n</pre> In\u00a0[9]: Copied! <pre># Define the name of the file that you copied and renamed from previous tutorials\nnwb_file_name = \"minirec20230622.nwb\"\nnwb_copy_file_name = \"minirec20230622_.nwb\"\nSpikeSorting &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> # Define the name of the file that you copied and renamed from previous tutorials nwb_file_name = \"minirec20230622.nwb\" nwb_copy_file_name = \"minirec20230622_.nwb\" SpikeSorting &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[9]: <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>sorting_path</p> <p>time_of_sort</p> in Unix time, to the nearest second minirec20230622_.nwb 0 01_s1_first9 default_hippocampus My Team mountainsort4 hippocampus_tutorial minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times /home/cb/wrk/zOther/data/\"sorting\"/minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_3335c236_spikesorting 1689971050 <p>Total: 1</p> <p>As of June 2021, members of the Frank Lab can use the <code>sortingview</code> web app for manual curation.</p> In\u00a0[\u00a0]: Copied! <pre># ERROR: curation_feed_uri not a field in SpikeSorting\n</pre> # ERROR: curation_feed_uri not a field in SpikeSorting In\u00a0[\u00a0]: Copied! <pre>workspace_uri = (SpikeSorting &amp; {\"nwb_file_name\": nwb_copy_file_name}).fetch1(\n    \"curation_feed_uri\"\n)\nprint(\n    f\"https://sortingview.vercel.app/workspace?workspace={workspace_uri}&amp;channel=franklab\"\n)\n</pre> workspace_uri = (SpikeSorting &amp; {\"nwb_file_name\": nwb_copy_file_name}).fetch1(     \"curation_feed_uri\" ) print(     f\"https://sortingview.vercel.app/workspace?workspace={workspace_uri}&amp;channel=franklab\" ) <p>This will take you to a workspace on the <code>sortingview</code> app. The workspace, which you can think of as a list of recording and associated sorting objects, was created at the end of spike sorting. On the workspace view, you will see a set of recordings that have been added to the workspace.</p> <p></p> <p>Clicking on a recording then takes you to a page that gives you information about the recording as well as the associated sorting objects.</p> <p></p> <p>Click on a sorting to see the curation view. Try exploring the many visualization widgets.</p> <p></p> <p>The most important is the <code>Units Table</code> and the <code>Curation</code> menu, which allows you to give labels to the units. The curation labels will persist even if you suddenly lose connection to the app; this is because the curation actions are appended to the workspace as soon as they are created. Note that if you are not logged in with your Google account, <code>Curation</code> menu may not be visible. Log in and refresh the page to access this feature.</p> <p></p>"}, {"location": "notebooks/11_Curation/#curation", "title": "Curation\u00b6", "text": ""}, {"location": "notebooks/11_Curation/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/11_Curation/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/11_Curation/#spikes-sorted", "title": "Spikes Sorted\u00b6", "text": "<p>Let's check that the sorting was successful in the previous notebook.</p>"}, {"location": "notebooks/11_Curation/#sortingview-web-app", "title": "<code>sortingview</code> web app\u00b6", "text": ""}, {"location": "notebooks/11_Curation/#up-next", "title": "Up Next\u00b6", "text": "<p>Next, we'll turn our attention to LFP data data.</p>"}, {"location": "notebooks/12_LFP/", "title": "LFP", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> In\u00a0[1]: Copied! <pre>import os\nimport copy\nimport datajoint as dj\nimport numpy as np\nimport pandas as pd\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.lfp as lfp\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import copy import datajoint as dj import numpy as np import pandas as pd  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.lfp as lfp  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2023-07-28 14:45:50,776][INFO]: Connecting root@localhost:3306\n[2023-07-28 14:45:50,804][INFO]: Connected root@localhost:3306\n</pre> <p>First, we select the NWB file, which corresponds to the dataset we want to extract LFP from.</p> In\u00a0[2]: Copied! <pre>nwb_file_name = \"minirec20230622_.nwb\"\n</pre> nwb_file_name = \"minirec20230622_.nwb\" <p>Next, we create the standard LFP Filters. This only needs to be done once.</p> In\u00a0[3]: Copied! <pre>sgc.FirFilterParameters().create_standard_filters()\nsgc.FirFilterParameters()\n</pre> sgc.FirFilterParameters().create_standard_filters() sgc.FirFilterParameters() Out[3]: <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>filter_type</p> <p>filter_low_stop</p> lowest frequency for stop band for low frequency side of filter <p>filter_low_pass</p> lowest frequency for pass band of low frequency side of filter <p>filter_high_pass</p> highest frequency for pass band for high frequency side of filter <p>filter_high_stop</p> highest frequency for stop band of high frequency side of filter <p>filter_comments</p> comments about the filter <p>filter_band_edges</p> numpy array containing the filter bands (redundant with individual parameters) <p>filter_coeff</p> numpy array containing the filter coefficients LFP 0-400 Hz 20000 lowpass 0.0 0.0 400.0 425.0 standard LFP filter for 20 KHz data =BLOB= =BLOB=LFP 0-400 Hz 30000 lowpass 0.0 0.0 400.0 425.0 standard LFP filter for 30 KHz data =BLOB= =BLOB= <p>Total: 2</p> <p>Now, we create an LFP electrode group, or the set of electrodes we want to filter for LFP data. We can grab all electrodes and brain regions as a data frame.</p> In\u00a0[4]: Copied! <pre>electrodes_df = (\n    pd.DataFrame(\n        (sgc.Electrode &amp; {\"nwb_file_name\": nwb_file_name, \"probe_electrode\": 0})\n        * sgc.BrainRegion\n    )\n    .loc[:, [\"nwb_file_name\", \"electrode_id\", \"region_name\"]]\n    .sort_values(by=\"electrode_id\")\n)\nelectrodes_df\n</pre> electrodes_df = (     pd.DataFrame(         (sgc.Electrode &amp; {\"nwb_file_name\": nwb_file_name, \"probe_electrode\": 0})         * sgc.BrainRegion     )     .loc[:, [\"nwb_file_name\", \"electrode_id\", \"region_name\"]]     .sort_values(by=\"electrode_id\") ) electrodes_df Out[4]: nwb_file_name electrode_id region_name 0 minirec20230622_.nwb 0 corpus callosum and associated subcortical whi... <p>For a larger dataset, we might want to filter by region, but our example data only has one electrode.</p> <pre>lfp_electrode_ids = electrodes_df.loc[\n    electrodes_df.region_name == \"ca1\"\n].electrode_id\n</pre> In\u00a0[5]: Copied! <pre>lfp_electrode_ids = [0]\nlfp_electrode_group_name = \"test\"\nlfp_eg_key = {\n    \"nwb_file_name\": nwb_file_name,\n    \"lfp_electrode_group_name\": lfp_electrode_group_name,\n}\n\nlfp.lfp_electrode.LFPElectrodeGroup.create_lfp_electrode_group(\n    nwb_file_name=nwb_file_name,\n    group_name=lfp_electrode_group_name,\n    electrode_list=lfp_electrode_ids,\n)\n</pre> lfp_electrode_ids = [0] lfp_electrode_group_name = \"test\" lfp_eg_key = {     \"nwb_file_name\": nwb_file_name,     \"lfp_electrode_group_name\": lfp_electrode_group_name, }  lfp.lfp_electrode.LFPElectrodeGroup.create_lfp_electrode_group(     nwb_file_name=nwb_file_name,     group_name=lfp_electrode_group_name,     electrode_list=lfp_electrode_ids, ) <p>We can verify the electrode list as follows</p> In\u00a0[6]: Copied! <pre>lfp.lfp_electrode.LFPElectrodeGroup.LFPElectrode() &amp; {\n    \"nwb_file_name\": nwb_file_name\n}\n</pre> lfp.lfp_electrode.LFPElectrodeGroup.LFPElectrode() &amp; {     \"nwb_file_name\": nwb_file_name } Out[6]: <p>nwb_file_name</p> name of the NWB file <p>lfp_electrode_group_name</p> the name of this group of electrodes <p>electrode_group_name</p> electrode group name from NWBFile <p>electrode_id</p> the unique number for this electrode minirec20230622_.nwb test 0 0 <p>Total: 1</p> <p>Recall from the previous notebook that <code>IntervalList</code> selects time frames from the experiment. We can select the interval and subset to the first <code>n</code> seconds...</p> In\u00a0[7]: Copied! <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_file_name}\n</pre> sgc.IntervalList &amp; {\"nwb_file_name\": nwb_file_name} Out[7]: Time intervals used for analysis <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>valid_times</p> numpy array with start and end times for each interval minirec20230622_.nwb 01_s1 =BLOB=minirec20230622_.nwb 01_s1_first9 =BLOB=minirec20230622_.nwb 02_s2 =BLOB=minirec20230622_.nwb minirec20230622_.nwb_01_s1_first9_0_default_hippocampus =BLOB=minirec20230622_.nwb minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times =BLOB=minirec20230622_.nwb pos 0 valid times =BLOB=minirec20230622_.nwb pos 1 valid times =BLOB=minirec20230622_.nwb pos 2 valid times =BLOB=minirec20230622_.nwb pos 3 valid times =BLOB=minirec20230622_.nwb raw data valid times =BLOB= <p>Total: 10</p> In\u00a0[8]: Copied! <pre>n = 9\norig_interval_list_name = \"01_s1\"\ninterval_list_name = orig_interval_list_name + f\"_first{n}\"\n\nvalid_times = (\n    sgc.IntervalList\n    &amp; {\n        \"nwb_file_name\": nwb_file_name,\n        \"interval_list_name\": orig_interval_list_name,\n    }\n).fetch1(\"valid_times\")\n\ninterval_key = {\n    \"nwb_file_name\": nwb_file_name,\n    \"interval_list_name\": interval_list_name,\n    \"valid_times\": np.asarray([[valid_times[0, 0], valid_times[0, 0] + n]]),\n}\n\nsgc.IntervalList.insert1(\n    interval_key,\n    skip_duplicates=True,\n)\n</pre> n = 9 orig_interval_list_name = \"01_s1\" interval_list_name = orig_interval_list_name + f\"_first{n}\"  valid_times = (     sgc.IntervalList     &amp; {         \"nwb_file_name\": nwb_file_name,         \"interval_list_name\": orig_interval_list_name,     } ).fetch1(\"valid_times\")  interval_key = {     \"nwb_file_name\": nwb_file_name,     \"interval_list_name\": interval_list_name,     \"valid_times\": np.asarray([[valid_times[0, 0], valid_times[0, 0] + n]]), }  sgc.IntervalList.insert1(     interval_key,     skip_duplicates=True, ) In\u00a0[9]: Copied! <pre>sgc.IntervalList() &amp; {\n    \"nwb_file_name\": nwb_file_name,\n    \"interval_list_name\": interval_list_name,\n}\n</pre> sgc.IntervalList() &amp; {     \"nwb_file_name\": nwb_file_name,     \"interval_list_name\": interval_list_name, } Out[9]: Time intervals used for analysis <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>valid_times</p> numpy array with start and end times for each interval minirec20230622_.nwb 01_s1_first9 =BLOB= <p>Total: 1</p> <p>LFPSelection combines the data, interval and filter</p> In\u00a0[10]: Copied! <pre>lfp_s_key = copy.deepcopy(lfp_eg_key)\n\nlfp_s_key.update(\n    {\n        \"target_interval_list_name\": interval_list_name,\n        \"filter_name\": \"LFP 0-400 Hz\",\n        \"filter_sampling_rate\": 30_000,\n    }\n)\n\nlfp.v1.LFPSelection.insert1(lfp_s_key, skip_duplicates=True)\n</pre> lfp_s_key = copy.deepcopy(lfp_eg_key)  lfp_s_key.update(     {         \"target_interval_list_name\": interval_list_name,         \"filter_name\": \"LFP 0-400 Hz\",         \"filter_sampling_rate\": 30_000,     } )  lfp.v1.LFPSelection.insert1(lfp_s_key, skip_duplicates=True) In\u00a0[11]: Copied! <pre>lfp.v1.LFPSelection() &amp; lfp_s_key\n</pre> lfp.v1.LFPSelection() &amp; lfp_s_key Out[11]: <p>nwb_file_name</p> name of the NWB file <p>lfp_electrode_group_name</p> the name of this group of electrodes <p>target_interval_list_name</p> descriptive name of this interval list <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter minirec20230622_.nwb test 01_s1_first9 LFP 0-400 Hz 30000 <p>Total: 1</p> <p><code>LFPV1</code> has a similar <code>populate</code> command as we've seen before.</p> <p>Notes:</p> <ul> <li>For full recordings, this takes ~2h when done locally, for all electrodes</li> <li>This <code>populate</code> also inserts into the LFP Merge Table, <code>LFPOutput</code>. For more on Merge Tables, see our documentation. In short, this collects different LFP processing streams into one table.</li> </ul> In\u00a0[12]: Copied! <pre>lfp.v1.LFPV1().populate(lfp_s_key)\n</pre> lfp.v1.LFPV1().populate(lfp_s_key) <pre>LFP: found 1 intervals &gt; 1.0 sec long.\nWriting new NWB file minirec20230622_BBU52RZ6OY.nwb\nFiltering data\nInterval 0: loading data into memory\n</pre> <p>We can now look at the LFP table to see the data we've extracted</p> In\u00a0[13]: Copied! <pre>lfp.LFPOutput.LFPV1() &amp; lfp_s_key\n</pre> lfp.LFPOutput.LFPV1() &amp; lfp_s_key Out[13]: <p>merge_id</p> <p>nwb_file_name</p> name of the NWB file <p>lfp_electrode_group_name</p> the name of this group of electrodes <p>target_interval_list_name</p> descriptive name of this interval list <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter 1e7fbe35-034b-ed8e-7965-a0467ae5c0a4 minirec20230622_.nwb test 01_s1_first9 LFP 0-400 Hz 30000 <p>Total: 1</p> In\u00a0[14]: Copied! <pre>lfp_key = {\"merge_id\": (lfp.LFPOutput.LFPV1() &amp; lfp_s_key).fetch1(\"merge_id\")}\nlfp_key\n</pre> lfp_key = {\"merge_id\": (lfp.LFPOutput.LFPV1() &amp; lfp_s_key).fetch1(\"merge_id\")} lfp_key Out[14]: <pre>{'merge_id': UUID('1e7fbe35-034b-ed8e-7965-a0467ae5c0a4')}</pre> In\u00a0[15]: Copied! <pre>lfp.LFPOutput &amp; lfp_key\n</pre> lfp.LFPOutput &amp; lfp_key Out[15]: <p>merge_id</p> <p>source</p> 1e7fbe35-034b-ed8e-7965-a0467ae5c0a4 LFPV1 <p>Total: 1</p> <p>From the Merge Table, we can get the keys for the LFP data we want to see</p> In\u00a0[16]: Copied! <pre>lfp_df = (lfp.LFPOutput &amp; lfp_key).fetch1_dataframe()\nlfp_df\n</pre> lfp_df = (lfp.LFPOutput &amp; lfp_key).fetch1_dataframe() lfp_df Out[16]: 0 time 1.687475e+09 -1448 1.687475e+09 -199 1.687475e+09 870 1.687475e+09 557 1.687475e+09 -196 ... ... 1.687475e+09 -4328 1.687475e+09 -6125 1.687475e+09 3180 1.687475e+09 3238 1.687475e+09 -1803 <p>9000 rows \u00d7 1 columns</p> In\u00a0[17]: Copied! <pre>lfp_sampling_rate = lfp.LFPOutput.merge_get_parent(lfp_key).fetch1(\n    \"lfp_sampling_rate\"\n)\n\nfilter_name = \"Theta 5-11 Hz\"\n\nsgc.common_filter.FirFilterParameters().add_filter(\n    filter_name,\n    lfp_sampling_rate,\n    \"bandpass\",\n    [4, 5, 11, 12],\n    \"theta filter for 1 Khz data\",\n)\n\nsgc.common_filter.FirFilterParameters() &amp; {\n    \"filter_name\": filter_name,\n    \"filter_sampling_rate\": lfp_sampling_rate,\n}\n</pre> lfp_sampling_rate = lfp.LFPOutput.merge_get_parent(lfp_key).fetch1(     \"lfp_sampling_rate\" )  filter_name = \"Theta 5-11 Hz\"  sgc.common_filter.FirFilterParameters().add_filter(     filter_name,     lfp_sampling_rate,     \"bandpass\",     [4, 5, 11, 12],     \"theta filter for 1 Khz data\", )  sgc.common_filter.FirFilterParameters() &amp; {     \"filter_name\": filter_name,     \"filter_sampling_rate\": lfp_sampling_rate, } Out[17]: <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>filter_type</p> <p>filter_low_stop</p> lowest frequency for stop band for low frequency side of filter <p>filter_low_pass</p> lowest frequency for pass band of low frequency side of filter <p>filter_high_pass</p> highest frequency for pass band for high frequency side of filter <p>filter_high_stop</p> highest frequency for stop band of high frequency side of filter <p>filter_comments</p> comments about the filter <p>filter_band_edges</p> numpy array containing the filter bands (redundant with individual parameters) <p>filter_coeff</p> numpy array containing the filter coefficients Theta 5-11 Hz 1000 bandpass 4.0 5.0 11.0 12.0 theta filter for 1 Khz data =BLOB= =BLOB= <p>Total: 1</p> In\u00a0[18]: Copied! <pre>sgc.IntervalList()\n</pre> sgc.IntervalList() Out[18]: Time intervals used for analysis <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>valid_times</p> numpy array with start and end times for each interval minirec20230622_.nwb 01_s1 =BLOB=minirec20230622_.nwb 01_s1_first9 =BLOB=minirec20230622_.nwb 02_s2 =BLOB=minirec20230622_.nwb lfp_test_01_s1_first9_valid times =BLOB=minirec20230622_.nwb minirec20230622_.nwb_01_s1_first9_0_default_hippocampus =BLOB=minirec20230622_.nwb minirec20230622_.nwb_01_s1_first9_0_default_hippocampus_none_artifact_removed_valid_times =BLOB=minirec20230622_.nwb pos 0 valid times =BLOB=minirec20230622_.nwb pos 1 valid times =BLOB=minirec20230622_.nwb pos 2 valid times =BLOB=minirec20230622_.nwb pos 3 valid times =BLOB=minirec20230622_.nwb raw data valid times =BLOB= <p>Total: 11</p> <p>We can specify electrodes of interest, and desired sampling rate.</p> In\u00a0[23]: Copied! <pre>from spyglass.lfp.analysis.v1 import lfp_band\n\nlfp_band_electrode_ids = [0]  # assumes we've filtered these electrodes\nlfp_band_sampling_rate = 100  # desired sampling rate\n\nlfp_band.LFPBandSelection().set_lfp_band_electrodes(\n    nwb_file_name=nwb_file_name,\n    lfp_merge_id=lfp_key[\"merge_id\"],\n    electrode_list=lfp_band_electrode_ids,\n    filter_name=filter_name,  # defined above\n    interval_list_name=interval_list_name,  # Defined in IntervalList above\n    reference_electrode_list=[-1],  # -1 means no ref electrode for all channels\n    lfp_band_sampling_rate=lfp_band_sampling_rate,\n)\n\nlfp_band.LFPBandSelection()\n</pre> from spyglass.lfp.analysis.v1 import lfp_band  lfp_band_electrode_ids = [0]  # assumes we've filtered these electrodes lfp_band_sampling_rate = 100  # desired sampling rate  lfp_band.LFPBandSelection().set_lfp_band_electrodes(     nwb_file_name=nwb_file_name,     lfp_merge_id=lfp_key[\"merge_id\"],     electrode_list=lfp_band_electrode_ids,     filter_name=filter_name,  # defined above     interval_list_name=interval_list_name,  # Defined in IntervalList above     reference_electrode_list=[-1],  # -1 means no ref electrode for all channels     lfp_band_sampling_rate=lfp_band_sampling_rate, )  lfp_band.LFPBandSelection() Out[23]: <p>lfp_merge_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>min_interval_len</p> the minimum length of a valid interval to filter 1e7fbe35-034b-ed8e-7965-a0467ae5c0a4 Theta 5-11 Hz 1000 minirec20230622_.nwb 01_s1_first9 100 1.0 <p>Total: 1</p> <p>Next we add an entry for the LFP Band and the electrodes we want to filter</p> In\u00a0[25]: Copied! <pre>lfp_band_key = (\n    lfp_band.LFPBandSelection\n    &amp; {\n        \"merge_id\": lfp_key[\"merge_id\"],\n        \"filter_name\": filter_name,\n        \"lfp_band_sampling_rate\": lfp_band_sampling_rate,\n    }\n).fetch1(\"KEY\")\nlfp_band_key\n</pre> lfp_band_key = (     lfp_band.LFPBandSelection     &amp; {         \"merge_id\": lfp_key[\"merge_id\"],         \"filter_name\": filter_name,         \"lfp_band_sampling_rate\": lfp_band_sampling_rate,     } ).fetch1(\"KEY\") lfp_band_key Out[25]: <pre>{'lfp_merge_id': UUID('1e7fbe35-034b-ed8e-7965-a0467ae5c0a4'),\n 'filter_name': 'Theta 5-11 Hz',\n 'filter_sampling_rate': 1000,\n 'nwb_file_name': 'minirec20230622_.nwb',\n 'target_interval_list_name': '01_s1_first9',\n 'lfp_band_sampling_rate': 100}</pre> <p>Check to make sure it worked</p> In\u00a0[26]: Copied! <pre>lfp_band.LFPBandSelection() &amp; lfp_band_key\n</pre> lfp_band.LFPBandSelection() &amp; lfp_band_key Out[26]: <p>lfp_merge_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>min_interval_len</p> the minimum length of a valid interval to filter 1e7fbe35-034b-ed8e-7965-a0467ae5c0a4 Theta 5-11 Hz 1000 minirec20230622_.nwb 01_s1_first9 100 1.0 <p>Total: 1</p> In\u00a0[28]: Copied! <pre>lfp_band.LFPBandV1().populate(lfp_band.LFPBandSelection() &amp; lfp_band_key)\nlfp_band.LFPBandV1()\n</pre> lfp_band.LFPBandV1().populate(lfp_band.LFPBandSelection() &amp; lfp_band_key) lfp_band.LFPBandV1() <pre>Writing new NWB file minirec20230622_H89T89A0CK.nwb\nInterval stop time 1687474809.1214128 is larger than last timestamp 1687474809.1204128, using last timestamp instead\n</pre> Out[28]: <p>lfp_merge_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>analysis_file_name</p> name of the file <p>interval_list_name</p> descriptive name of this interval list <p>lfp_band_object_id</p> the NWB object ID for loading this object from the file 1e7fbe35-034b-ed8e-7965-a0467ae5c0a4 Theta 5-11 Hz 1000 minirec20230622_.nwb 01_s1_first9 100 minirec20230622_H89T89A0CK.nwb 01_s1_first9 lfp band 100Hz 71bbb1a8-409e-48eb-a336-8c70fd5d6b74 <p>Total: 1</p> <p>Now we can plot the original signal, the LFP filtered trace, and the theta filtered trace together. Get the three electrical series objects and the indices of the electrodes we band pass filtered</p> <p>Note: Much of the code below could be replaced by a function calls that would return the data from each electrical series</p> In\u00a0[29]: Copied! <pre>orig_eseries = (sgc.Raw() &amp; {\"nwb_file_name\": nwb_file_name}).fetch_nwb()[0][\n    \"raw\"\n]\norig_elect_indices = sgc.get_electrode_indices(\n    orig_eseries, lfp_band_electrode_ids\n)\norig_timestamps = np.asarray(orig_eseries.timestamps)\n</pre> orig_eseries = (sgc.Raw() &amp; {\"nwb_file_name\": nwb_file_name}).fetch_nwb()[0][     \"raw\" ] orig_elect_indices = sgc.get_electrode_indices(     orig_eseries, lfp_band_electrode_ids ) orig_timestamps = np.asarray(orig_eseries.timestamps) In\u00a0[30]: Copied! <pre>lfp_eseries = lfp.LFPOutput.fetch_nwb(lfp_key)[0][\"lfp\"]\nlfp_elect_indices = sgc.get_electrode_indices(\n    lfp_eseries, lfp_band_electrode_ids\n)\nlfp_timestamps = np.asarray(lfp_eseries.timestamps)\n</pre> lfp_eseries = lfp.LFPOutput.fetch_nwb(lfp_key)[0][\"lfp\"] lfp_elect_indices = sgc.get_electrode_indices(     lfp_eseries, lfp_band_electrode_ids ) lfp_timestamps = np.asarray(lfp_eseries.timestamps) In\u00a0[33]: Copied! <pre>lfp_band_eseries = (lfp_band.LFPBandV1 &amp; lfp_band_key).fetch_nwb()[0][\n    \"lfp_band\"\n]\nlfp_band_elect_indices = sgc.get_electrode_indices(\n    lfp_band_eseries, lfp_band_electrode_ids\n)\nlfp_band_timestamps = np.asarray(lfp_band_eseries.timestamps)\n</pre> lfp_band_eseries = (lfp_band.LFPBandV1 &amp; lfp_band_key).fetch_nwb()[0][     \"lfp_band\" ] lfp_band_elect_indices = sgc.get_electrode_indices(     lfp_band_eseries, lfp_band_electrode_ids ) lfp_band_timestamps = np.asarray(lfp_band_eseries.timestamps) <p>Get a list of times for the first run epoch and then select a 2 second interval 100 seconds from the beginning</p> In\u00a0[34]: Copied! <pre>plottimes = [valid_times[0][0] + 1, valid_times[0][0] + 8]\n</pre> plottimes = [valid_times[0][0] + 1, valid_times[0][0] + 8] In\u00a0[35]: Copied! <pre># get the time indices for each dataset\norig_time_ind = np.where(\n    np.logical_and(\n        orig_timestamps &gt; plottimes[0], orig_timestamps &lt; plottimes[1]\n    )\n)[0]\n\nlfp_time_ind = np.where(\n    np.logical_and(lfp_timestamps &gt; plottimes[0], lfp_timestamps &lt; plottimes[1])\n)[0]\nlfp_band_time_ind = np.where(\n    np.logical_and(\n        lfp_band_timestamps &gt; plottimes[0],\n        lfp_band_timestamps &lt; plottimes[1],\n    )\n)[0]\n</pre> # get the time indices for each dataset orig_time_ind = np.where(     np.logical_and(         orig_timestamps &gt; plottimes[0], orig_timestamps &lt; plottimes[1]     ) )[0]  lfp_time_ind = np.where(     np.logical_and(lfp_timestamps &gt; plottimes[0], lfp_timestamps &lt; plottimes[1]) )[0] lfp_band_time_ind = np.where(     np.logical_and(         lfp_band_timestamps &gt; plottimes[0],         lfp_band_timestamps &lt; plottimes[1],     ) )[0] In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(\n    orig_eseries.timestamps[orig_time_ind],\n    orig_eseries.data[orig_time_ind, orig_elect_indices[0]],\n    \"k-\",\n)\nplt.plot(\n    lfp_eseries.timestamps[lfp_time_ind],\n    lfp_eseries.data[lfp_time_ind, lfp_elect_indices[0]],\n    \"b-\",\n)\nplt.plot(\n    lfp_band_eseries.timestamps[lfp_band_time_ind],\n    lfp_band_eseries.data[lfp_band_time_ind, lfp_band_elect_indices[0]],\n    \"r-\",\n)\nplt.xlabel(\"Time (sec)\")\nplt.ylabel(\"Amplitude (AD units)\")\n\n# Uncomment to see plot\n# plt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(     orig_eseries.timestamps[orig_time_ind],     orig_eseries.data[orig_time_ind, orig_elect_indices[0]],     \"k-\", ) plt.plot(     lfp_eseries.timestamps[lfp_time_ind],     lfp_eseries.data[lfp_time_ind, lfp_elect_indices[0]],     \"b-\", ) plt.plot(     lfp_band_eseries.timestamps[lfp_band_time_ind],     lfp_band_eseries.data[lfp_band_time_ind, lfp_band_elect_indices[0]],     \"r-\", ) plt.xlabel(\"Time (sec)\") plt.ylabel(\"Amplitude (AD units)\")  # Uncomment to see plot # plt.show()"}, {"location": "notebooks/12_LFP/#lfp-extraction", "title": "LFP Extraction\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#select-data", "title": "Select data\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#create-filters", "title": "Create Filters\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#electrode-group", "title": "Electrode Group\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#intervallist", "title": "<code>IntervalList</code>\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#lfpselection", "title": "<code>LFPSelection</code>\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#populate-lfp", "title": "Populate LFP\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#lfp-band", "title": "LFP Band\u00b6", "text": "<p>Now that we've created the LFP object we can perform a second level of filtering for a band of interest, in this case the theta band. We first need to create the filter.</p>"}, {"location": "notebooks/12_LFP/#plotting", "title": "Plotting\u00b6", "text": ""}, {"location": "notebooks/12_LFP/#next-steps", "title": "Next Steps\u00b6", "text": "<p>Next, we'll use look at Theta bands within LFP data.</p>"}, {"location": "notebooks/14_Theta/", "title": "Theta", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> <li>To run this notebook, you should have already completed the LFP notebook and populated the <code>LFPBand</code> table.</li> </ul> <p>In this tutorial, we demonstrate how to generate analytic signals from the LFP data, as well as how to compute theta phases and power.</p> In\u00a0[4]: Copied! <pre>import os\nimport datajoint as dj\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.lfp.analysis.v1.lfp_band as lfp_band\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import datajoint as dj import numpy as np import matplotlib.pyplot as plt  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.lfp.analysis.v1.lfp_band as lfp_band  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) In\u00a0[5]: Copied! <pre>lfp_band.LFPBandV1()\n</pre> lfp_band.LFPBandV1() Out[5]: <p>lfp_merge_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>analysis_file_name</p> name of the file <p>interval_list_name</p> descriptive name of this interval list <p>lfp_band_object_id</p> the NWB object ID for loading this object from the file 1e7fbe35-034b-ed8e-7965-a0467ae5c0a4 Theta 5-11 Hz 1000 minirec20230622_.nwb 01_s1_first9 100 minirec20230622_H89T89A0CK.nwb 01_s1_first9 lfp band 100Hz 71bbb1a8-409e-48eb-a336-8c70fd5d6b74 <p>Total: 1</p> <p>Now, we create a keys to reference the theta band data.</p> In\u00a0[6]: Copied! <pre>nwb_file_name = \"minirec20230622_.nwb\"\nlfp_key = dict(\n    nwb_file_name=nwb_file_name,\n    filter_name=\"Theta 5-11 Hz\",\n)\nlfp_band.LFPBandV1() &amp; lfp_key\n</pre> nwb_file_name = \"minirec20230622_.nwb\" lfp_key = dict(     nwb_file_name=nwb_file_name,     filter_name=\"Theta 5-11 Hz\", ) lfp_band.LFPBandV1() &amp; lfp_key Out[6]: <p>lfp_merge_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>analysis_file_name</p> name of the file <p>interval_list_name</p> descriptive name of this interval list <p>lfp_band_object_id</p> the NWB object ID for loading this object from the file 1e7fbe35-034b-ed8e-7965-a0467ae5c0a4 Theta 5-11 Hz 1000 minirec20230622_.nwb 01_s1_first9 100 minirec20230622_H89T89A0CK.nwb 01_s1_first9 lfp band 100Hz 71bbb1a8-409e-48eb-a336-8c70fd5d6b74 <p>Total: 1</p> <p>We do not need all electrodes for theta phase/power, so we define a list for analyses. When working with full data, this list might limit to hippocampal reference electrodes.</p> <p>Make sure that the chosen electrodes already exist in the LFPBand data; if not, go to the LFP tutorial to generate them.</p> In\u00a0[7]: Copied! <pre>electrode_list = [0]\n\nall_electrodes = (  # All available electrode ids\n    (lfp_band.LFPBandV1() &amp; lfp_key).fetch_nwb()[0][\"lfp_band\"]\n).electrodes.data[:]\n\nnp.isin(electrode_list, all_electrodes)  # Check if our list is in 'all'\n</pre> electrode_list = [0]  all_electrodes = (  # All available electrode ids     (lfp_band.LFPBandV1() &amp; lfp_key).fetch_nwb()[0][\"lfp_band\"] ).electrodes.data[:]  np.isin(electrode_list, all_electrodes)  # Check if our list is in 'all' Out[7]: <pre>array([ True])</pre> <p>Next, we'll compute the theta analytic signal.</p> In\u00a0[9]: Copied! <pre>theta_analytic_signal = (\n    lfp_band.LFPBandV1() &amp; lfp_key\n).compute_analytic_signal(electrode_list=electrode_list)\n\ntheta_analytic_signal\n</pre> theta_analytic_signal = (     lfp_band.LFPBandV1() &amp; lfp_key ).compute_analytic_signal(electrode_list=electrode_list)  theta_analytic_signal Out[9]: electrode 0 time 1.687475e+09 -124.000000-133.973800j 1.687475e+09 -30.000000-308.834746j 1.687475e+09 56.000000-267.135234j 1.687475e+09 139.000000-293.141114j 1.687475e+09 227.000000-236.059129j ... ... 1.687475e+09 -548.000000+29.032951j 1.687475e+09 -464.000000-225.303955j 1.687475e+09 -291.000000-329.028626j 1.687475e+09 -86.000000-404.908506j 1.687475e+09 92.000000-178.143737j <p>900 rows \u00d7 1 columns</p> <p>In the dataframe above, the index is the timestamps, and the columns are the analytic signals of theta band (complex numbers) for each electrode.</p> <p>Using a similar method, we can compute theta phase and power from the LFPBand table.</p> In\u00a0[10]: Copied! <pre>theta_phase = (lfp_band.LFPBandV1() &amp; lfp_key).compute_signal_phase(\n    electrode_list=electrode_list\n)\ntheta_power = (lfp_band.LFPBandV1() &amp; lfp_key).compute_signal_power(\n    electrode_list=electrode_list\n)\n</pre> theta_phase = (lfp_band.LFPBandV1() &amp; lfp_key).compute_signal_phase(     electrode_list=electrode_list ) theta_power = (lfp_band.LFPBandV1() &amp; lfp_key).compute_signal_power(     electrode_list=electrode_list ) <p>We can get the theta band data to plot with the theta phase results.</p> In\u00a0[11]: Copied! <pre>theta_band = (lfp_band.LFPBandV1() &amp; lfp_key).fetch_nwb()[0][\"lfp_band\"]\nelectrode_index = np.isin(theta_band.electrodes.data[:], electrode_list)\ntheta_band_selected = theta_band.data[:, electrode_index]\n</pre> theta_band = (lfp_band.LFPBandV1() &amp; lfp_key).fetch_nwb()[0][\"lfp_band\"] electrode_index = np.isin(theta_band.electrodes.data[:], electrode_list) theta_band_selected = theta_band.data[:, electrode_index] In\u00a0[13]: Copied! <pre>electrode_id = electrode_list[0]  # the electrode for which we want to plot\nplot_start, plot_end = 0, 5000  # start/end time of plotting\n\nfig, ax1 = plt.subplots(figsize=(20, 6))\nax1.set_xlabel(\"Time (sec)\", fontsize=15)\nax1.set_ylabel(\"Amplitude (AD units)\", fontsize=15)\nax1.plot(\n    theta_phase.index[plot_start:plot_end],\n    theta_band_selected[\n        plot_start:plot_end, np.where(np.array(electrode_list) == 0)[0][0]\n    ],\n    \"k-\",\n    linewidth=1,\n    alpha=0.9,\n)\nax1.tick_params(axis=\"y\", labelcolor=\"k\")\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\nax2.set_ylabel(\"Phase(deg)\", color=\"b\", fontsize=15)\nax2.plot(\n    theta_phase.index[plot_start:plot_end],\n    theta_phase[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],\n    \"b\",\n)\nax2.tick_params(axis=\"y\", labelcolor=\"b\")\nax2.axhline(y=0, color=\"r\", linestyle=\"-\")\n\nfig.tight_layout()\nax1.set_title(\n    f\"Theta band amplitude and phase, electrode {electrode_id}\",\n    fontsize=20,\n)\n</pre> electrode_id = electrode_list[0]  # the electrode for which we want to plot plot_start, plot_end = 0, 5000  # start/end time of plotting  fig, ax1 = plt.subplots(figsize=(20, 6)) ax1.set_xlabel(\"Time (sec)\", fontsize=15) ax1.set_ylabel(\"Amplitude (AD units)\", fontsize=15) ax1.plot(     theta_phase.index[plot_start:plot_end],     theta_band_selected[         plot_start:plot_end, np.where(np.array(electrode_list) == 0)[0][0]     ],     \"k-\",     linewidth=1,     alpha=0.9, ) ax1.tick_params(axis=\"y\", labelcolor=\"k\")  ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis ax2.set_ylabel(\"Phase(deg)\", color=\"b\", fontsize=15) ax2.plot(     theta_phase.index[plot_start:plot_end],     theta_phase[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],     \"b\", ) ax2.tick_params(axis=\"y\", labelcolor=\"b\") ax2.axhline(y=0, color=\"r\", linestyle=\"-\")  fig.tight_layout() ax1.set_title(     f\"Theta band amplitude and phase, electrode {electrode_id}\",     fontsize=20, ) <p>We can also plot the theta power.</p> In\u00a0[14]: Copied! <pre>electrode_id = electrode_list[0]  # the electrode for which we want to plot\nplot_start, plot_end = 0, 5000  # start/end time of plotting\n\nfig, ax = plt.subplots(figsize=(20, 6))\nax.set_xlabel(\"Time (sec)\", fontsize=15)\nax.set_ylabel(\"Theta power\", fontsize=15)\nax.plot(\n    theta_power.index[plot_start:plot_end],\n    theta_power[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],\n    \"k-\",\n    linewidth=1,\n)\nax.tick_params(axis=\"y\", labelcolor=\"k\")\nax.set_title(\n    f\"Theta band power, electrode {electrode_id}\",\n    fontsize=20,\n)\n</pre> electrode_id = electrode_list[0]  # the electrode for which we want to plot plot_start, plot_end = 0, 5000  # start/end time of plotting  fig, ax = plt.subplots(figsize=(20, 6)) ax.set_xlabel(\"Time (sec)\", fontsize=15) ax.set_ylabel(\"Theta power\", fontsize=15) ax.plot(     theta_power.index[plot_start:plot_end],     theta_power[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],     \"k-\",     linewidth=1, ) ax.tick_params(axis=\"y\", labelcolor=\"k\") ax.set_title(     f\"Theta band power, electrode {electrode_id}\",     fontsize=20, )"}, {"location": "notebooks/14_Theta/#theta-phase-and-power", "title": "Theta phase and power\u00b6", "text": ""}, {"location": "notebooks/14_Theta/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/14_Theta/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/14_Theta/#acquire-signal", "title": "Acquire Signal\u00b6", "text": "<p>First, we'll acquire the theta band analytic signal from the <code>LFPBand</code> data for electrodes of interest. We can grab keys from this table.</p>"}, {"location": "notebooks/14_Theta/#compute-phase-and-power", "title": "Compute phase and power\u00b6", "text": ""}, {"location": "notebooks/14_Theta/#plot-results", "title": "Plot results\u00b6", "text": "<p>We can overlay theta and detected phase for each electrode.</p> <p>Note: The red horizontal line indicates phase 0, corresponding to the through of theta.</p>"}, {"location": "notebooks/14_Theta/#up-next", "title": "Up Next\u00b6", "text": "<p>Next, we'll turn our attention to position data.</p>"}, {"location": "notebooks/20_Position_Trodes/", "title": "Position Trodes", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> <p>In this tutorial, we'll process position data extracted with Trodes Tracking by</p> <ul> <li>Defining parameters</li> <li>Processing raw position</li> <li>Extracting centroid and orientation</li> <li>Insert the results into the <code>TrodesPosV1</code> table</li> <li>Plotting the head position/direction results for quality assurance</li> </ul> <p>The pipeline takes the 2D video pixel data of green/red LEDs, and computes:</p> <ul> <li>head position (in cm)</li> <li>head orientation (in radians)</li> <li>head velocity (in cm/s)</li> <li>head speed (in cm/s)</li> </ul> In\u00a0[1]: Copied! <pre>import os\nimport datajoint as dj\nimport matplotlib.pyplot as plt\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.position as sgp\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import datajoint as dj import matplotlib.pyplot as plt  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.position as sgp  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2023-10-04 13:02:09,333][INFO]: Connecting root@localhost:3306\n[2023-10-04 13:02:09,344][INFO]: Connected root@localhost:3306\n</pre> <p>First, we'll grab let us make sure that the session we want to analyze is inserted into the <code>RawPosition</code> table</p> In\u00a0[2]: Copied! <pre>from spyglass.utils.nwb_helper_fn import get_nwb_copy_filename\n\n# Define the name of the file that you copied and renamed\nnwb_file_name = \"minirec20230622.nwb\"\nnwb_copy_file_name = get_nwb_copy_filename(nwb_file_name)\n</pre> from spyglass.utils.nwb_helper_fn import get_nwb_copy_filename  # Define the name of the file that you copied and renamed nwb_file_name = \"minirec20230622.nwb\" nwb_copy_file_name = get_nwb_copy_filename(nwb_file_name) In\u00a0[3]: Copied! <pre>sgc.common_behav.RawPosition() &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.common_behav.RawPosition() &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[3]: <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list minirec20230622_.nwb pos 0 valid timesminirec20230622_.nwb pos 1 valid times <p>Total: 2</p> <p>Parameters are set by the <code>TrodesPosParams</code> table, with a <code>default</code> set available. To adjust the default, insert a new set into this table. The parameters are...</p> <ul> <li><code>max_separation</code>, default 9 cm: maximum acceptable distance between red and green LEDs.<ul> <li>If exceeded, the times are marked as NaNs and inferred by interpolation.</li> <li>Useful when the inferred LED position tracks a reflection instead of the true position.</li> </ul> </li> <li><code>max_speed</code>, default 300.0 cm/s: maximum speed the animal can move.<ul> <li>If exceeded, times are marked as NaNs and inferred by interpolation.</li> <li>Useful to prevent big jumps in position.</li> </ul> </li> <li><code>position_smoothing_duration</code>, default 0.100 s: LED position smoothing before computing average position to get head position.</li> <li><code>speed_smoothing_std_dev</code>, default 0.100 s: standard deviation of the Gaussian kernel used to smooth the head speed.</li> <li><code>front_led1</code>, default 1 (True), use <code>xloc</code>/<code>yloc</code>: Which LED is the front LED for calculating the head direction.<ul> <li>1: LED corresponding to <code>xloc</code>, <code>yloc</code> in the <code>RawPosition</code> table is the front, <code>xloc2</code>, <code>yloc2</code> as the back.</li> <li>0: LED corresponding to <code>xloc2</code>, <code>yloc2</code> in the <code>RawPosition</code> table is the front, <code>xloc</code>, <code>yloc</code> as the back.</li> </ul> </li> </ul> <p>We can see these defaults with <code>TrodesPosParams().default_params</code>.</p> In\u00a0[4]: Copied! <pre>from pprint import pprint\n\nparameters = sgp.v1.TrodesPosParams().default_params\npprint(parameters)\n</pre> from pprint import pprint  parameters = sgp.v1.TrodesPosParams().default_params pprint(parameters) <pre>{'is_upsampled': 0,\n 'led1_is_front': 1,\n 'max_LED_separation': 9.0,\n 'max_plausible_speed': 300.0,\n 'orient_smoothing_std_dev': 0.001,\n 'position_smoothing_duration': 0.125,\n 'speed_smoothing_std_dev': 0.1,\n 'upsampling_interpolation_method': 'linear',\n 'upsampling_sampling_rate': None}\n</pre> <p>For the <code>minirec</code> demo file, only one LED is moving. The following paramset will allow us to process this data.</p> In\u00a0[5]: Copied! <pre>trodes_params_name = \"single_led\"\ntrodes_params = {\n    \"max_separation\": 10000.0,\n    \"max_speed\": 300.0,\n    \"position_smoothing_duration\": 0.125,\n    \"speed_smoothing_std_dev\": 0.1,\n    \"orient_smoothing_std_dev\": 0.001,\n    \"led1_is_front\": 1,\n    \"is_upsampled\": 0,\n    \"upsampling_sampling_rate\": None,\n    \"upsampling_interpolation_method\": \"linear\",\n}\nsgp.v1.TrodesPosParams.insert1(\n    {\n        \"trodes_pos_params_name\": trodes_params_name,\n        \"params\": trodes_params,\n    },\n    skip_duplicates=True,\n)\nsgp.v1.TrodesPosParams()\n</pre> trodes_params_name = \"single_led\" trodes_params = {     \"max_separation\": 10000.0,     \"max_speed\": 300.0,     \"position_smoothing_duration\": 0.125,     \"speed_smoothing_std_dev\": 0.1,     \"orient_smoothing_std_dev\": 0.001,     \"led1_is_front\": 1,     \"is_upsampled\": 0,     \"upsampling_sampling_rate\": None,     \"upsampling_interpolation_method\": \"linear\", } sgp.v1.TrodesPosParams.insert1(     {         \"trodes_pos_params_name\": trodes_params_name,         \"params\": trodes_params,     },     skip_duplicates=True, ) sgp.v1.TrodesPosParams() Out[5]: <p>trodes_pos_params_name</p> name for this set of parameters <p>params</p> default =BLOB=single_led =BLOB= <p>Total: 2</p> <p>Later, we'll pair the above parameters with an interval from our NWB file and insert into <code>TrodesPosSelection</code>.</p> <p>First, let's select an interval from the <code>IntervalList</code> table.</p> In\u00a0[6]: Copied! <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[6]: Time intervals used for analysis <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>valid_times</p> numpy array with start and end times for each interval minirec20230622_.nwb 01_s1 =BLOB=minirec20230622_.nwb 02_s2 =BLOB=minirec20230622_.nwb pos 0 valid times =BLOB=minirec20230622_.nwb pos 1 valid times =BLOB=minirec20230622_.nwb raw data valid times =BLOB= <p>Total: 5</p> <p>The raw position in pixels is in the <code>RawPosition</code> table is extracted from the video data by the algorithm in Trodes. We have timepoints available for the duration when position tracking was turned on and off, which may be a subset of the video itself.</p> <p><code>fetch1_dataframe</code> returns the position of the LEDs as a pandas dataframe where time is the index.</p> In\u00a0[7]: Copied! <pre>interval_list_name = \"pos 0 valid times\"  # pos # is epoch # minus 1\nraw_position_df = (\n    sgc.RawPosition()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": interval_list_name,\n    }\n).fetch1_dataframe()\nraw_position_df\n</pre> interval_list_name = \"pos 0 valid times\"  # pos # is epoch # minus 1 raw_position_df = (     sgc.RawPosition()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": interval_list_name,     } ).fetch1_dataframe() raw_position_df Out[7]: xloc1 yloc1 xloc2 yloc2 time 1.687475e+09 445 567 0 0 1.687475e+09 445 567 0 0 1.687475e+09 445 567 0 0 1.687475e+09 444 568 0 0 1.687475e+09 444 568 0 0 ... ... ... ... ... 1.687475e+09 479 536 0 0 1.687475e+09 480 534 0 0 1.687475e+09 480 533 0 0 1.687475e+09 481 530 0 0 1.687475e+09 481 530 0 0 <p>267 rows \u00d7 4 columns</p> <p>Let's just quickly plot the two LEDs to get a sense of the inputs to the pipeline:</p> In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(raw_position_df.xloc1, raw_position_df.yloc1, color=\"green\")\n# Uncomment for multiple LEDs\n# ax.plot(raw_position_df.xloc2, raw_position_df.yloc2, color=\"red\")\nax.set_xlabel(\"x-position [pixels]\", fontsize=18)\nax.set_ylabel(\"y-position [pixels]\", fontsize=18)\nax.set_title(\"Raw Position\", fontsize=28)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(raw_position_df.xloc1, raw_position_df.yloc1, color=\"green\") # Uncomment for multiple LEDs # ax.plot(raw_position_df.xloc2, raw_position_df.yloc2, color=\"red\") ax.set_xlabel(\"x-position [pixels]\", fontsize=18) ax.set_ylabel(\"y-position [pixels]\", fontsize=18) ax.set_title(\"Raw Position\", fontsize=28) Out[8]: <pre>Text(0.5, 1.0, 'Raw Position')</pre> <p>To associate a set of parameters with a given interval, insert them into the <code>TrodesPosSelection</code> table.</p> In\u00a0[9]: Copied! <pre>trodes_s_key = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": interval_list_name,\n    \"trodes_pos_params_name\": trodes_params_name,\n}\nsgp.v1.TrodesPosSelection.insert1(\n    trodes_s_key,\n    skip_duplicates=True,\n)\n</pre> trodes_s_key = {     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": interval_list_name,     \"trodes_pos_params_name\": trodes_params_name, } sgp.v1.TrodesPosSelection.insert1(     trodes_s_key,     skip_duplicates=True, ) <p>Now let's check to see if we've inserted the parameters correctly:</p> In\u00a0[10]: Copied! <pre>trodes_key = (sgp.v1.TrodesPosSelection() &amp; trodes_s_key).fetch1(\"KEY\")\n</pre> trodes_key = (sgp.v1.TrodesPosSelection() &amp; trodes_s_key).fetch1(\"KEY\") <p>We can run the pipeline for our chosen interval/parameters by using the <code>TrodesPosV1.populate</code>.</p> In\u00a0[11]: Copied! <pre>sgp.v1.TrodesPosV1.populate(trodes_key)\n</pre> sgp.v1.TrodesPosV1.populate(trodes_key) <p>Each NWB file, interval, and parameter set is now associated with a new analysis file and object ID.</p> In\u00a0[12]: Copied! <pre>sgp.v1.TrodesPosV1 &amp; trodes_key\n</pre> sgp.v1.TrodesPosV1 &amp; trodes_key Out[12]: <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>trodes_pos_params_name</p> name for this set of parameters <p>analysis_file_name</p> name of the file <p>position_object_id</p> <p>orientation_object_id</p> <p>velocity_object_id</p> minirec20230622_.nwb pos 0 valid times single_led minirec20230622_X7FUNFE30W.nwb 8663056f-aad6-492c-b26b-01da857aecdb c36c76d3-a078-4cea-9228-d260b3b82912 be5a7902-6a9e-476b-becd-7d636a9d4cb8 <p>Total: 1</p> <p>To retrieve the results as a pandas DataFrame with time as the index, we use <code>TrodesPosV1.fetch1_dataframe</code>.</p> <p>This dataframe has the following columns:</p> <ul> <li><code>position_{x,y}</code>: X or Y position of the head in cm.</li> <li><code>orientation</code>: Direction of the head relative to the bottom left corner in radians</li> <li><code>velocity_{x,y}</code>: Directional change in head position over time in cm/s</li> <li><code>speed</code>: the magnitude of the change in head position over time in cm/s</li> </ul> In\u00a0[13]: Copied! <pre>position_info = (\n    sgp.v1.TrodesPosV1()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n    }\n).fetch1_dataframe()\nposition_info\n</pre> position_info = (     sgp.v1.TrodesPosV1()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,     } ).fetch1_dataframe() position_info Out[13]: video_frame_ind position_x position_y orientation velocity_x velocity_y speed time 1.687475e+09 0.0 22.250000 28.350000 0.905373 -0.148627 0.116866 0.189071 1.687475e+09 1.0 22.250000 28.350000 0.905373 -0.219948 0.162667 0.273565 1.687475e+09 2.0 22.250000 28.350000 0.905373 -0.298575 0.205042 0.362201 1.687475e+09 3.0 22.233333 28.366667 0.906022 -0.371837 0.233439 0.439041 1.687475e+09 4.0 22.216667 28.383333 0.906671 -0.424639 0.238911 0.487234 ... ... ... ... ... ... ... ... 1.687475e+09 262.0 23.766667 27.066667 0.850225 2.738364 -3.998619 4.846400 1.687475e+09 263.0 23.933333 26.800000 0.841843 2.347307 -3.708857 4.389245 1.687475e+09 264.0 23.983333 26.716667 0.839258 1.907429 -3.237540 3.757652 1.687475e+09 265.0 24.016667 26.616667 0.836703 1.457185 -2.647347 3.021893 1.687475e+09 266.0 24.033333 26.550000 0.835110 1.040384 -2.021305 2.273340 <p>267 rows \u00d7 7 columns</p> <p><code>.index</code> on the pandas dataframe gives us timestamps.</p> In\u00a0[17]: Copied! <pre>position_info.index\n</pre> position_info.index Out[17]: <pre>Index([1687474800.1833298,  1687474800.216676,  1687474800.250001,\n        1687474800.283326,  1687474800.316672, 1687474801.8658931,\n       1687474801.8992178, 1687474801.9325643,  1687474801.965889,\n        1687474801.999214,\n       ...\n        1687474810.265809,  1687474810.299134, 1687474810.3324802,\n       1687474810.3658051,   1687474810.39913,  1687474810.432476,\n        1687474810.465801,  1687474810.499126, 1687474810.5324721,\n        1687474810.565797],\n      dtype='float64', name='time', length=267)</pre> <p>We should always spot check our results to verify that the pipeline worked correctly.</p> <p>Let's plot some of the variables first:</p> In\u00a0[20]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(8, 8))\nax.plot(position_info.position_x, position_info.position_y)\nax.set_xlabel(\"x-position [cm]\", fontsize=16)\nax.set_ylabel(\"y-position [cm]\", fontsize=16)\nax.set_title(\"Position\", fontsize=20)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(8, 8)) ax.plot(position_info.position_x, position_info.position_y) ax.set_xlabel(\"x-position [cm]\", fontsize=16) ax.set_ylabel(\"y-position [cm]\", fontsize=16) ax.set_title(\"Position\", fontsize=20) Out[20]: <pre>Text(0.5, 1.0, 'Position')</pre> In\u00a0[22]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(8, 8))\nax.plot(position_info.velocity_x, position_info.velocity_y)\nax.set_xlabel(\"x-velocity [cm/s]\", fontsize=16)\nax.set_ylabel(\"y-velocity [cm/s]\", fontsize=16)\nax.set_title(\"Velocity\", fontsize=20)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(8, 8)) ax.plot(position_info.velocity_x, position_info.velocity_y) ax.set_xlabel(\"x-velocity [cm/s]\", fontsize=16) ax.set_ylabel(\"y-velocity [cm/s]\", fontsize=16) ax.set_title(\"Velocity\", fontsize=20) Out[22]: <pre>Text(0.5, 1.0, 'Velocity')</pre> In\u00a0[24]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(16, 3))\nax.plot(position_info.index, position_info.speed)\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Speed [cm/s]\", fontsize=16)\nax.set_title(\"Head Speed\", fontsize=20)\nax.set_xlim((position_info.index.min(), position_info.index.max()))\n</pre> fig, ax = plt.subplots(1, 1, figsize=(16, 3)) ax.plot(position_info.index, position_info.speed) ax.set_xlabel(\"Time\", fontsize=16) ax.set_ylabel(\"Speed [cm/s]\", fontsize=16) ax.set_title(\"Head Speed\", fontsize=20) ax.set_xlim((position_info.index.min(), position_info.index.max())) Out[24]: <pre>(1687474800.1833298, 1687474810.565797)</pre> In\u00a0[17]: Copied! <pre>sgp.v1.TrodesPosVideo().populate(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": interval_list_name,\n        \"position_info_param_name\": trodes_params_name,\n    }\n)\n</pre> sgp.v1.TrodesPosVideo().populate(     {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": interval_list_name,         \"position_info_param_name\": trodes_params_name,     } ) <pre>Loading position data...\nLoading video data...\nFound 0 videos for {'nwb_file_name': 'minirec20230622_.nwb', 'epoch': 1}\n</pre> In\u00a0[18]: Copied! <pre>sgp.v1.TrodesPosVideo()\n</pre> sgp.v1.TrodesPosVideo() Out[18]: <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>trodes_pos_params_name</p> name for this set of parameters <p>has_video</p> minirec20230622_.nwb pos 0 valid times single_led 0 <p>Total: 1</p> In\u00a0[28]: Copied! <pre>trodes_params_up_name = trodes_params_name + \"_upsampled\"\ntrodes_params_up = {\n    **trodes_params,\n    \"is_upsampled\": 1,\n    \"upsampling_sampling_rate\": 500,\n}\nsgp.v1.TrodesPosParams.insert1(\n    {\n        \"trodes_pos_params_name\": trodes_params_up_name,\n        \"params\": trodes_params_up,\n    },\n    skip_duplicates=True,\n)\n\nsgp.v1.TrodesPosParams()\n</pre> trodes_params_up_name = trodes_params_name + \"_upsampled\" trodes_params_up = {     **trodes_params,     \"is_upsampled\": 1,     \"upsampling_sampling_rate\": 500, } sgp.v1.TrodesPosParams.insert1(     {         \"trodes_pos_params_name\": trodes_params_up_name,         \"params\": trodes_params_up,     },     skip_duplicates=True, )  sgp.v1.TrodesPosParams() Out[28]: <p>trodes_pos_params_name</p> name for this set of parameters <p>params</p> default =BLOB=default_led0 =BLOB=single_led =BLOB=single_led_upsampled =BLOB= <p>Total: 4</p> In\u00a0[29]: Copied! <pre>trodes_s_up_key = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": interval_list_name,\n    \"trodes_pos_params_name\": trodes_params_up_name,\n}\nsgp.v1.TrodesPosSelection.insert1(\n    trodes_s_up_key,\n    skip_duplicates=True,\n)\nsgp.v1.TrodesPosV1.populate(trodes_s_up_key)\n</pre> trodes_s_up_key = {     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": interval_list_name,     \"trodes_pos_params_name\": trodes_params_up_name, } sgp.v1.TrodesPosSelection.insert1(     trodes_s_up_key,     skip_duplicates=True, ) sgp.v1.TrodesPosV1.populate(trodes_s_up_key) <pre>Computing position for: {'nwb_file_name': 'minirec20230622_.nwb', 'interval_list_name': 'pos 0 valid times', 'trodes_pos_params_name': 'single_led_upsampled'}\nWriting new NWB file minirec20230622_P97SEW5WZC.nwb\nNo video frame index found. Assuming all camera frames are present.\n</pre> In\u00a0[30]: Copied! <pre>upsampled_position_info = (\n    sgp.v1.TrodesPosV1()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"position_info_param_name\": trodes_params_up_name,\n    }\n).fetch1_dataframe()\n\nupsampled_position_info\n</pre> upsampled_position_info = (     sgp.v1.TrodesPosV1()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"position_info_param_name\": trodes_params_up_name,     } ).fetch1_dataframe()  upsampled_position_info Out[30]: video_frame_ind position_x position_y orientation velocity_x velocity_y speed time 1.687475e+09 0.0 22.250000 28.350000 0.905373 -0.148627 0.116866 0.189071 1.687475e+09 1.0 22.250000 28.350000 0.905373 -0.219948 0.162667 0.273565 1.687475e+09 2.0 22.250000 28.350000 0.905373 -0.298575 0.205042 0.362201 1.687475e+09 3.0 22.233333 28.366667 0.906022 -0.371837 0.233439 0.439041 1.687475e+09 4.0 22.216667 28.383333 0.906671 -0.424639 0.238911 0.487234 ... ... ... ... ... ... ... ... 1.687475e+09 262.0 23.766667 27.066667 0.850225 2.738364 -3.998619 4.846400 1.687475e+09 263.0 23.933333 26.800000 0.841843 2.347307 -3.708857 4.389245 1.687475e+09 264.0 23.983333 26.716667 0.839258 1.907429 -3.237540 3.757652 1.687475e+09 265.0 24.016667 26.616667 0.836703 1.457185 -2.647347 3.021893 1.687475e+09 266.0 24.033333 26.550000 0.835110 1.040384 -2.021305 2.273340 <p>267 rows \u00d7 7 columns</p> In\u00a0[31]: Copied! <pre>fig, axes = plt.subplots(\n    1, 2, figsize=(16, 8), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.position_x, position_info.position_y)\naxes[0].set_xlabel(\"x-position [cm]\", fontsize=16)\naxes[0].set_ylabel(\"y-position [cm]\", fontsize=16)\naxes[0].set_title(\"Position\", fontsize=20)\n\naxes[1].plot(\n    upsampled_position_info.position_x,\n    upsampled_position_info.position_y,\n)\naxes[1].set_xlabel(\"x-position [cm]\", fontsize=16)\naxes[1].set_ylabel(\"y-position [cm]\", fontsize=16)\naxes[1].set_title(\"Upsampled Position\", fontsize=20)\n</pre> fig, axes = plt.subplots(     1, 2, figsize=(16, 8), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.position_x, position_info.position_y) axes[0].set_xlabel(\"x-position [cm]\", fontsize=16) axes[0].set_ylabel(\"y-position [cm]\", fontsize=16) axes[0].set_title(\"Position\", fontsize=20)  axes[1].plot(     upsampled_position_info.position_x,     upsampled_position_info.position_y, ) axes[1].set_xlabel(\"x-position [cm]\", fontsize=16) axes[1].set_ylabel(\"y-position [cm]\", fontsize=16) axes[1].set_title(\"Upsampled Position\", fontsize=20) Out[31]: <pre>Text(0.5, 1.0, 'Upsampled Position')</pre> In\u00a0[32]: Copied! <pre>fig, axes = plt.subplots(\n    2, 1, figsize=(16, 6), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.index, position_info.speed)\naxes[0].set_xlabel(\"Time\", fontsize=16)\naxes[0].set_ylabel(\"Speed [cm/s]\", fontsize=16)\naxes[0].set_title(\"Speed\", fontsize=20)\naxes[0].set_xlim((position_info.index.min(), position_info.index.max()))\n\naxes[1].plot(upsampled_position_info.index, upsampled_position_info.speed)\naxes[1].set_xlabel(\"Time\", fontsize=16)\naxes[1].set_ylabel(\"Speed [cm/s]\", fontsize=16)\naxes[1].set_title(\"Upsampled Speed\", fontsize=20)\n</pre> fig, axes = plt.subplots(     2, 1, figsize=(16, 6), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.index, position_info.speed) axes[0].set_xlabel(\"Time\", fontsize=16) axes[0].set_ylabel(\"Speed [cm/s]\", fontsize=16) axes[0].set_title(\"Speed\", fontsize=20) axes[0].set_xlim((position_info.index.min(), position_info.index.max()))  axes[1].plot(upsampled_position_info.index, upsampled_position_info.speed) axes[1].set_xlabel(\"Time\", fontsize=16) axes[1].set_ylabel(\"Speed [cm/s]\", fontsize=16) axes[1].set_title(\"Upsampled Speed\", fontsize=20) Out[32]: <pre>Text(0.5, 1.0, 'Upsampled Speed')</pre> In\u00a0[33]: Copied! <pre>fig, axes = plt.subplots(\n    1, 2, figsize=(16, 8), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.velocity_x, position_info.velocity_y)\naxes[0].set_xlabel(\"x-velocity [cm/s]\", fontsize=16)\naxes[0].set_ylabel(\"y-velocity [cm/s]\", fontsize=16)\naxes[0].set_title(\"Velocity\", fontsize=20)\n\naxes[1].plot(\n    upsampled_position_info.velocity_x,\n    upsampled_position_info.velocity_y,\n)\naxes[1].set_xlabel(\"x-velocity [cm/s]\", fontsize=16)\naxes[1].set_ylabel(\"y-velocity [cm/s]\", fontsize=16)\naxes[1].set_title(\"Upsampled Velocity\", fontsize=20)\n</pre> fig, axes = plt.subplots(     1, 2, figsize=(16, 8), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.velocity_x, position_info.velocity_y) axes[0].set_xlabel(\"x-velocity [cm/s]\", fontsize=16) axes[0].set_ylabel(\"y-velocity [cm/s]\", fontsize=16) axes[0].set_title(\"Velocity\", fontsize=20)  axes[1].plot(     upsampled_position_info.velocity_x,     upsampled_position_info.velocity_y, ) axes[1].set_xlabel(\"x-velocity [cm/s]\", fontsize=16) axes[1].set_ylabel(\"y-velocity [cm/s]\", fontsize=16) axes[1].set_title(\"Upsampled Velocity\", fontsize=20) Out[33]: <pre>Text(0.5, 1.0, 'Upsampled Velocity')</pre> <p>In the next notebook, we'll explore using DeepLabCut to generate position data from video.</p>"}, {"location": "notebooks/20_Position_Trodes/#trodes-position", "title": "Trodes Position\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#loading-the-data", "title": "Loading the data\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#setting-parameters", "title": "Setting parameters\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#select-interval", "title": "Select interval\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#pairing-interval-and-parameters", "title": "Pairing interval and parameters\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#running-the-pipeline", "title": "Running the pipeline\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#examine-results", "title": "Examine results\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#plots", "title": "Plots\u00b6", "text": ""}, {"location": "notebooks/20_Position_Trodes/#video", "title": "Video\u00b6", "text": "<p>To keep <code>minirec</code> small, the download link does not include videos by default.</p> <p>(Download links coming soon)</p> <p>Full datasets can be further visualized by plotting the results on the video, which will appear in the current working directory.</p>"}, {"location": "notebooks/20_Position_Trodes/#upsampling-position", "title": "Upsampling position\u00b6", "text": "<p>To get position data in smaller in time bins, we can upsample using the following parameters</p> <ul> <li><code>is_upsampled</code>, default 0 (False): If 1, perform upsampling.</li> <li><code>upsampling_sampling_rate</code>, default None: the rate to upsample to (e.g., 33 Hz video might be upsampled to 500 Hz).</li> <li><code>upsampling_interpolation_method</code>, default linear: interpolation method. See pandas.DataFrame.interpolate for alternate methods.</li> </ul>"}, {"location": "notebooks/20_Position_Trodes/#up-next", "title": "Up Next\u00b6", "text": ""}, {"location": "notebooks/21_Position_DLC_1/", "title": "DLC From Scratch", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> <p>This tutorial will extract position via DeepLabCut (DLC). It will walk through...</p> <ul> <li>creating a DLC project</li> <li>extracting and labeling frames</li> <li>training your model</li> </ul> <p>If you have a pre-trained project, you can either skip to the next tutorial to load it into the database, or skip to the following tutorial to start pose estimation with a model that is already inserted.</p> <p>Here is a schematic showing the tables used in this pipeline.</p> <p></p> <ul> <li>Imports</li> <li><code>DLCProject</code></li> <li><code>DLCModelTraining</code></li> <li><code>DLCModel</code></li> </ul> <p>You can click on any header to return to the Table of Contents</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport datajoint as dj\nfrom pprint import pprint\n\nimport spyglass.common as sgc\nimport spyglass.position.v1 as sgp\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import datajoint as dj from pprint import pprint  import spyglass.common as sgc import spyglass.position.v1 as sgp  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2023-07-28 14:45:50,776][INFO]: Connecting root@localhost:3306\n[2023-07-28 14:45:50,804][INFO]: Connected root@localhost:3306\n</pre> Notes:<ul> <li>         The cells within this <code>DLCProject</code> step need to be performed          in a local Jupyter notebook to allow for use of the frame labeling GUI     </li> <li>         Please do not add to the <code>BodyPart</code> table in the production          database unless necessary.     </li> </ul> <p>We'll begin by looking at the <code>BodyPart</code> table, which stores standard names of body parts used in DLC models throughout the lab with a concise description.</p> In\u00a0[2]: Copied! <pre>sgp.BodyPart()\n</pre> sgp.BodyPart() Out[2]: <p>bodypart</p> <p>bodypart_description</p> greenLED green LED on implant LED ringredLED redLEDredLED_C center red LED on implant LED ringredLED_L left red LED on implant LED ringredLED_R right red LED on implant LED ringtailBase base of the tail on subjectwhiteLED white LED on headstage <p>Total: 7</p> <p>If the bodyparts you plan to use in your model are not yet in the table, here is code to add bodyparts:</p> <pre>sgp.BodyPart.insert(\n    [\n        {\"bodypart\": \"bp_1\", \"bodypart_description\": \"concise descrip\"},\n        {\"bodypart\": \"bp_2\", \"bodypart_description\": \"concise descrip\"},\n    ],\n    skip_duplicates=True,\n)\n</pre> <p>To train a model, we'll need to extract frames, which we can label as training data. We can construct a list of videos from which we'll extract frames.</p> <p>The list can either contain dictionaries identifying behavioral videos for NWB files that have already been added to Spyglass, or absolute file paths to the videos you want to use.</p> <p>For this tutorial, we'll use two videos for which we already have frames labeled.</p> In\u00a0[3]: Copied! <pre>video_list = [\n    {\"nwb_file_name\": \"J1620210529_.nwb\", \"epoch\": 2},\n    {\"nwb_file_name\": \"peanut20201103_.nwb\", \"epoch\": 4},\n]\n</pre> video_list = [     {\"nwb_file_name\": \"J1620210529_.nwb\", \"epoch\": 2},     {\"nwb_file_name\": \"peanut20201103_.nwb\", \"epoch\": 4}, ] <p>Before creating our project, we need to define a few variables.</p> <ul> <li>A team name, as shown in <code>LabTeam</code> for setting permissions. Here, we'll use \"LorenLab\".</li> <li>A <code>project_name</code>, as a unique identifier for this DLC project. Here, we'll use \"tutorial_scratch_yourinitials\"</li> <li><code>bodyparts</code> is a list of body parts for which we want to extract position. The pre-labeled frames we're using include the bodyparts listed below.</li> <li>Number of frames to extract/label as <code>frames_per_video</code>. A true project might use 200, but we'll use 100 for efficiency.</li> </ul> In\u00a0[4]: Copied! <pre>team_name = \"LorenLab\"\nproject_name = \"tutorial_scratch_DG\"\nframes_per_video = 100\nbodyparts = [\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"]\nproject_key = sgp.DLCProject.insert_new_project(\n    project_name=project_name,\n    bodyparts=bodyparts,\n    lab_team=team_name,\n    frames_per_video=frames_per_video,\n    video_list=video_list,\n    skip_duplicates=True,\n)\n</pre> team_name = \"LorenLab\" project_name = \"tutorial_scratch_DG\" frames_per_video = 100 bodyparts = [\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"] project_key = sgp.DLCProject.insert_new_project(     project_name=project_name,     bodyparts=bodyparts,     lab_team=team_name,     frames_per_video=frames_per_video,     video_list=video_list,     skip_duplicates=True, ) <pre>project name: tutorial_scratch_DG is already in use.\n</pre> <p>After initializing our project, we would typically extract and label frames. This has already been done for this tutorial data, using the following commands to pull up the DLC GUI:</p> <pre>sgp.DLCProject().run_extract_frames(project_key)\nsgp.DLCProject().run_label_frames(project_key)\n</pre> <p>In order to use pre-labeled frames, you'll need to change the values in the labeled-data files. You can do that using the <code>import_labeled_frames</code> method, which expects:</p> <ul> <li><code>project_key</code> from your new project.</li> <li>The absolute path to the project directory from which we'll import labeled frames.</li> <li>The filenames, without extension, of the videos from which we want frames.</li> </ul> In\u00a0[5]: Copied! <pre>sgp.DLCProject.import_labeled_frames(\n    project_key.copy(),\n    import_project_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/\",\n    video_filenames=[\"20201103_peanut_04_r2\", \"20210529_J16_02_r1\"],\n    skip_duplicates=True,\n)\n</pre> sgp.DLCProject.import_labeled_frames(     project_key.copy(),     import_project_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/\",     video_filenames=[\"20201103_peanut_04_r2\", \"20210529_J16_02_r1\"],     skip_duplicates=True, ) <pre>/home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_project.py:451: FutureWarning: inplace is deprecated and will be removed in a future version.\n  dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n/home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_project.py:451: FutureWarning: inplace is deprecated and will be removed in a future version.\n  dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n2023-04-20 14:51:08.706450: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-20 14:51:08.897194: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n</pre> <pre>Loading DLC 2.2.3...\nOpenCV is built with OpenMP support. This usually results in poor performance. For details, see https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py\n</pre>      This step and beyond should be run on a GPU-enabled machine.  In\u00a0[7]: Copied! <pre>sgp.dlc_utils.get_gpu_memory()\n</pre> sgp.dlc_utils.get_gpu_memory() Out[7]: <pre>{0: 80383, 1: 35, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 7: 35, 8: 35, 9: 35}</pre> <p>Set GPU core:</p> In\u00a0[8]: Copied! <pre>gputouse = 1  ## 1-9\n</pre> gputouse = 1  ## 1-9 <p>Now we'll define the rest of our parameters and insert the entry.</p> <p>To see all possible parameters, try:</p> <pre>sgp.DLCModelTrainingParams.get_accepted_params()\n</pre> In\u00a0[9]: Copied! <pre>training_params_name = \"tutorial\"\nsgp.DLCModelTrainingParams.insert_new_params(\n    paramset_name=training_params_name,\n    params={\n        \"trainingsetindex\": 0,\n        \"shuffle\": 1,\n        \"gputouse\": gputouse,\n        \"net_type\": \"resnet_50\",\n        \"augmenter_type\": \"imgaug\",\n    },\n    skip_duplicates=True,\n)\n</pre> training_params_name = \"tutorial\" sgp.DLCModelTrainingParams.insert_new_params(     paramset_name=training_params_name,     params={         \"trainingsetindex\": 0,         \"shuffle\": 1,         \"gputouse\": gputouse,         \"net_type\": \"resnet_50\",         \"augmenter_type\": \"imgaug\",     },     skip_duplicates=True, ) <pre>New param set not added\nA param set with name: tutorial already exists\n</pre> <p>Next we'll modify the <code>project_key</code> to include the entries for <code>DLCModelTraining</code></p> In\u00a0[19]: Copied! <pre># project_key['project_path'] = os.path.dirname(project_key['config_path'])\nif \"config_path\" in project_key:\n    del project_key[\"config_path\"]\n</pre> # project_key['project_path'] = os.path.dirname(project_key['config_path']) if \"config_path\" in project_key:     del project_key[\"config_path\"] <p>We can insert an entry into <code>DLCModelTrainingSelection</code> and populate <code>DLCModelTraining</code>.</p> <p>Note: You can stop training at any point using <code>I + I</code> or interrupt the Kernel</p> In\u00a0[18]: Copied! <pre>sgp.DLCModelTrainingSelection.heading\n</pre> sgp.DLCModelTrainingSelection.heading Out[18]: <pre># Specification for a DLC model training instance\nproject_name         : varchar(100)                 # name of DLC project\ndlc_training_params_name : varchar(50)                  # descriptive name of parameter set\ntraining_id          : int                          # unique integer,\n---\nmodel_prefix=\"\"      : varchar(32)                  # </pre> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelTrainingSelection().insert1(\n    {\n        **project_key,\n        \"dlc_training_params_name\": training_params_name,\n        \"training_id\": 0,\n        \"model_prefix\": \"\",\n    }\n)\nmodel_training_key = (\n    sgp.DLCModelTrainingSelection\n    &amp; {\n        **project_key,\n        \"dlc_training_params_name\": training_params_name,\n    }\n).fetch1(\"KEY\")\nsgp.DLCModelTraining.populate(model_training_key)\n</pre> sgp.DLCModelTrainingSelection().insert1(     {         **project_key,         \"dlc_training_params_name\": training_params_name,         \"training_id\": 0,         \"model_prefix\": \"\",     } ) model_training_key = (     sgp.DLCModelTrainingSelection     &amp; {         **project_key,         \"dlc_training_params_name\": training_params_name,     } ).fetch1(\"KEY\") sgp.DLCModelTraining.populate(model_training_key) <p>Here we'll make sure that the entry made it into the table properly!</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelTraining() &amp; model_training_key\n</pre> sgp.DLCModelTraining() &amp; model_training_key <p>Populating <code>DLCModelTraining</code> automatically inserts the entry into <code>DLCModelSource</code>, which is used to select between models trained using Spyglass vs. other tools.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource() &amp; model_training_key\n</pre> sgp.DLCModelSource() &amp; model_training_key <p>The <code>source</code> field will only accept \"FromImport\" or \"FromUpstream\" as entries. Let's checkout the <code>FromUpstream</code> part table attached to <code>DLCModelSource</code> below.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource.FromUpstream() &amp; model_training_key\n</pre> sgp.DLCModelSource.FromUpstream() &amp; model_training_key In\u00a0[\u00a0]: Copied! <pre>pprint(sgp.DLCModelParams.get_default())\n</pre> pprint(sgp.DLCModelParams.get_default()) <p>Here is the syntax to add your own parameter set:</p> <pre>dlc_model_params_name = \"make_this_yours\"\nparams = {\n    \"params\": {},\n    \"shuffle\": 1,\n    \"trainingsetindex\": 0,\n    \"model_prefix\": \"\",\n}\nsgp.DLCModelParams.insert1(\n    {\"dlc_model_params_name\": dlc_model_params_name, \"params\": params},\n    skip_duplicates=True,\n)\n</pre> <p>We can insert sets of parameters into <code>DLCModelSelection</code> and populate <code>DLCModel</code>.</p> In\u00a0[\u00a0]: Copied! <pre>temp_model_key = (sgp.DLCModelSource &amp; model_training_key).fetch1(\"KEY\")\nsgp.DLCModelSelection().insert1(\n    {**temp_model_key, \"dlc_model_params_name\": \"default\"}, skip_duplicates=True\n)\nmodel_key = (sgp.DLCModelSelection &amp; temp_model_key).fetch1(\"KEY\")\nsgp.DLCModel.populate(model_key)\n</pre> temp_model_key = (sgp.DLCModelSource &amp; model_training_key).fetch1(\"KEY\") sgp.DLCModelSelection().insert1(     {**temp_model_key, \"dlc_model_params_name\": \"default\"}, skip_duplicates=True ) model_key = (sgp.DLCModelSelection &amp; temp_model_key).fetch1(\"KEY\") sgp.DLCModel.populate(model_key) <p>Again, let's make sure that everything looks correct in <code>DLCModel</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModel() &amp; model_key\n</pre> sgp.DLCModel() &amp; model_key Return To Table of Contents"}, {"location": "notebooks/21_Position_DLC_1/#position-deeplabcut-from-scratch", "title": "Position - DeepLabCut from Scratch\u00b6", "text": ""}, {"location": "notebooks/21_Position_DLC_1/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/21_Position_DLC_1/#table-of-contents", "title": "Table of Contents\u00b6", "text": ""}, {"location": "notebooks/21_Position_DLC_1/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/21_Position_DLC_1/#dlcproject", "title": "DLCProject \u00b6", "text": ""}, {"location": "notebooks/21_Position_DLC_1/#dlcmodeltraining", "title": "DLCModelTraining\u00b6", "text": "<p>Please make sure you're running this notebook on a GPU-enabled machine.</p> <p>Now that we've imported existing frames, we can get ready to train our model.</p> <p>First, we'll need to define a set of parameters for <code>DLCModelTrainingParams</code>, which will get used by DeepLabCut during training. Let's start with <code>gputouse</code>, which determines which GPU core to use.</p> <p>The cell below determines which core has space and set the <code>gputouse</code> variable accordingly.</p>"}, {"location": "notebooks/21_Position_DLC_1/#dlcmodel", "title": "DLCModel \u00b6", "text": "<p>Next we'll populate the <code>DLCModel</code> table, which holds all the relevant information for all trained models.</p> <p>First, we'll need to determine a set of parameters for our model to select the correct model file. Here is the default:</p>"}, {"location": "notebooks/21_Position_DLC_1/#next-steps", "title": "Next Steps\u00b6", "text": "<p>With our trained model in place, we're ready to move on to pose estimation (notebook coming soon!).</p>"}, {"location": "notebooks/22_Position_DLC_2/", "title": "DLC From Model", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> <p>This is a tutorial will cover how to extract position given a pre-trained DeepLabCut (DLC) model. It will walk through adding your DLC model to Spyglass.</p> <p>If you already have a model in the database, skip to the next tutorial.</p> In\u00a0[13]: Copied! <pre>import os\nimport datajoint as dj\nfrom pprint import pprint\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nfrom spyglass.settings import load_config\n\nload_config(base_dir=\"/home/cb/wrk/zOther/data/\")\n\nimport spyglass.common as sgc\nimport spyglass.position.v1 as sgp\nfrom spyglass.position import PositionOutput\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import datajoint as dj from pprint import pprint  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  from spyglass.settings import load_config  load_config(base_dir=\"/home/cb/wrk/zOther/data/\")  import spyglass.common as sgc import spyglass.position.v1 as sgp from spyglass.position import PositionOutput  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) Here is a schematic showing the tables used in this notebook. <p></p> <p>We'll look at the BodyPart table, which stores standard names of body parts used within DLC models.</p> Notes:<ul> <li>         Please do not add to the <code>BodyPart</code> table in the production          database unless necessary.     </li> </ul> In\u00a0[\u00a0]: Copied! <pre>sgp.BodyPart()\n</pre> sgp.BodyPart() <p>We can <code>insert_existing_project</code> into the <code>DLCProject</code> table using:</p> <ul> <li><code>project_name</code>: a short, unique, descriptive project name to reference throughout the pipeline</li> <li><code>lab_team</code>: team name from <code>LabTeam</code></li> <li><code>config_path</code>: string path to a DLC <code>config.yaml</code></li> <li><code>bodyparts</code>: optional list of bodyparts used in the project</li> <li><code>frames_per_video</code>: optional, number of frames to extract for training from each video</li> </ul> In\u00a0[\u00a0]: Copied! <pre>project_name = \"tutorial_DG\"\nlab_team = \"LorenLab\"\nproject_key = sgp.DLCProject.insert_existing_project(\n    project_name=project_name,\n    lab_team=lab_team,\n    config_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/config.yaml\",\n    bodyparts=[\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"],\n    frames_per_video=200,\n    skip_duplicates=True,\n)\n</pre> project_name = \"tutorial_DG\" lab_team = \"LorenLab\" project_key = sgp.DLCProject.insert_existing_project(     project_name=project_name,     lab_team=lab_team,     config_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/config.yaml\",     bodyparts=[\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"],     frames_per_video=200,     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCProject() &amp; {\"project_name\": project_name}\n</pre> sgp.DLCProject() &amp; {\"project_name\": project_name} <p>The <code>DLCModelInput</code> table has <code>dlc_model_name</code> and <code>project_name</code> as primary keys and <code>project_path</code> as a secondary key.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelInput()\n</pre> sgp.DLCModelInput() <p>We can modify the <code>project_key</code> to replace <code>config_path</code> with <code>project_path</code> to fit with the fields in <code>DLCModelInput</code></p> In\u00a0[\u00a0]: Copied! <pre>print(f\"current project_key:\\n{project_key}\")\nif not \"project_path\" in project_key:\n    project_key[\"project_path\"] = os.path.dirname(project_key[\"config_path\"])\n    del project_key[\"config_path\"]\n    print(f\"updated project_key:\\n{project_key}\")\n</pre> print(f\"current project_key:\\n{project_key}\") if not \"project_path\" in project_key:     project_key[\"project_path\"] = os.path.dirname(project_key[\"config_path\"])     del project_key[\"config_path\"]     print(f\"updated project_key:\\n{project_key}\") <p>After adding a unique <code>dlc_model_name</code> to <code>project_key</code>, we insert into <code>DLCModelInput</code>.</p> In\u00a0[\u00a0]: Copied! <pre>dlc_model_name = \"tutorial_model_DG\"\nsgp.DLCModelInput().insert1(\n    {\"dlc_model_name\": dlc_model_name, **project_key}, skip_duplicates=True\n)\nsgp.DLCModelInput()\n</pre> dlc_model_name = \"tutorial_model_DG\" sgp.DLCModelInput().insert1(     {\"dlc_model_name\": dlc_model_name, **project_key}, skip_duplicates=True ) sgp.DLCModelInput() <p>Inserting into <code>DLCModelInput</code> will also populate <code>DLCModelSource</code>, which records whether or not a model was trained with Spyglass.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource() &amp; project_key\n</pre> sgp.DLCModelSource() &amp; project_key <p>The <code>source</code> field will only accept \"FromImport\" or \"FromUpstream\" as entries. Let's checkout the <code>FromUpstream</code> part table attached to <code>DLCModelSource</code> below.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource.FromImport() &amp; project_key\n</pre> sgp.DLCModelSource.FromImport() &amp; project_key <p>Next we'll get ready to populate the <code>DLCModel</code> table, which holds all the relevant information for both pre-trained models and models trained within Spyglass.First we'll need to determine a set of parameters for our model to select the correct model file.We can visualize a default set below:</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelParams.get_default()\n</pre> sgp.DLCModelParams.get_default() <p>Here is the syntax to add your own parameter set:</p> <pre>dlc_model_params_name = \"make_this_yours\"\nparams = {\n    \"params\": {},\n    \"shuffle\": 1,\n    \"trainingsetindex\": 0,\n    \"model_prefix\": \"\",\n}\nsgp.DLCModelParams.insert1(\n    {\"dlc_model_params_name\": dlc_model_params_name, \"params\": params},\n    skip_duplicates=True,\n)\n</pre> <p>We can insert sets of parameters into <code>DLCModelSelection</code> and populate <code>DLCModel</code>.</p> In\u00a0[\u00a0]: Copied! <pre>temp_model_key = (sgp.DLCModelSource.FromImport() &amp; project_key).fetch1(\"KEY\")\nsgp.DLCModelSelection().insert1(\n    {**temp_model_key, \"dlc_model_params_name\": \"default\"}, skip_duplicates=True\n)\nmodel_key = (sgp.DLCModelSelection &amp; temp_model_key).fetch1(\"KEY\")\nsgp.DLCModel.populate(model_key)\n</pre> temp_model_key = (sgp.DLCModelSource.FromImport() &amp; project_key).fetch1(\"KEY\") sgp.DLCModelSelection().insert1(     {**temp_model_key, \"dlc_model_params_name\": \"default\"}, skip_duplicates=True ) model_key = (sgp.DLCModelSelection &amp; temp_model_key).fetch1(\"KEY\") sgp.DLCModel.populate(model_key) <p>And of course make sure it populated correctly</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModel() &amp; model_key\n</pre> sgp.DLCModel() &amp; model_key <code>Return To Table of Contents</code>"}, {"location": "notebooks/22_Position_DLC_2/#position-deeplabcut-pretrained", "title": "Position - DeepLabCut PreTrained\u00b6", "text": ""}, {"location": "notebooks/22_Position_DLC_2/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/22_Position_DLC_2/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/22_Position_DLC_2/#table-of-contents", "title": "Table of Contents\u00b6", "text": "<ul> <li><code>DLCProject</code></li> <li><code>DLCModel</code></li> </ul> <p>You can click on any header to return to the Table of Contents</p>"}, {"location": "notebooks/22_Position_DLC_2/#dlcproject", "title": "DLCProject \u00b6", "text": ""}, {"location": "notebooks/22_Position_DLC_2/#dlcmodel", "title": "DLCModel \u00b6", "text": ""}, {"location": "notebooks/22_Position_DLC_2/#next-steps", "title": "Next Steps\u00b6", "text": "<p>With our trained model in place, we're ready to move on to pose estimation (notebook coming soon!).</p>"}, {"location": "notebooks/23_Position_DLC_3/", "title": "DLC Prediction", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> <p>This tutorial will extract position via DeepLabCut (DLC). It will walk through...</p> <ul> <li>executing pose estimation</li> <li>processing the pose estimation output to extract a centroid and orientation</li> <li>inserting the resulting information into the <code>IntervalPositionInfo</code> table</li> </ul> <p>This tutorial assumes you already have a model in your database. If that's not the case, you can either train one from scratch or load an existing project.</p> <p>Here is a schematic showing the tables used in this pipeline.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport datajoint as dj\nfrom pprint import pprint\n\nimport spyglass.common as sgc\nimport spyglass.position.v1 as sgp\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import datajoint as dj from pprint import pprint  import spyglass.common as sgc import spyglass.position.v1 as sgp  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2023-07-28 14:45:50,776][INFO]: Connecting root@localhost:3306\n[2023-07-28 14:45:50,804][INFO]: Connected root@localhost:3306\n</pre> <p>For longer videos, we'll need GPU support. The cell below determines which core has space and set the <code>gputouse</code> variable accordingly.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.dlc_utils.get_gpu_memory()\n</pre> sgp.dlc_utils.get_gpu_memory() <pre>{0: 80383, 1: 35, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 7: 35, 8: 35, 9: 35}</pre> <p>Set GPU core:</p> In\u00a0[\u00a0]: Copied! <pre>gputouse = 1  ## 1-9\n</pre> gputouse = 1  ## 1-9 In\u00a0[\u00a0]: Copied! <pre>nwb_file_name = \"J1620210604_.nwb\"\nepoch = 14\nsgc.VideoFile() &amp; {\"nwb_file_name\": nwb_file_name, \"epoch\": epoch}\n</pre> nwb_file_name = \"J1620210604_.nwb\" epoch = 14 sgc.VideoFile() &amp; {\"nwb_file_name\": nwb_file_name, \"epoch\": epoch} <p>Using <code>insert_estimation_task</code> will convert out video to be in .mp4 format (DLC struggles with .h264) and determine the directory in which we'll store the pose estimation results.</p> <ul> <li><code>task_mode</code> (trigger or load) determines whether or not populating <code>DLCPoseEstimation</code> triggers a new pose estimation, or loads an existing.</li> <li><code>video_file_num</code> will be 0 in almost all cases.</li> <li><code>gputouse</code> was already set during training. It may be a good idea to make sure that core is still free before moving forward.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>pose_estimation_key = sgp.DLCPoseEstimationSelection.insert_estimation_task(\n    {\n        \"nwb_file_name\": nwb_file_name,\n        \"epoch\": epoch,\n        \"video_file_num\": 0,\n        **model_key,\n    },\n    task_mode=\"trigger\",\n    params={\"gputouse\": gputouse, \"videotype\": \"mp4\"},\n)\n</pre> pose_estimation_key = sgp.DLCPoseEstimationSelection.insert_estimation_task(     {         \"nwb_file_name\": nwb_file_name,         \"epoch\": epoch,         \"video_file_num\": 0,         **model_key,     },     task_mode=\"trigger\",     params={\"gputouse\": gputouse, \"videotype\": \"mp4\"}, ) <p>Note: Populating <code>DLCPoseEstimation</code> may take some time for full datasets</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPoseEstimation().populate(pose_estimation_key)\n</pre> sgp.DLCPoseEstimation().populate(pose_estimation_key) <p>Let's visualize the output from Pose Estimation</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPoseEstimation() &amp; pose_estimation_key).fetch_dataframe()\n</pre> (sgp.DLCPoseEstimation() &amp; pose_estimation_key).fetch_dataframe() <p>After pose estimation, we can interpolate over low likelihood periods and smooth the resulting position.</p> <p>First we define some parameters. We can see the default parameter set below.</p> In\u00a0[\u00a0]: Copied! <pre>pprint(sgp.DLCSmoothInterpParams.get_default())\nsi_params_name = \"default\"\n</pre> pprint(sgp.DLCSmoothInterpParams.get_default()) si_params_name = \"default\" <p>To change any of these parameters, one would do the following:</p> <pre>si_params_name = \"your_unique_param_name\"\nparams = {\n    \"smoothing_params\": {\n        \"smoothing_duration\": 0.00,\n        \"smooth_method\": \"moving_avg\",\n    },\n    \"interp_params\": {\"likelihood_thresh\": 0.00},\n    \"max_plausible_speed\": 0,\n    \"speed_smoothing_std_dev\": 0.000,\n}\nsgp.DLCSmoothInterpParams().insert1(\n    {\"dlc_si_params_name\": si_params_name, \"params\": params},\n    skip_duplicates=True,\n)\n</pre> <p>We'll create a dictionary with the correct set of keys for the <code>DLCSmoothInterpSelection</code> table</p> In\u00a0[\u00a0]: Copied! <pre>si_key = pose_estimation_key.copy()\nfields = list(sgp.DLCSmoothInterpSelection.fetch().dtype.fields.keys())\nsi_key = {key: val for key, val in si_key.items() if key in fields}\nsi_key\n</pre> si_key = pose_estimation_key.copy() fields = list(sgp.DLCSmoothInterpSelection.fetch().dtype.fields.keys()) si_key = {key: val for key, val in si_key.items() if key in fields} si_key <p>We can insert all of the bodyparts we want to process into <code>DLCSmoothInterpSelection</code>. Here are the bodyparts we have available to us:</p> In\u00a0[\u00a0]: Copied! <pre>pprint((sgp.DLCPoseEstimation.BodyPart &amp; pose_estimation_key).fetch(\"bodypart\"))\n</pre> pprint((sgp.DLCPoseEstimation.BodyPart &amp; pose_estimation_key).fetch(\"bodypart\")) <p>We can use <code>insert1</code> to insert a single bodypart, but would suggest using <code>insert</code> to insert a list of keys with different bodyparts.</p> <p>We'll set a list of bodyparts and then insert them into <code>DLCSmoothInterpSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>bodyparts = [\"greenLED\", \"redLED_C\"]\nsgp.DLCSmoothInterpSelection.insert(\n    [\n        {\n            **si_key,\n            \"bodypart\": bodypart,\n            \"dlc_si_params_name\": si_params_name,\n        }\n        for bodypart in bodyparts\n    ],\n    skip_duplicates=True,\n)\n</pre> bodyparts = [\"greenLED\", \"redLED_C\"] sgp.DLCSmoothInterpSelection.insert(     [         {             **si_key,             \"bodypart\": bodypart,             \"dlc_si_params_name\": si_params_name,         }         for bodypart in bodyparts     ],     skip_duplicates=True, ) <p>And verify the entry:</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpSelection() &amp; si_key\n</pre> sgp.DLCSmoothInterpSelection() &amp; si_key <p>Now, we populate <code>DLCSmoothInterp</code>, which will perform smoothing and interpolation on all of the bodyparts specified.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterp().populate(si_key)\n</pre> sgp.DLCSmoothInterp().populate(si_key) <p>And let's visualize the resulting position data using a scatter plot</p> In\u00a0[\u00a0]: Copied! <pre>(\n    sgp.DLCSmoothInterp() &amp; {**si_key, \"bodypart\": bodyparts[0]}\n).fetch1_dataframe().plot.scatter(x=\"x\", y=\"y\", s=1, figsize=(5, 5))\n</pre> (     sgp.DLCSmoothInterp() &amp; {**si_key, \"bodypart\": bodyparts[0]} ).fetch1_dataframe().plot.scatter(x=\"x\", y=\"y\", s=1, figsize=(5, 5)) <p>After smoothing/interpolation, we need to select bodyparts from which we want to derive a centroid and orientation, which is performed by the <code>DLCSmoothInterpCohort</code> table.</p> <p>First, let's make a key that represents the 'cohort', using <code>dlc_si_cohort_selection_name</code>. We'll need a bodypart dictionary using bodypart keys and smoothing/interpolation parameters used as value.</p> In\u00a0[\u00a0]: Copied! <pre>cohort_key = si_key.copy()\nif \"bodypart\" in cohort_key:\n    del cohort_key[\"bodypart\"]\nif \"dlc_si_params_name\" in cohort_key:\n    del cohort_key[\"dlc_si_params_name\"]\ncohort_key[\"dlc_si_cohort_selection_name\"] = \"green_red_led\"\ncohort_key[\"bodyparts_params_dict\"] = {\n    \"greenLED\": si_params_name,\n    \"redLED_C\": si_params_name,\n}\nprint(cohort_key)\n</pre> cohort_key = si_key.copy() if \"bodypart\" in cohort_key:     del cohort_key[\"bodypart\"] if \"dlc_si_params_name\" in cohort_key:     del cohort_key[\"dlc_si_params_name\"] cohort_key[\"dlc_si_cohort_selection_name\"] = \"green_red_led\" cohort_key[\"bodyparts_params_dict\"] = {     \"greenLED\": si_params_name,     \"redLED_C\": si_params_name, } print(cohort_key) <p>We'll insert the cohort into <code>DLCSmoothInterpCohortSelection</code> and populate <code>DLCSmoothInterpCohort</code>, which collates the separately smoothed and interpolated bodyparts into a single entry.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpCohortSelection().insert1(cohort_key, skip_duplicates=True)\nsgp.DLCSmoothInterpCohort.populate(cohort_key)\n</pre> sgp.DLCSmoothInterpCohortSelection().insert1(cohort_key, skip_duplicates=True) sgp.DLCSmoothInterpCohort.populate(cohort_key) <p>And verify the entry:</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpCohort.BodyPart() &amp; cohort_key\n</pre> sgp.DLCSmoothInterpCohort.BodyPart() &amp; cohort_key <p>With this cohort, we can determine a centroid using another set of parameters.</p> In\u00a0[\u00a0]: Copied! <pre># Here is the default set\nprint(sgp.DLCCentroidParams.get_default())\ncentroid_params_name = \"default\"\n</pre> # Here is the default set print(sgp.DLCCentroidParams.get_default()) centroid_params_name = \"default\" <p>Here is the syntax to add your own parameters:</p> <pre>centroid_params = {\n    \"centroid_method\": \"two_pt_centroid\",\n    \"points\": {\n        \"greenLED\": \"greenLED\",\n        \"redLED_C\": \"redLED_C\",\n    },\n    \"speed_smoothing_std_dev\": 0.100,\n}\ncentroid_params_name = \"your_unique_param_name\"\nsgp.DLCCentroidParams.insert1(\n    {\n        \"dlc_centroid_params_name\": centroid_params_name,\n        \"params\": centroid_params,\n    },\n    skip_duplicates=True,\n)\n</pre> <p>We'll make a key to insert into <code>DLCCentroidSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>centroid_key = cohort_key.copy()\nfields = list(sgp.DLCCentroidSelection.fetch().dtype.fields.keys())\ncentroid_key = {key: val for key, val in centroid_key.items() if key in fields}\ncentroid_key[\"dlc_centroid_params_name\"] = centroid_params_name\npprint(centroid_key)\n</pre> centroid_key = cohort_key.copy() fields = list(sgp.DLCCentroidSelection.fetch().dtype.fields.keys()) centroid_key = {key: val for key, val in centroid_key.items() if key in fields} centroid_key[\"dlc_centroid_params_name\"] = centroid_params_name pprint(centroid_key) <p>After inserting into the selection table, we can populate <code>DLCCentroid</code></p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCCentroidSelection.insert1(centroid_key, skip_duplicates=True)\nsgp.DLCCentroid.populate(centroid_key)\n</pre> sgp.DLCCentroidSelection.insert1(centroid_key, skip_duplicates=True) sgp.DLCCentroid.populate(centroid_key) <p>Here we can visualize the resulting centroid position</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCCentroid() &amp; centroid_key).fetch1_dataframe().plot.scatter(\n    x=\"position_x\",\n    y=\"position_y\",\n    c=\"speed\",\n    colormap=\"viridis\",\n    alpha=0.5,\n    s=0.5,\n    figsize=(10, 10),\n)\n</pre> (sgp.DLCCentroid() &amp; centroid_key).fetch1_dataframe().plot.scatter(     x=\"position_x\",     y=\"position_y\",     c=\"speed\",     colormap=\"viridis\",     alpha=0.5,     s=0.5,     figsize=(10, 10), ) <p>We'll go through a similar process for orientation.</p> In\u00a0[\u00a0]: Copied! <pre>pprint(sgp.DLCOrientationParams.get_default())\ndlc_orientation_params_name = \"default\"\n</pre> pprint(sgp.DLCOrientationParams.get_default()) dlc_orientation_params_name = \"default\" <p>We'll prune the <code>cohort_key</code> we used above and add our <code>dlc_orientation_params_name</code> to make it suitable for <code>DLCOrientationSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>fields = list(sgp.DLCOrientationSelection.fetch().dtype.fields.keys())\norient_key = {key: val for key, val in cohort_key.items() if key in fields}\norient_key[\"dlc_orientation_params_name\"] = dlc_orientation_params_name\nprint(orient_key)\n</pre> fields = list(sgp.DLCOrientationSelection.fetch().dtype.fields.keys()) orient_key = {key: val for key, val in cohort_key.items() if key in fields} orient_key[\"dlc_orientation_params_name\"] = dlc_orientation_params_name print(orient_key) <p>We'll insert into <code>DLCOrientationSelection</code> and then populate <code>DLCOrientation</code></p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCOrientationSelection().insert1(orient_key, skip_duplicates=True)\nsgp.DLCOrientation().populate(orient_key)\n</pre> sgp.DLCOrientationSelection().insert1(orient_key, skip_duplicates=True) sgp.DLCOrientation().populate(orient_key) <p>We can fetch the orientation as a dataframe as quality assurance.</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCOrientation() &amp; orient_key).fetch1_dataframe()\n</pre> (sgp.DLCOrientation() &amp; orient_key).fetch1_dataframe() <p>After processing the position data, we have to do a few table manipulations to standardize various outputs.</p> <p>To summarize, we brought in a pretrained DLC project, used that model to run pose estimation on a new behavioral video, smoothed and interpolated the result, formed a cohort of bodyparts, and determined the centroid and orientation of this cohort.</p> <p>Now we'll populate <code>DLCPos</code> with our centroid/orientation entries above.</p> In\u00a0[\u00a0]: Copied! <pre>fields = list(sgp.DLCPos.fetch().dtype.fields.keys())\ndlc_key = {key: val for key, val in centroid_key.items() if key in fields}\ndlc_key[\"dlc_si_cohort_centroid\"] = centroid_key[\"dlc_si_cohort_selection_name\"]\ndlc_key[\"dlc_si_cohort_orientation\"] = orient_key[\n    \"dlc_si_cohort_selection_name\"\n]\ndlc_key[\"dlc_orientation_params_name\"] = orient_key[\n    \"dlc_orientation_params_name\"\n]\npprint(dlc_key)\n</pre> fields = list(sgp.DLCPos.fetch().dtype.fields.keys()) dlc_key = {key: val for key, val in centroid_key.items() if key in fields} dlc_key[\"dlc_si_cohort_centroid\"] = centroid_key[\"dlc_si_cohort_selection_name\"] dlc_key[\"dlc_si_cohort_orientation\"] = orient_key[     \"dlc_si_cohort_selection_name\" ] dlc_key[\"dlc_orientation_params_name\"] = orient_key[     \"dlc_orientation_params_name\" ] pprint(dlc_key) <p>Now we can insert into <code>DLCPosSelection</code> and populate <code>DLCPos</code> with our <code>dlc_key</code></p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosSelection().insert1(dlc_key, skip_duplicates=True)\nsgp.DLCPos().populate(dlc_key)\n</pre> sgp.DLCPosSelection().insert1(dlc_key, skip_duplicates=True) sgp.DLCPos().populate(dlc_key) <p>Fetched as a dataframe, we expect the following 8 columns:</p> <ul> <li>time</li> <li>video_frame_ind</li> <li>position_x</li> <li>position_y</li> <li>orientation</li> <li>velocity_x</li> <li>velocity_y</li> <li>speed</li> </ul> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPos() &amp; dlc_key).fetch1_dataframe()\n</pre> (sgp.DLCPos() &amp; dlc_key).fetch1_dataframe() <p>We can also fetch the <code>pose_eval_result</code>, which contains the percentage of frames that each bodypart was below the likelihood threshold of 0.95.</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPos() &amp; dlc_key).fetch1(\"pose_eval_result\")\n</pre> (sgp.DLCPos() &amp; dlc_key).fetch1(\"pose_eval_result\") <p>We can create a video with the centroid and orientation overlaid on the original video. This will also plot the likelihood of each bodypart used in the cohort. This is optional, but a good quality assurance step.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideoParams.insert_default()\n</pre> sgp.DLCPosVideoParams.insert_default() In\u00a0[\u00a0]: Copied! <pre>params = {\n    \"percent_frames\": 0.05,\n    \"incl_likelihood\": True,\n}\nsgp.DLCPosVideoParams.insert1(\n    {\"dlc_pos_video_params_name\": \"five_percent\", \"params\": params},\n    skip_duplicates=True,\n)\n</pre> params = {     \"percent_frames\": 0.05,     \"incl_likelihood\": True, } sgp.DLCPosVideoParams.insert1(     {\"dlc_pos_video_params_name\": \"five_percent\", \"params\": params},     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideoSelection.insert1(\n    {**dlc_key, \"dlc_pos_video_params_name\": \"five_percent\"},\n    skip_duplicates=True,\n)\n</pre> sgp.DLCPosVideoSelection.insert1(     {**dlc_key, \"dlc_pos_video_params_name\": \"five_percent\"},     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideo().populate(dlc_key)\n</pre> sgp.DLCPosVideo().populate(dlc_key) <p><code>PositionOutput</code> is the final table of the pipeline and is automatically populated when we populate <code>DLCPosV1</code></p> In\u00a0[\u00a0]: Copied! <pre>sgp.PositionOutput() &amp; dlc_key\n</pre> sgp.PositionOutput() &amp; dlc_key <p><code>PositionOutput</code> also has a part table, similar to the <code>DLCModelSource</code> table above. Let's check that out as well.</p> In\u00a0[\u00a0]: Copied! <pre>PositionOutput.DLCPosV1() &amp; dlc_key\n</pre> PositionOutput.DLCPosV1() &amp; dlc_key In\u00a0[\u00a0]: Copied! <pre>(PositionOutput.DLCPosV1() &amp; dlc_key).fetch1_dataframe()\n</pre> (PositionOutput.DLCPosV1() &amp; dlc_key).fetch1_dataframe() <p>We can use the <code>PositionVideo</code> table to create a video that overlays just the centroid and orientation on the video. This table uses the parameter <code>plot</code> to determine whether to plot the entry deriving from the DLC arm or from the Trodes arm of the position pipeline. This parameter also accepts 'all', which will plot both (if they exist) in order to compare results.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.PositionVideoSelection().insert1(\n    {\n        \"nwb_file_name\": \"J1620210604_.nwb\",\n        \"interval_list_name\": \"pos 13 valid times\",\n        \"trodes_position_id\": 0,\n        \"dlc_position_id\": 1,\n        \"plot\": \"DLC\",\n        \"output_dir\": \"/home/dgramling/Src/\",\n    }\n)\n</pre> sgp.PositionVideoSelection().insert1(     {         \"nwb_file_name\": \"J1620210604_.nwb\",         \"interval_list_name\": \"pos 13 valid times\",         \"trodes_position_id\": 0,         \"dlc_position_id\": 1,         \"plot\": \"DLC\",         \"output_dir\": \"/home/dgramling/Src/\",     } ) In\u00a0[\u00a0]: Copied! <pre>sgp.PositionVideo.populate({\"plot\": \"DLC\"})\n</pre> sgp.PositionVideo.populate({\"plot\": \"DLC\"}) <p>CONGRATULATIONS!! Please treat yourself to a nice tea break :-)</p> Return To Table of Contents"}, {"location": "notebooks/23_Position_DLC_3/#position-deeplabcut-estimation", "title": "Position - DeepLabCut Estimation\u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#table-of-contents", "title": "Table of Contents\u00b6", "text": "<ul> <li>Imports</li> <li>GPU</li> <li><code>DLCPoseEstimation</code></li> <li><code>DLCSmoothInterp</code></li> <li><code>DLCCentroid</code></li> <li><code>DLCOrientation</code></li> <li><code>DLCPos</code></li> <li><code>DLCPosVideo</code></li> <li><code>PosSource</code></li> <li><code>IntervalPositionInfo</code></li> </ul> <p>You can click on any header to return to the Table of Contents</p>"}, {"location": "notebooks/23_Position_DLC_3/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#gpu", "title": "GPU\u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#dlcposeestimation", "title": "DLCPoseEstimation \u00b6", "text": "<p>With our trained model in place, we're ready to set up Pose Estimation on a behavioral video of your choice. We can select a video with <code>nwb_file_name</code> and <code>epoch</code>, making sure there's an entry in the <code>VideoFile</code> table.</p>"}, {"location": "notebooks/23_Position_DLC_3/#dlcsmoothinterp", "title": "DLCSmoothInterp \u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#dlcsmoothinterpcohort", "title": "DLCSmoothInterpCohort \u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#dlccentroid", "title": "DLCCentroid \u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#dlcorientation", "title": "DLCOrientation \u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#dlcpos", "title": "DLCPos \u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#dlcposvideo", "title": "DLCPosVideo \u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#positionoutput", "title": "PositionOutput \u00b6", "text": ""}, {"location": "notebooks/23_Position_DLC_3/#positionvideo", "title": "PositionVideo\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/", "title": "Linearization", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> <p>This pipeline takes 2D position data from the <code>IntervalPositionInfo</code> table and \"linearizes\" it to 1D position. If you haven't already done so, please generate input data with either the Trodes or DLC notebooks (1, 2, 3).</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n</pre> %reload_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import os\nimport pynwb\nimport numpy as np\nimport datajoint as dj\nimport matplotlib.pyplot as plt\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.position.v1 as sgp\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import pynwb import numpy as np import datajoint as dj import matplotlib.pyplot as plt  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.position.v1 as sgp  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2023-07-28 14:45:50,776][INFO]: Connecting root@localhost:3306\n[2023-07-28 14:45:50,804][INFO]: Connected root@localhost:3306\n</pre> <p>To retrieve 2D position data, we'll specify an nwb file, a position time interval, and the set of parameters used to compute the position info.</p> In\u00a0[2]: Copied! <pre>nwb_file_name = \"chimi20200216_new.nwb\"\nnwb_copy_file_name = sgc.nwb_helper_fn.get_nwb_copy_filename(nwb_file_name)\nnwb_copy_file_name\n</pre> nwb_file_name = \"chimi20200216_new.nwb\" nwb_copy_file_name = sgc.nwb_helper_fn.get_nwb_copy_filename(nwb_file_name) nwb_copy_file_name <pre>[2022-08-04 16:16:35,902][INFO]: Connecting zoldello@lmf-db.cin.ucsf.edu:3306\n[2022-08-04 16:16:35,953][INFO]: Connected zoldello@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>/home/zoldello/anaconda3/envs/spyglass/lib/python3.9/site-packages/position_tools/core.py:3: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n  from scipy.ndimage.filters import gaussian_filter1d\n</pre> Out[2]: <pre>'chimi20200216_new_.nwb'</pre> <p>We will fetch the pandas dataframe from the <code>IntervalPositionInfo</code>.</p> In\u00a0[3]: Copied! <pre>position_info = (\n    sgc.common_position.IntervalPositionInfo()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default\",\n    }\n).fetch1_dataframe()\nposition_info\n</pre> position_info = (     sgc.common_position.IntervalPositionInfo()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default\",     } ).fetch1_dataframe() position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_ZF24K2JHC1.nwb\n</pre> Out[3]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.999696 1.387074 2.848838 3.168573 1.581887e+09 90.844337 211.417287 3.078386 3.123201 3.411111 4.624939 1.581887e+09 90.637025 211.707525 -3.114572 5.431643 4.089597 6.799085 1.581887e+09 90.802875 211.596958 -3.033109 8.097753 4.979262 9.506138 1.581887e+09 91.288579 211.482443 -3.062550 10.840482 6.071373 12.424880 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.452467 -0.986926 0.348276 0.218575 0.411182 1.581888e+09 182.158583 201.397183 -0.978610 0.279135 -0.058413 0.285182 1.581888e+09 182.213867 201.341900 -0.957589 0.193798 -0.283200 0.343162 1.581888e+09 182.158583 201.341900 -0.970083 0.110838 -0.417380 0.431846 1.581888e+09 182.158583 201.286617 -0.936414 0.045190 -0.453966 0.456209 <p>39340 rows \u00d7 6 columns</p> <p>Before linearizing, plotting the head position will help us understand the data.</p> In\u00a0[4]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(\n    position_info.head_position_x,\n    position_info.head_position_y,\n    color=\"lightgrey\",\n)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\nax.set_title(\"Head Position\", fontsize=28)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(     position_info.head_position_x,     position_info.head_position_y,     color=\"lightgrey\", ) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) ax.set_title(\"Head Position\", fontsize=28) Out[4]: <pre>Text(0.5, 1.0, 'Head Position')</pre> <p>To linearize, we need a graph of nodes and edges to represent track geometry in the <code>TrackGraph</code> table with four variables:</p> <ul> <li><code>node_positions</code> (cm): the 2D positions of the graph</li> <li><code>edges</code>: how the nodes are connected, as pairs of node indices, labeled by their respective index in <code>node_positions</code>.</li> <li><code>linear_edge_order</code>: layout of edges in linear space in order, as tuples.</li> <li><code>linear_edge_spacing</code>: spacing between each edge, as either a single number for all gaps or an array with a number for each gap. Gaps may be important for edges not connected in 2D space.</li> </ul> <p>For example, (79.910, 216.720) is the 2D position of node 0 and (183.784, 45.375) is the 2D position of node 8. Edge (0, 8) means there is an edge between node 0 and node 8. Nodes order controls order in 1D space. Edge (0, 1) connects from node 0 to 1. Edge (1, 0) would connect from node 1 to 0, reversing the linear positions for that edge.</p> <p>For more examples, see this notebook.</p> In\u00a0[5]: Copied! <pre>node_positions = np.array(\n    [\n        (79.910, 216.720),  # top left well 0\n        (132.031, 187.806),  # top middle intersection 1\n        (183.718, 217.713),  # top right well 2\n        (132.544, 132.158),  # middle intersection 3\n        (87.202, 101.397),  # bottom left intersection 4\n        (31.340, 126.110),  # middle left well 5\n        (180.337, 104.799),  # middle right intersection 6\n        (92.693, 42.345),  # bottom left well 7\n        (183.784, 45.375),  # bottom right well 8\n        (231.338, 136.281),  # middle right well 9\n    ]\n)\n\nedges = np.array(\n    [\n        (0, 1),\n        (1, 2),\n        (1, 3),\n        (3, 4),\n        (4, 5),\n        (3, 6),\n        (6, 9),\n        (4, 7),\n        (6, 8),\n    ]\n)\n\nlinear_edge_order = [\n    (3, 6),\n    (6, 8),\n    (6, 9),\n    (3, 1),\n    (1, 2),\n    (1, 0),\n    (3, 4),\n    (4, 5),\n    (4, 7),\n]\nlinear_edge_spacing = 15\n</pre> node_positions = np.array(     [         (79.910, 216.720),  # top left well 0         (132.031, 187.806),  # top middle intersection 1         (183.718, 217.713),  # top right well 2         (132.544, 132.158),  # middle intersection 3         (87.202, 101.397),  # bottom left intersection 4         (31.340, 126.110),  # middle left well 5         (180.337, 104.799),  # middle right intersection 6         (92.693, 42.345),  # bottom left well 7         (183.784, 45.375),  # bottom right well 8         (231.338, 136.281),  # middle right well 9     ] )  edges = np.array(     [         (0, 1),         (1, 2),         (1, 3),         (3, 4),         (4, 5),         (3, 6),         (6, 9),         (4, 7),         (6, 8),     ] )  linear_edge_order = [     (3, 6),     (6, 8),     (6, 9),     (3, 1),     (1, 2),     (1, 0),     (3, 4),     (4, 5),     (4, 7), ] linear_edge_spacing = 15 <p>With these variables, we then add a <code>track_graph_name</code> and the corresponding <code>environment</code>.</p> In\u00a0[6]: Copied! <pre>sgc.common_position.TrackGraph.insert1(\n    {\n        \"track_graph_name\": \"6 arm\",\n        \"environment\": \"6 arm\",\n        \"node_positions\": node_positions,\n        \"edges\": edges,\n        \"linear_edge_order\": linear_edge_order,\n        \"linear_edge_spacing\": linear_edge_spacing,\n    },\n    skip_duplicates=True,\n)\n\ngraph = sgc.common_position.TrackGraph &amp; {\"track_graph_name\": \"6 arm\"}\ngraph\n</pre> sgc.common_position.TrackGraph.insert1(     {         \"track_graph_name\": \"6 arm\",         \"environment\": \"6 arm\",         \"node_positions\": node_positions,         \"edges\": edges,         \"linear_edge_order\": linear_edge_order,         \"linear_edge_spacing\": linear_edge_spacing,     },     skip_duplicates=True, )  graph = sgc.common_position.TrackGraph &amp; {\"track_graph_name\": \"6 arm\"} graph Out[6]: <p>track_graph_name</p> <p>environment</p> Type of Environment <p>node_positions</p> 2D position of track_graph nodes, shape (n_nodes, 2) <p>edges</p> shape (n_edges, 2) <p>linear_edge_order</p> order of track graph edges in the linear space, shape (n_edges, 2) <p>linear_edge_spacing</p> amount of space between edges in the linear space, shape (n_edges,) <p>linear_edge_specialty</p> denote what edges (denote by 5) are going to be lumped to what edge (denote by 1), shape (n_edges,) 6 arm 6 arm =BLOB= =BLOB= =BLOB= =BLOB= =BLOB= <p>Total: 1</p> <p><code>TrackGraph</code> has several methods for visualizing in 2D and 1D space. <code>plot_track_graph</code> plots in 2D to make sure our layout makes sense.</p> In\u00a0[7]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(\n    position_info.head_position_x,\n    position_info.head_position_y,\n    color=\"lightgrey\",\n    alpha=0.7,\n    zorder=0,\n)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\ngraph.plot_track_graph(ax=ax)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(     position_info.head_position_x,     position_info.head_position_y,     color=\"lightgrey\",     alpha=0.7,     zorder=0, ) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) graph.plot_track_graph(ax=ax) <p><code>plot_track_graph_as_1D</code> shows what this looks like in 1D.</p> In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(20, 1))\ngraph.plot_track_graph_as_1D(ax=ax)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(20, 1)) graph.plot_track_graph_as_1D(ax=ax) <p>By default, linearization assigns each 2D position to its nearest point on the track graph. This is then translated into 1D space.</p> <p>If <code>use_hmm</code> is set to <code>true</code>, a Hidden Markov model is used to assign points. The HMM takes into account the prior position and edge, and can keep the position from suddenly jumping to another. Position jumping like this may occur at intersections or the head position swings closer to a non-target reward well while on another edge.</p> In\u00a0[9]: Copied! <pre>sgc.common_position.LinearizationParameters.insert1(\n    {\"linearization_param_name\": \"default\"}, skip_duplicates=True\n)\nsgc.common_position.LinearizationParameters()\n</pre> sgc.common_position.LinearizationParameters.insert1(     {\"linearization_param_name\": \"default\"}, skip_duplicates=True ) sgc.common_position.LinearizationParameters() Out[9]: <p>linearization_param_name</p> name for this set of parameters <p>use_hmm</p> use HMM to determine linearization <p>route_euclidean_distance_scaling</p> How much to prefer route distances between successive time points that are closer to the euclidean distance. Smaller numbers mean the route distance is more likely to be close to the euclidean distance. <p>sensor_std_dev</p> Uncertainty of position sensor (in cm). <p>diagonal_bias</p> Biases the transition matrix to prefer the current track segment. default 0 1.0 5.0 0.5 <p>Total: 1</p> <p>With linearization parameters, we specify the position interval we wish to linearize.</p> In\u00a0[10]: Copied! <pre>sgc.common_position.IntervalLinearizationSelection.insert1(\n    {\n        \"position_info_param_name\": \"default\",\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"track_graph_name\": \"6 arm\",\n        \"linearization_param_name\": \"default\",\n    },\n    skip_duplicates=True,\n)\n\nsgc.common_position.IntervalLinearizationSelection()\n</pre> sgc.common_position.IntervalLinearizationSelection.insert1(     {         \"position_info_param_name\": \"default\",         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"track_graph_name\": \"6 arm\",         \"linearization_param_name\": \"default\",     },     skip_duplicates=True, )  sgc.common_position.IntervalLinearizationSelection() Out[10]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>track_graph_name</p> <p>linearization_param_name</p> name for this set of parameters default chimi20200216_new_.nwb pos 1 valid times 6 arm default <p>Total: 1</p> <p>And then run linearization by populating <code>IntervalLinearizedPosition</code>.</p> In\u00a0[11]: Copied! <pre>sgc.common_position.IntervalLinearizedPosition().populate()\nsgc.common_position.IntervalLinearizedPosition()\n</pre> sgc.common_position.IntervalLinearizedPosition().populate() sgc.common_position.IntervalLinearizedPosition() <pre>Computing linear position for: {'position_info_param_name': 'default', 'nwb_file_name': 'chimi20200216_new_.nwb', 'interval_list_name': 'pos 1 valid times', 'track_graph_name': '6 arm', 'linearization_param_name': 'default'}\nWriting new NWB file chimi20200216_new_GBGCXYMIWB.nwb\n</pre> Out[11]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>track_graph_name</p> <p>linearization_param_name</p> name for this set of parameters <p>analysis_file_name</p> name of the file <p>linearized_position_object_id</p> default chimi20200216_new_.nwb pos 1 valid times 6 arm default chimi20200216_new_GBGCXYMIWB.nwb 8d132da2-c1e4-402f-ba3a-4b8725a6c87a <p>Total: 1</p> <p>Running <code>fetch1_dataframe</code> will retrieve the linear position data, including...</p> <ul> <li><code>time</code>: dataframe index</li> <li><code>linear_position</code>: 1D linearized position</li> <li><code>track_segment_id</code>: index number of the edges given to track graph</li> <li><code>projected_{x,y}_position</code>: 2D position projected to the track graph</li> </ul> In\u00a0[12]: Copied! <pre>linear_position_df = (\n    IntervalLinearizedPosition()\n    &amp; {\n        \"position_info_param_name\": \"default\",\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"track_graph_name\": \"6 arm\",\n        \"linearization_param_name\": \"default\",\n    }\n).fetch1_dataframe()\nlinear_position_df\n</pre> linear_position_df = (     IntervalLinearizedPosition()     &amp; {         \"position_info_param_name\": \"default\",         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"track_graph_name\": \"6 arm\",         \"linearization_param_name\": \"default\",     } ).fetch1_dataframe() linear_position_df Out[12]: linear_position track_segment_id projected_x_position projected_y_position time 1.581887e+09 412.042773 0 90.802281 210.677533 1.581887e+09 412.364853 0 90.520636 210.833775 1.581887e+09 412.686934 0 90.238990 210.990018 1.581887e+09 412.488270 0 90.412714 210.893645 1.581887e+09 412.007991 0 90.832697 210.660660 ... ... ... ... ... 1.581888e+09 340.401589 1 175.500994 212.958497 1.581888e+09 340.373902 1 175.477029 212.944630 1.581888e+09 340.394065 1 175.494481 212.954729 1.581888e+09 340.346214 1 175.453064 212.930764 1.581888e+09 340.318527 1 175.429100 212.916898 <p>39340 rows \u00d7 4 columns</p> <p>We'll plot the 1D position over time, colored by edge, and use the 1D track graph layout on the y-axis.</p> In\u00a0[13]: Copied! <pre>fig, ax = plt.subplots(figsize=(20, 13))\nax.scatter(\n    linear_position_df.index,\n    linear_position_df.linear_position,\n    c=linear_position_df.track_segment_id,\n    s=1,\n)\ngraph.plot_track_graph_as_1D(\n    ax=ax, axis=\"y\", other_axis_start=linear_position_df.index[-1] + 10\n)\n\nax.set_xlabel(\"Time [s]\", fontsize=18)\nax.set_ylabel(\"Linear Position [cm]\", fontsize=18)\nax.set_title(\"Linear Position\", fontsize=28)\n</pre> fig, ax = plt.subplots(figsize=(20, 13)) ax.scatter(     linear_position_df.index,     linear_position_df.linear_position,     c=linear_position_df.track_segment_id,     s=1, ) graph.plot_track_graph_as_1D(     ax=ax, axis=\"y\", other_axis_start=linear_position_df.index[-1] + 10 )  ax.set_xlabel(\"Time [s]\", fontsize=18) ax.set_ylabel(\"Linear Position [cm]\", fontsize=18) ax.set_title(\"Linear Position\", fontsize=28) Out[13]: <pre>Text(0.5, 1.0, 'Linear Position')</pre> <p>We can also plot the 2D position projected to the track graph</p> In\u00a0[14]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(\n    position_info.head_position_x,\n    position_info.head_position_y,\n    color=\"lightgrey\",\n    alpha=0.7,\n    zorder=0,\n)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\nax.plot(\n    linear_position_df.projected_x_position,\n    linear_position_df.projected_y_position,\n)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(     position_info.head_position_x,     position_info.head_position_y,     color=\"lightgrey\",     alpha=0.7,     zorder=0, ) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) ax.plot(     linear_position_df.projected_x_position,     linear_position_df.projected_y_position, ) Out[14]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f5d0807e760&gt;]</pre> <p>Note: Work in Progress</p> In\u00a0[15]: Copied! <pre>%matplotlib widget\n\nkey = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": \"pos 1 valid times\",\n}\n\nepoch = (\n    int(\n        key[\"interval_list_name\"]\n        .replace(\"pos \", \"\")\n        .replace(\" valid times\", \"\")\n    )\n    + 1\n)\nvideo_info = (\n    sgc.common_behav.VideoFile()\n    &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch}\n).fetch1()\n\nio = pynwb.NWBHDF5IO(\"/stelmo/nwb/raw/\" + video_info[\"nwb_file_name\"], \"r\")\nnwb_file = io.read()\nnwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\nvideo_filename = nwb_video.external_file.value[0]\n\nfig, ax = plt.subplots(figsize=(8, 8))\npicker = sgc.common_position.NodePicker(ax=ax, video_filename=video_filename)\n</pre> %matplotlib widget  key = {     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": \"pos 1 valid times\", }  epoch = (     int(         key[\"interval_list_name\"]         .replace(\"pos \", \"\")         .replace(\" valid times\", \"\")     )     + 1 ) video_info = (     sgc.common_behav.VideoFile()     &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch} ).fetch1()  io = pynwb.NWBHDF5IO(\"/stelmo/nwb/raw/\" + video_info[\"nwb_file_name\"], \"r\") nwb_file = io.read() nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]] video_filename = nwb_video.external_file.value[0]  fig, ax = plt.subplots(figsize=(8, 8)) picker = sgc.common_position.NodePicker(ax=ax, video_filename=video_filename) <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <p>After selection, we can retrieve the data using the <code>node_positions</code> and <code>edges</code> attributes</p> In\u00a0[16]: Copied! <pre>picker.node_positions\n</pre> picker.node_positions Out[16]: <pre>array([], dtype=float64)</pre> In\u00a0[17]: Copied! <pre>picker.edges\n</pre> picker.edges Out[17]: <pre>[[]]</pre> In\u00a0[31]: Copied! <pre>%matplotlib widget\n\nfig, ax = plt.subplots(figsize=(8, 8))\nselector = sgc.common_position.SelectFromCollection(ax, video_filename)\n\nprint(\"Select points in the figure by enclosing them within a polygon.\")\nprint(\"Press the 'esc' key to start a new polygon.\")\nprint(\"Try holding the 'shift' key to move all of the vertices.\")\nprint(\"Try holding the 'ctrl' key to move a single vertex.\")\n</pre> %matplotlib widget  fig, ax = plt.subplots(figsize=(8, 8)) selector = sgc.common_position.SelectFromCollection(ax, video_filename)  print(\"Select points in the figure by enclosing them within a polygon.\") print(\"Press the 'esc' key to start a new polygon.\") print(\"Try holding the 'shift' key to move all of the vertices.\") print(\"Try holding the 'ctrl' key to move a single vertex.\") <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <pre>Select points in the figure by enclosing them within a polygon.\nPress the 'esc' key to start a new polygon.\nTry holding the 'shift' key to move all of the vertices.\nTry holding the 'ctrl' key to move a single vertex.\n</pre>"}, {"location": "notebooks/24_Linearization/#position-linearization", "title": "Position - Linearization\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#retrieve-2d-position", "title": "Retrieve 2D position\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#specifying-the-track", "title": "Specifying the track\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#parameters", "title": "Parameters\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#linearization", "title": "Linearization\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#examine-data", "title": "Examine data\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#interactive-selection", "title": "Interactive selection\u00b6", "text": ""}, {"location": "notebooks/24_Linearization/#nodepicker", "title": "<code>NodePicker</code>\u00b6", "text": "<p>Linearization heavily depends on the track graph is specified. To help simplify setting the nodes/edges, we can use the <code>NodePicker</code> to interactively set node positions and edges based on the video.</p>"}, {"location": "notebooks/24_Linearization/#selector", "title": "Selector\u00b6", "text": "<p>We can also draw a 2d polygon around the track and attempt to recover the graph.</p>"}, {"location": "notebooks/24_Linearization/#up-next", "title": "Up Next\u00b6", "text": "<p>Next, we'll combine ephys and position data in a process called ripple detection.</p>"}, {"location": "notebooks/30_Ripple_Detection/", "title": "Ripple Detection", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> </ul> <p>Ripple detection depends on a set of LFPs, the parameters used for detection and the speed of the animal. You will need <code>RippleLFPSelection</code>, <code>RippleParameters</code>, and <code>PositionOutput</code> to be populated accordingly.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport copy\nimport datajoint as dj\nimport numpy as np\nimport pandas as pd\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.position as sgp\nimport spyglass.lfp as lfp\nimport spyglass.lfp.analysis.v1 as lfp_analysis\nfrom spyglass.lfp import LFPOutput\nimport spyglass.lfp.v1 as sglfp\nfrom spyglass.position import PositionOutput\nimport spyglass.ripple.v1 as sgrip\nimport spyglass.ripple.v1 as sgr\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import copy import datajoint as dj import numpy as np import pandas as pd  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.position as sgp import spyglass.lfp as lfp import spyglass.lfp.analysis.v1 as lfp_analysis from spyglass.lfp import LFPOutput import spyglass.lfp.v1 as sglfp from spyglass.position import PositionOutput import spyglass.ripple.v1 as sgrip import spyglass.ripple.v1 as sgr  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2023-07-28 14:45:50,776][INFO]: Connecting root@localhost:3306\n[2023-07-28 14:45:50,804][INFO]: Connected root@localhost:3306\n</pre> <p>First, we'll pick the electrodes on which we'll run ripple detection on, using <code>RippleLFPSelection.set_lfp_electrodes</code></p> In\u00a0[2]: Copied! <pre>?sgr.RippleLFPSelection.set_lfp_electrodes\n</pre> ?sgr.RippleLFPSelection.set_lfp_electrodes <pre>Signature:\nsgrip.RippleLFPSelection.set_lfp_electrodes(\n    key,\n    electrode_list=None,\n    group_name='CA1',\n    **kwargs,\n)\nDocstring:\nRemoves all electrodes for the specified nwb file and then\nadds back the electrodes in the list\n\nParameters\n----------\nkey : dict\n    dictionary corresponding to the LFPBand entry to use for\n    ripple detection\nelectrode_list : list\n    list of electrodes from LFPBandSelection.LFPBandElectrode\n    to be used as the ripple LFP during detection\ngroup_name : str, optional\n    description of the electrode group, by default \"CA1\"\nFile:      ~/Src2/spyglass/src/spyglass/ripple/v1/ripple.py\nType:      function</pre> <p>We'll need the <code>nwb_file_name</code>, an <code>electrode_list</code>, and to a <code>group_name</code>.</p> <ul> <li>By default, <code>group_name</code> is set to CA1 for ripple detection, but we could alternatively use PFC.</li> <li>We use <code>nwb_file_name</code> to explore which electrodes are available for the <code>electrode_list</code>.</li> </ul> In\u00a0[3]: Copied! <pre>nwb_file_name = \"tonks20211103_.nwb\"\ninterval_list_name = \"test interval\"\nfilter_name = \"Ripple 150-250 Hz\"\n</pre> nwb_file_name = \"tonks20211103_.nwb\" interval_list_name = \"test interval\" filter_name = \"Ripple 150-250 Hz\" <p>Now we can look at <code>electrode_id</code> in the <code>Electrode</code> table:</p> In\u00a0[4]: Copied! <pre>electrodes = (\n    (sgc.Electrode() &amp; {\"nwb_file_name\": nwb_file_name})\n    * (\n        lfp_analysis.LFPBandSelection.LFPBandElectrode()\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"filter_name\": filter_name,\n            \"target_interval_list_name\": interval_list_name,\n        }\n    )\n    * sgc.BrainRegion\n).fetch(format=\"frame\")\nelectrodes\n</pre> electrodes = (     (sgc.Electrode() &amp; {\"nwb_file_name\": nwb_file_name})     * (         lfp_analysis.LFPBandSelection.LFPBandElectrode()         &amp; {             \"nwb_file_name\": nwb_file_name,             \"filter_name\": filter_name,             \"target_interval_list_name\": interval_list_name,         }     )     * sgc.BrainRegion ).fetch(format=\"frame\") electrodes Out[4]: probe_id probe_shank probe_electrode name original_reference_electrode x y z filtering impedance bad_channel x_warped y_warped z_warped contacts region_name subregion_name subsubregion_name nwb_file_name electrode_group_name electrode_id lfp_merge_id filter_name filter_sampling_rate target_interval_list_name lfp_band_sampling_rate lfp_electrode_group_name reference_elect_id region_id tonks20211103_.nwb 7 28 2f3c93d5-5d5d-2d47-75b3-c346dddbd312 Ripple 150-250 Hz 1000 test interval 100 CA1_test -1 19 tetrode_12.5 0 0 28 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None 1000 CA1_test -1 19 tetrode_12.5 0 0 28 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None 8 32 2f3c93d5-5d5d-2d47-75b3-c346dddbd312 Ripple 150-250 Hz 1000 test interval 100 CA1_test -1 19 tetrode_12.5 0 0 32 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None 1000 CA1_test -1 19 tetrode_12.5 0 0 32 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None <p>For ripple detection, we want only tetrodes, and only the first good wire on each tetrode. We will assume that is the first wire on each tetrode. I will do this using pandas syntax but you could use datajoint to filter this table as well. Here is the filtered table.</p> In\u00a0[5]: Copied! <pre>hpc_names = [\"ca1\", \"hippocampus\", \"CA1\", \"Hippocampus\"]\nelectrodes.loc[\n    (electrodes.region_name.isin(hpc_names)) &amp; (electrodes.probe_electrode == 0)\n]\n</pre> hpc_names = [\"ca1\", \"hippocampus\", \"CA1\", \"Hippocampus\"] electrodes.loc[     (electrodes.region_name.isin(hpc_names)) &amp; (electrodes.probe_electrode == 0) ] Out[5]: probe_id probe_shank probe_electrode name original_reference_electrode x y z filtering impedance bad_channel x_warped y_warped z_warped contacts region_name subregion_name subsubregion_name nwb_file_name electrode_group_name electrode_id lfp_merge_id filter_name filter_sampling_rate target_interval_list_name lfp_band_sampling_rate lfp_electrode_group_name reference_elect_id region_id tonks20211103_.nwb 7 28 2f3c93d5-5d5d-2d47-75b3-c346dddbd312 Ripple 150-250 Hz 1000 test interval 100 CA1_test -1 19 tetrode_12.5 0 0 28 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None 1000 CA1_test -1 19 tetrode_12.5 0 0 28 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None 8 32 2f3c93d5-5d5d-2d47-75b3-c346dddbd312 Ripple 150-250 Hz 1000 test interval 100 CA1_test -1 19 tetrode_12.5 0 0 32 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None 1000 CA1_test -1 19 tetrode_12.5 0 0 32 44 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 ca1 None None <p>We only want the electrode_id to put in the <code>electrode_list</code>:</p> In\u00a0[13]: Copied! <pre>electrode_list = np.unique(\n    (\n        electrodes.loc[\n            (electrodes.region_name.isin(hpc_names))\n            &amp; (electrodes.probe_electrode == 0)\n        ]\n        .reset_index()\n        .electrode_id\n    ).tolist()\n)\n\nelectrode_list.sort()\n</pre> electrode_list = np.unique(     (         electrodes.loc[             (electrodes.region_name.isin(hpc_names))             &amp; (electrodes.probe_electrode == 0)         ]         .reset_index()         .electrode_id     ).tolist() )  electrode_list.sort() <p>By default, <code>set_lfp_electrodes</code> will use all the available electrodes from <code>LFPBandV1</code>.</p> <p>We can insert into <code>RippleLFPSelection</code> and the <code>RippleLFPElectrode</code> part table, passing the key for the entry from <code>LFPBandV1</code>, our <code>electrode_list</code>, and the <code>group_name</code> into <code>set_lfp_electrodes</code></p> In\u00a0[18]: Copied! <pre>group_name = \"CA1_test\"\n\nlfp_band_key = (\n    lfp_analysis.LFPBandV1()\n    &amp; {\"filter_name\": filter_name, \"nwb_file_name\": nwb_file_name}\n).fetch1(\"KEY\")\n\nsgr.RippleLFPSelection.set_lfp_electrodes(\n    lfp_band_key,\n    electrode_list=electrode_list,\n    group_name=group_name,\n)\n</pre> group_name = \"CA1_test\"  lfp_band_key = (     lfp_analysis.LFPBandV1()     &amp; {\"filter_name\": filter_name, \"nwb_file_name\": nwb_file_name} ).fetch1(\"KEY\")  sgr.RippleLFPSelection.set_lfp_electrodes(     lfp_band_key,     electrode_list=electrode_list,     group_name=group_name, ) In\u00a0[20]: Copied! <pre>sgr.RippleLFPSelection.RippleLFPElectrode()\n</pre> sgr.RippleLFPSelection.RippleLFPElectrode() Out[20]: <p>lfp_merge_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>group_name</p> <p>lfp_electrode_group_name</p> the name of this group of electrodes <p>electrode_group_name</p> electrode group name from NWBFile <p>electrode_id</p> the unique number for this electrode <p>reference_elect_id</p> the reference electrode to use; -1 for no reference 2f3c93d5-5d5d-2d47-75b3-c346dddbd312 Ripple 150-250 Hz 1000 tonks20211103_.nwb test interval 100 CA1_test CA1_test 7 28 -12f3c93d5-5d5d-2d47-75b3-c346dddbd312 Ripple 150-250 Hz 1000 tonks20211103_.nwb test interval 100 CA1_test CA1_test 8 32 -1 <p>Total: 2</p> <p>Here's the ripple selection key we'll use downstream</p> In\u00a0[28]: Copied! <pre>rip_sel_key = (sgrip.RippleLFPSelection &amp; lfp_band_key).fetch1(\"KEY\")\n</pre> rip_sel_key = (sgrip.RippleLFPSelection &amp; lfp_band_key).fetch1(\"KEY\") In\u00a0[21]: Copied! <pre>sgr.RippleParameters()\n</pre> sgr.RippleParameters() Out[21]: <p>ripple_param_name</p> a name for this set of parameters <p>ripple_param_dict</p> dictionary of parameters default =BLOB= <p>Total: 1</p> <p>Here are the default ripple parameters:</p> In\u00a0[22]: Copied! <pre>(sgrip.RippleParameters() &amp; {\"ripple_param_name\": \"default\"}).fetch1()\n</pre> (sgrip.RippleParameters() &amp; {\"ripple_param_name\": \"default\"}).fetch1() Out[22]: <pre>{'ripple_param_name': 'default',\n 'ripple_param_dict': {'speed_name': 'head_speed',\n  'ripple_detection_algorithm': 'Kay_ripple_detector',\n  'ripple_detection_params': {'speed_threshold': 4.0,\n   'minimum_duration': 0.015,\n   'zscore_threshold': 2.0,\n   'smoothing_sigma': 0.004,\n   'close_ripple_threshold': 0.0}}}</pre> <ul> <li><code>filter_name</code>: which bandpass filter is used</li> <li><code>speed_name</code>: the name of the speed parameters in <code>IntervalPositionInfo</code></li> </ul> <p>For the <code>Kay_ripple_detector</code> (options are currently Kay and Karlsson, see <code>ripple_detection</code> package for specifics) the parameters are:</p> <ul> <li><code>speed_threshold</code> (cm/s): maximum speed the animal can move</li> <li><code>minimum_duration</code> (s): minimum time above threshold</li> <li><code>zscore_threshold</code> (std): minimum value to be considered a ripple, in standard deviations from mean</li> <li><code>smoothing_sigma</code> (s): how much to smooth the signal in time</li> <li><code>close_ripple_threshold</code> (s): exclude ripples closer than this amount</li> </ul> In\u00a0[27]: Copied! <pre>pos_key = sgp.PositionOutput.merge_get_part(\n    {\n        \"nwb_file_name\": nwb_file_name,\n        \"position_info_param_name\": \"default\",\n        \"interval_list_name\": \"pos 1 valid times\",\n    }\n).fetch1(\"KEY\")\n(sgp.PositionOutput &amp; pos_key).fetch1_dataframe()\n</pre> pos_key = sgp.PositionOutput.merge_get_part(     {         \"nwb_file_name\": nwb_file_name,         \"position_info_param_name\": \"default\",         \"interval_list_name\": \"pos 1 valid times\",     } ).fetch1(\"KEY\") (sgp.PositionOutput &amp; pos_key).fetch1_dataframe() Out[27]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.635961e+09 98.670000 78.320000 1.878849 -0.212384 -1.050933e+00 1.072179 1.635961e+09 98.615000 78.210000 1.899349 -0.143244 -1.136351e+00 1.145344 1.635961e+09 98.633333 78.173333 1.919567 -0.031501 -1.123425e+00 1.123867 1.635961e+09 98.596667 78.100000 1.932884 0.094982 -1.013202e+00 1.017644 1.635961e+09 98.633333 78.100000 1.946067 0.194273 -8.272934e-01 0.849798 ... ... ... ... ... ... ... 1.635963e+09 96.323333 71.500000 -2.265535 -0.415082 -1.486577e-05 0.415082 1.635963e+09 96.286667 71.500000 -2.158799 -0.413708 -3.243187e-06 0.413708 1.635963e+09 96.250000 71.500000 -2.034444 -0.374655 -6.383825e-07 0.374655 1.635963e+09 96.250000 71.500000 -2.034444 -0.307793 -1.133319e-07 0.307793 1.635963e+09 96.250000 71.500000 -2.034444 -0.229237 -1.813955e-08 0.229237 <p>48950 rows \u00d7 6 columns</p> <p>We'll use the <code>head_speed</code> above as part of <code>RippleParameters</code>.</p> <p>Now we can put everything together.</p> In\u00a0[32]: Copied! <pre>key = {\n    \"ripple_param_name\": \"default\",\n    **rip_sel_key,\n    \"pos_merge_id\": pos_key[\"merge_id\"],\n}\nsgrip.RippleTimesV1().populate(key)\n</pre> key = {     \"ripple_param_name\": \"default\",     **rip_sel_key,     \"pos_merge_id\": pos_key[\"merge_id\"], } sgrip.RippleTimesV1().populate(key) <pre>Computing ripple times for: {'lfp_merge_id': UUID('2f3c93d5-5d5d-2d47-75b3-c346dddbd312'), 'filter_name': 'Ripple 150-250 Hz', 'filter_sampling_rate': 1000, 'nwb_file_name': 'tonks20211103_.nwb', 'target_interval_list_name': 'test interval', 'lfp_band_sampling_rate': 100, 'group_name': 'CA1_test', 'ripple_param_name': 'default', 'pos_merge_id': UUID('68959dc8-f8a3-c3c0-a534-096b3bc10f6c')}\n</pre> <pre>&lt;__array_function__ internals&gt;:200: RuntimeWarning: invalid value encountered in cast\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/ripple_detection/detectors.py:31: RuntimeWarning: invalid value encountered in sqrt\n  return np.sqrt(ripple_consensus_trace)\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.1 because version 1.6.0 is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.6.0-alpha is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.2.0 because version 0.3.0 is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/pynwb/behavior.py:46: UserWarning: SpatialSeries 'series_0' has data shape (66792, 4) which is not compliant with NWB 2.5 and greater. The second dimension should have length &lt;= 3 to represent at most x, y, z.\n  warnings.warn(\"SpatialSeries '%s' has data shape %s which is not compliant with NWB 2.5 and greater. \"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/pynwb/behavior.py:46: UserWarning: SpatialSeries 'series_1' has data shape (48950, 4) which is not compliant with NWB 2.5 and greater. The second dimension should have length &lt;= 3 to represent at most x, y, z.\n  warnings.warn(\"SpatialSeries '%s' has data shape %s which is not compliant with NWB 2.5 and greater. \"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/pynwb/behavior.py:46: UserWarning: SpatialSeries 'series_2' has data shape (98507, 4) which is not compliant with NWB 2.5 and greater. The second dimension should have length &lt;= 3 to represent at most x, y, z.\n  warnings.warn(\"SpatialSeries '%s' has data shape %s which is not compliant with NWB 2.5 and greater. \"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/pynwb/behavior.py:46: UserWarning: SpatialSeries 'series_3' has data shape (44892, 4) which is not compliant with NWB 2.5 and greater. The second dimension should have length &lt;= 3 to represent at most x, y, z.\n  warnings.warn(\"SpatialSeries '%s' has data shape %s which is not compliant with NWB 2.5 and greater. \"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/pynwb/behavior.py:46: UserWarning: SpatialSeries 'series_4' has data shape (82313, 4) which is not compliant with NWB 2.5 and greater. The second dimension should have length &lt;= 3 to represent at most x, y, z.\n  warnings.warn(\"SpatialSeries '%s' has data shape %s which is not compliant with NWB 2.5 and greater. \"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/pynwb/behavior.py:46: UserWarning: SpatialSeries 'series_5' has data shape (81566, 4) which is not compliant with NWB 2.5 and greater. The second dimension should have length &lt;= 3 to represent at most x, y, z.\n  warnings.warn(\"SpatialSeries '%s' has data shape %s which is not compliant with NWB 2.5 and greater. \"\n/home/dgramling/anaconda3/envs/spyglass-position3/lib/python3.9/site-packages/pynwb/behavior.py:46: UserWarning: SpatialSeries 'series_6' has data shape (83811, 4) which is not compliant with NWB 2.5 and greater. The second dimension should have length &lt;= 3 to represent at most x, y, z.\n  warnings.warn(\"SpatialSeries '%s' has data shape %s which is not compliant with NWB 2.5 and greater. \"\n</pre> <pre>Writing new NWB file tonks20211103_6VF2MQ65RR.nwb\n</pre> <pre>/home/dgramling/Src2/datajoint-python/datajoint/external.py:276: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty.\n  if check_hash:\n</pre> <p>And then <code>fetch1_dataframe</code> for ripple times</p> In\u00a0[33]: Copied! <pre>ripple_times = (sgrip.RippleTimesV1() &amp; key).fetch1_dataframe()\nripple_times\n</pre> ripple_times = (sgrip.RippleTimesV1() &amp; key).fetch1_dataframe() ripple_times Out[33]: start_time end_time id 0 1.635961e+09 1.635961e+09 1 1.635961e+09 1.635961e+09 2 1.635961e+09 1.635961e+09 3 1.635961e+09 1.635961e+09 4 1.635961e+09 1.635961e+09 5 1.635961e+09 1.635961e+09 6 1.635961e+09 1.635961e+09 7 1.635961e+09 1.635961e+09 8 1.635961e+09 1.635961e+09 9 1.635961e+09 1.635961e+09 10 1.635961e+09 1.635961e+09 11 1.635961e+09 1.635961e+09 12 1.635961e+09 1.635961e+09 13 1.635961e+09 1.635961e+09 14 1.635961e+09 1.635961e+09 15 1.635961e+09 1.635961e+09 16 1.635961e+09 1.635961e+09"}, {"location": "notebooks/30_Ripple_Detection/#ripple-detection", "title": "Ripple Detection\u00b6", "text": ""}, {"location": "notebooks/30_Ripple_Detection/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/30_Ripple_Detection/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/30_Ripple_Detection/#selecting-electrodes", "title": "Selecting Electrodes\u00b6", "text": ""}, {"location": "notebooks/30_Ripple_Detection/#setting-ripple-parameters", "title": "Setting Ripple Parameters\u00b6", "text": ""}, {"location": "notebooks/30_Ripple_Detection/#check-interval-speed", "title": "Check interval speed\u00b6", "text": "<p>The speed for this interval should exist under the default position parameter set and for a given interval.</p>"}, {"location": "notebooks/30_Ripple_Detection/#run-ripple-detection", "title": "Run Ripple Detection\u00b6", "text": ""}, {"location": "notebooks/30_Ripple_Detection/#up-next", "title": "Up Next\u00b6", "text": "<p>Next, we'll extract mark indicator.</p>"}, {"location": "notebooks/31_Extract_Mark_Indicators/", "title": "Extract Mark Indicators", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on clusterless decoding in Spyglass</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>For additional info on DataJoint syntax, including table definitions and inserts, see the Insert Data notebook</li> <li>Prior to running, please familiarize yourself with the spike sorting pipeline and generate input position data with either the Trodes or DLC notebooks (1, 2, 3).</li> </ul> <p>The goal of this notebook is to populate the <code>UnitMarksIndicator</code> table, which depends on a series of tables in the spike sorting pipeline:</p> <ul> <li><code>SpikeSorting</code> -&gt; <code>CuratedSpikeSorting</code> -&gt; <code>UnitMarks</code> -&gt; <code>UnitMarkIndicators</code></li> </ul> <p>While clusterless decoding avoids actual spike sorting and curation, we need to pass through these tables to maintain (relative) pipeline simplicity. Pass-through tables keep spike sorting and clusterless mark extraction as similar as possible, by using shared steps. Here, \"spike sorting\" involves simple thresholding (sorter: clusterless_thresholder). The  <code>populate_mark_indicators</code> will run each of these steps provided we have data in  <code>SpikeSortingSelection</code> and <code>IntervalPositionInfo</code>.</p> <p><code>SpikeSortingSelection</code> depends on:</p> <ul> <li><code>SpikeSortingRecording</code></li> <li><code>SpikeSorterParameters</code></li> <li><code>ArtifactRemovedIntervalList</code>.</li> </ul> <p><code>SpikeSortingRecording</code> depends on:</p> <ul> <li><code>SortGroup</code></li> <li><code>SortInterval</code></li> <li><code>SpikeSortingPreprocessingParameters</code></li> <li><code>LabTeam</code></li> </ul> In\u00a0[2]: Copied! <pre>import os\nimport datajoint as dj\nfrom pprint import pprint\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.spikesorting as sgs\nimport spyglass.decoding as sgd\nimport spyglass.utils as sgu\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\nwarnings.simplefilter(\"ignore\", category=UserWarning)\n</pre> import os import datajoint as dj from pprint import pprint  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.spikesorting as sgs import spyglass.decoding as sgd import spyglass.utils as sgu  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) warnings.simplefilter(\"ignore\", category=UserWarning) <pre>[2023-08-02 18:58:19,324][INFO]: Connecting root@localhost:3306\n[2023-08-02 18:58:19,360][INFO]: Connected root@localhost:3306\n/home/cb/miniconda3/envs/spy/lib/python3.9/site-packages/spikeinterface/sortingcomponents/peak_detection.py:643: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit(parallel=False)\n/home/cb/miniconda3/envs/spy/lib/python3.9/site-packages/spikeinterface/sortingcomponents/peak_detection.py:668: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @numba.jit(parallel=False)\nCupy is not installed or GPU is not detected. Ignore this message if not using GPU\n</pre> In\u00a0[\u00a0]: Copied! <pre>nwb_file_name = \"J1620210531.nwb\"\nnwb_copy_file_name = sgu.nwb_helper_fn.get_nwb_copy_filename(nwb_file_name)\n</pre> nwb_file_name = \"J1620210531.nwb\" nwb_copy_file_name = sgu.nwb_helper_fn.get_nwb_copy_filename(nwb_file_name) <p>We can investigate the <code>populate_mark_indicators</code> function we want to use with <code>?</code>:</p> In\u00a0[2]: Copied! <pre>?sgd.clusterless.populate_mark_indicators\n</pre> ?sgd.clusterless.populate_mark_indicators <pre>/home/edeno/miniconda3/envs/spyglass/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(mpl.__version__) &gt;= \"3.0\":\n/home/edeno/miniconda3/envs/spyglass/lib/python3.8/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n</pre> <pre>Connecting edeno@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>Signature:\npopulate_mark_indicators(\n    spikesorting_selection_keys: list,\n    mark_param_name='default',\n    position_info_param_name='default_decoding',\n)\nDocstring: &lt;no docstring&gt;\nFile:      /stelmo/edeno/nwb_datajoint/src/spyglass/decoding/clusterless.py\nType:      function\n</pre> <p>From the docstring, we see that we need to supply <code>spikesorting_selection_keys</code>, which is a list of <code>SpikeSorting</code> keys as dictionaries. We provide a list to extract marks for many electrode at once.</p> <p>Here are the primary keys required by <code>SpikeSorting</code>:</p> In\u00a0[3]: Copied! <pre>sgs.SpikeSorting.primary_key\n</pre> sgs.SpikeSorting.primary_key Out[3]: <pre>['nwb_file_name',\n 'sort_group_id',\n 'sort_interval_name',\n 'preproc_params_name',\n 'team_name',\n 'sorter',\n 'sorter_params_name',\n 'artifact_removed_interval_list_name']</pre> <p>Here is an example of what <code>spikesorting_selection_keys</code> should look like:</p> In\u00a0[4]: Copied! <pre>spikesorting_selections = [\n    {\n        \"nwb_file_name\": \"J1620210531_.nwb\",\n        \"sort_group_id\": 0,\n        \"sort_interval_name\": \"raw data valid times no premaze no home\",\n        \"preproc_params_name\": \"franklab_tetrode_hippocampus\",\n        \"team_name\": \"JG_DG\",\n        \"sorter\": \"clusterless_thresholder\",\n        \"sorter_params_name\": \"clusterless_fixed\",\n        \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",\n    },\n    {\n        \"nwb_file_name\": \"J1620210531_.nwb\",\n        \"sort_group_id\": 1,\n        \"sort_interval_name\": \"raw data valid times no premaze no home\",\n        \"preproc_params_name\": \"franklab_tetrode_hippocampus\",\n        \"team_name\": \"JG_DG\",\n        \"sorter\": \"clusterless_thresholder\",\n        \"sorter_params_name\": \"clusterless_fixed\",\n        \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_1_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",\n    },\n]\n</pre> spikesorting_selections = [     {         \"nwb_file_name\": \"J1620210531_.nwb\",         \"sort_group_id\": 0,         \"sort_interval_name\": \"raw data valid times no premaze no home\",         \"preproc_params_name\": \"franklab_tetrode_hippocampus\",         \"team_name\": \"JG_DG\",         \"sorter\": \"clusterless_thresholder\",         \"sorter_params_name\": \"clusterless_fixed\",         \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",     },     {         \"nwb_file_name\": \"J1620210531_.nwb\",         \"sort_group_id\": 1,         \"sort_interval_name\": \"raw data valid times no premaze no home\",         \"preproc_params_name\": \"franklab_tetrode_hippocampus\",         \"team_name\": \"JG_DG\",         \"sorter\": \"clusterless_thresholder\",         \"sorter_params_name\": \"clusterless_fixed\",         \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_1_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",     }, ] <p>WARNING: This process relies on both <code>SpikeSortingSelection</code> and <code>IntervalPositionInfo</code>. You can check your database with the following:</p> In\u00a0[5]: Copied! <pre>sgs.SpikeSortingSelection &amp; {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"sorter\": \"clusterless_thresholder\",\n}\n</pre> sgs.SpikeSortingSelection &amp; {     \"nwb_file_name\": nwb_copy_file_name,     \"sorter\": \"clusterless_thresholder\", } Out[5]: Table for holding selection of recording and parameters for each spike sorting run <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>import_path</p> optional path to previous curated sorting output J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times J1620210531_.nwb 1 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_1_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times <p>Total: 2</p> <p>Remember to replace the nwb_file_name with your own nwb file name</p> In\u00a0[6]: Copied! <pre>sgc.IntervalPositionInfo &amp; {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"position_info_param_name\": \"default_decoding\",\n}\n</pre> sgc.IntervalPositionInfo &amp; {     \"nwb_file_name\": nwb_copy_file_name,     \"position_info_param_name\": \"default_decoding\", } Out[6]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>analysis_file_name</p> name of the file <p>head_position_object_id</p> <p>head_orientation_object_id</p> <p>head_velocity_object_id</p> default_decoding J1620210531_.nwb pos 0 valid times J1620210531_XOKZ4G53LE.nwb 200c54a9-57dc-4ed4-8e48-85cc9387b922 26f4ec71-2f5f-4ef2-82f1-ee3a89048168 7f9ffe2f-31e5-47ac-bad9-558440cd5f1ddefault_decoding J1620210531_.nwb pos 1 valid times J1620210531_B257TS35XJ.nwb c164cdfd-c328-49b2-b6b4-f8663a17f5d6 e1f26617-c44b-4464-942d-5e6755606789 f3c69ac6-5f0a-4622-827b-44a769ce4f77default_decoding J1620210531_.nwb pos 10 valid times J1620210531_1RICS57YYG.nwb 2394ec2d-c076-4b11-a40b-91e4b51b81fa 005080ee-b93f-4b3c-bcd0-595b46fedab2 47158c4f-3632-4ce1-9f1e-043f1ea5a5ecdefault_decoding J1620210531_.nwb pos 11 valid times J1620210531_HZSI1BGTE7.nwb 295aa187-81cd-4209-b96d-163f12a716ed 19a95d85-8cb9-4e06-82ee-26bc6df7c8c8 9abb39ce-f0e2-4637-b9e9-5f7b6891201ddefault_decoding J1620210531_.nwb pos 12 valid times J1620210531_M0UMC7TZ49.nwb ae4c015c-9d5f-4ca3-bbcd-049c1e1ae50b 1bbc244c-4d85-4e63-9d69-10815e37276c be8f6ba5-3cc9-4ac3-96af-5c3e84b9851cdefault_decoding J1620210531_.nwb pos 13 valid times J1620210531_TREA1SOT03.nwb c4bfe562-f523-495c-99e5-681524661264 65023b5e-321b-4d6e-b975-ec03cbd46247 ce391291-6107-420a-a73d-9a18d343a47adefault_decoding J1620210531_.nwb pos 14 valid times J1620210531_93PXV7F4CX.nwb 8521f813-635d-41cd-8f10-b00f26a7f957 75d08124-57f7-481b-ae44-0752a5912c18 84ce4054-8c11-49f2-8295-7d8173cb7eb7default_decoding J1620210531_.nwb pos 15 valid times J1620210531_FUJ1B1OKOA.nwb e72e5fcd-3d80-463a-a9d1-305e6e99f8db 6e8dc0e2-ab02-4305-bf2e-e2d208bae77a b7589719-49ba-48e7-a20b-88a911c3f8a5default_decoding J1620210531_.nwb pos 16 valid times J1620210531_76HL0SNP59.nwb b3124712-55db-4b0b-9bdc-77f03a0b8c98 08bfd82c-1e7c-4aa7-bd9e-256577b26e9c 85ffd435-3dc4-49ab-adea-a3327817a51ddefault_decoding J1620210531_.nwb pos 17 valid times J1620210531_5F8VRT0OC4.nwb 78b8f7e4-fb90-438d-8551-d15a4d97484a 7b96d15c-9721-44ea-9ecd-d8a2b60711d6 12336768-227e-4c0c-9ade-b3c7ac6c2de3default_decoding J1620210531_.nwb pos 18 valid times J1620210531_PP7XK1G26S.nwb 39ab3238-a0dd-4906-95c5-8e534458e371 ca0f939d-f0a7-4df4-9946-2f605ef5ab76 571277d0-885e-4d0e-b802-6ad8047ee3f1default_decoding J1620210531_.nwb pos 19 valid times J1620210531_AQZHSP2IDM.nwb b5a21053-3239-4943-aee1-3227b4915d5f 9f0c61c8-90ed-4a74-873c-e250aa872a2a 714f6ceb-6d70-474c-8c42-6f9fa3ff77e1 <p>...</p> <p>Total: 21</p> <p>Now that we've checked those, we can run the function:</p> In\u00a0[7]: Copied! <pre>sgd.clusterless.populate_mark_indicators(spikesorting_selections)\n</pre> sgd.clusterless.populate_mark_indicators(spikesorting_selections) <p>We can verify that this worked:</p> In\u00a0[8]: Copied! <pre>plt.plot(position_info.head_position_x, position_info.head_position_y)\n</pre> plt.plot(position_info.head_position_x, position_info.head_position_y) Out[8]: <p>curation_id</p> a number correponding to the index of this curation <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>mark_param_name</p> a name for this set of parameters <p>interval_list_name</p> descriptive name of this interval list <p>sampling_rate</p> <p>analysis_file_name</p> name of the file <p>marks_indicator_object_id</p> 0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 0 valid times 500.0 J1620210531_NS19JB65RU.nwb d0de79cc-f2be-4e86-b4e8-e58a8aef1c020 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 1 valid times 500.0 J1620210531_YP9MXAPE2G.nwb 7598dc69-a024-4140-8b37-d1342a0e78290 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 10 valid times 500.0 J1620210531_AGOI7DXRQX.nwb 12a60beb-3ac2-41ae-a091-eebd0b14672c0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 11 valid times 500.0 J1620210531_VXGQCG8UYO.nwb d0d40c96-e5a1-494f-b8a7-bf475304d6400 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 12 valid times 500.0 J1620210531_NF8RWZGCSG.nwb 2b6476a7-143f-4760-bafa-e4959f0e735a0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 13 valid times 500.0 J1620210531_F7OHK7UMO3.nwb 84aea599-dbf1-4d11-bca2-00aa6ae9967a0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 14 valid times 500.0 J1620210531_MPXHIH43Y1.nwb 78224e49-0118-458d-9aee-727db98fc3060 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 15 valid times 500.0 J1620210531_YFG1894358.nwb 721526f8-df6d-442e-84ee-bf7dd6ff8c190 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 16 valid times 500.0 J1620210531_VJ5V0LRJKU.nwb 39258f97-4508-4068-9b2b-ce419a5d718d0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 17 valid times 500.0 J1620210531_NACXLH30KV.nwb bf312f88-af31-4247-83dc-9633d191401a0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 18 valid times 500.0 J1620210531_LFKSWYVVRS.nwb af89e3e1-71f7-4c7c-a5ef-94d3cc7a12030 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 19 valid times 500.0 J1620210531_R9V8JD4IQA.nwb 3616a7b7-fca2-43bb-9c72-44eb3c22af80 <p>...</p> <p>Total: 42</p> In\u00a0[9]: Copied! <pre>position_info.shape, marks.shape\n</pre> position_info.shape, marks.shape Out[9]: <pre>((655645, 6), (655645, 4, 22))</pre> In\u00a0[10]: Copied! <pre>from spyglass.common.common_interval import interval_list_intersect\nfrom spyglass.common import IntervalList\n\nkey = {}\nkey[\"interval_list_name\"] = \"02_r1\"\nkey[\"nwb_file_name\"] = nwb_copy_file_name\n\ninterval = (\n    IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": key[\"interval_list_name\"],\n    }\n).fetch1(\"valid_times\")\n\nvalid_ephys_times = (\n    IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": \"raw data valid times\",\n    }\n).fetch1(\"valid_times\")\nposition_interval_names = (\n    IntervalPositionInfo\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"position_info_param_name\": \"default_decoding\",\n    }\n).fetch(\"interval_list_name\")\nvalid_pos_times = [\n    (\n        IntervalList\n        &amp; {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": pos_interval_name,\n        }\n    ).fetch1(\"valid_times\")\n    for pos_interval_name in position_interval_names\n]\n\nintersect_interval = interval_list_intersect(\n    interval_list_intersect(interval, valid_ephys_times), valid_pos_times[0]\n)\nvalid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1])\nvalid_time_slice\n</pre> from spyglass.common.common_interval import interval_list_intersect from spyglass.common import IntervalList  key = {} key[\"interval_list_name\"] = \"02_r1\" key[\"nwb_file_name\"] = nwb_copy_file_name  interval = (     IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": key[\"interval_list_name\"],     } ).fetch1(\"valid_times\")  valid_ephys_times = (     IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": \"raw data valid times\",     } ).fetch1(\"valid_times\") position_interval_names = (     IntervalPositionInfo     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"position_info_param_name\": \"default_decoding\",     } ).fetch(\"interval_list_name\") valid_pos_times = [     (         IntervalList         &amp; {             \"nwb_file_name\": key[\"nwb_file_name\"],             \"interval_list_name\": pos_interval_name,         }     ).fetch1(\"valid_times\")     for pos_interval_name in position_interval_names ]  intersect_interval = interval_list_intersect(     interval_list_intersect(interval, valid_ephys_times), valid_pos_times[0] ) valid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1]) valid_time_slice Out[10]: <pre>slice(1581886916.3153033, 1581888227.5987928, None)</pre> In\u00a0[\u00a0]: Copied! <pre>from replay_trajectory_classification import ClusterlessClassifier\nfrom replay_trajectory_classification.environments import Environment\nfrom replay_trajectory_classification.continuous_state_transitions import (\n    RandomWalk,\n    Uniform,\n)\nfrom spyglass.decoding.clusterless import ClusterlessClassifierParameters\n\nmarks = marks.sel(time=valid_time_slice)\nposition_info = position_info.loc[valid_time_slice]\n\nparameters = (\n    ClusterlessClassifierParameters()\n    &amp; {\"classifier_param_name\": \"default_decoding_gpu\"}\n).fetch1()\n\nparameters[\"classifier_params\"][\"clusterless_algorithm_params\"] = {\n    \"mark_std\": 24.0,\n    \"position_std\": 3.0,\n    \"block_size\": int(2**13),\n    \"disable_progress_bar\": False,\n    \"use_diffusion\": False,\n}\nparameters[\"classifier_params\"][\"environments\"][0] = Environment(\n    place_bin_size=3.0\n)\n\n\nimport cupy as cp\n\nwith cp.cuda.Device(0):\n    classifier = ClusterlessClassifier(**parameters[\"classifier_params\"])\n    classifier.fit(\n        position=position_info[[\"head_position_x\", \"head_position_y\"]].values,\n        multiunits=marks.values,\n        **parameters[\"fit_params\"],\n    )\n    results = classifier.predict(\n        multiunits=marks.values,\n        time=position_info.index,\n        **parameters[\"predict_params\"],\n    )\n    logging.info(\"Done!\")\n</pre> from replay_trajectory_classification import ClusterlessClassifier from replay_trajectory_classification.environments import Environment from replay_trajectory_classification.continuous_state_transitions import (     RandomWalk,     Uniform, ) from spyglass.decoding.clusterless import ClusterlessClassifierParameters  marks = marks.sel(time=valid_time_slice) position_info = position_info.loc[valid_time_slice]  parameters = (     ClusterlessClassifierParameters()     &amp; {\"classifier_param_name\": \"default_decoding_gpu\"} ).fetch1()  parameters[\"classifier_params\"][\"clusterless_algorithm_params\"] = {     \"mark_std\": 24.0,     \"position_std\": 3.0,     \"block_size\": int(2**13),     \"disable_progress_bar\": False,     \"use_diffusion\": False, } parameters[\"classifier_params\"][\"environments\"][0] = Environment(     place_bin_size=3.0 )   import cupy as cp  with cp.cuda.Device(0):     classifier = ClusterlessClassifier(**parameters[\"classifier_params\"])     classifier.fit(         position=position_info[[\"head_position_x\", \"head_position_y\"]].values,         multiunits=marks.values,         **parameters[\"fit_params\"],     )     results = classifier.predict(         multiunits=marks.values,         time=position_info.index,         **parameters[\"predict_params\"],     )     logging.info(\"Done!\") In\u00a0[16]: Copied! <pre>from spyglass.decoding.visualization import (\n    create_interactive_2D_decoding_figurl,\n)\n\n\nview = create_interactive_2D_decoding_figurl(\n    position_info,\n    marks,\n    results,\n    classifier.environments[0].place_bin_size,\n    position_name=[\"head_position_x\", \"head_position_y\"],\n    head_direction_name=\"head_orientation\",\n    speed_name=\"head_speed\",\n    posterior_type=\"acausal_posterior\",\n    sampling_frequency=500,\n    view_height=800,\n)\n</pre> from spyglass.decoding.visualization import (     create_interactive_2D_decoding_figurl, )   view = create_interactive_2D_decoding_figurl(     position_info,     marks,     results,     classifier.environments[0].place_bin_size,     position_name=[\"head_position_x\", \"head_position_y\"],     head_direction_name=\"head_orientation\",     speed_name=\"head_speed\",     posterior_type=\"acausal_posterior\",     sampling_frequency=500,     view_height=800, )"}, {"location": "notebooks/31_Extract_Mark_Indicators/#mark-indicators", "title": "Mark Indicators\u00b6", "text": ""}, {"location": "notebooks/31_Extract_Mark_Indicators/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/31_Extract_Mark_Indicators/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/31_Extract_Mark_Indicators/#select-data", "title": "Select Data\u00b6", "text": ""}, {"location": "notebooks/31_Extract_Mark_Indicators/#spike-sorting-selection", "title": "Spike Sorting Selection\u00b6", "text": ""}, {"location": "notebooks/31_Extract_Mark_Indicators/#populate-mark-indicators", "title": "Populate Mark Indicators\u00b6", "text": ""}, {"location": "notebooks/31_Extract_Mark_Indicators/#up-next", "title": "Up Next\u00b6", "text": "<p>Next, we'll start the process of decoding representations of position with ephys data. This can be done either with GPUs or clusterless.</p>"}, {"location": "notebooks/32_Decoding_with_GPUs/", "title": "Decoding with GPUs", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on decoding in Spyglass. To set up your Spyglass environment and database, see the Setup notebook.</p> <p>In this tutorial, we'll set up GPU access for subsequent decoding analyses. While this notebook doesn't have any direct prerequisites, you will need Spike Sorting data for the next step.</p> In\u00a0[\u00a0]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Sat Jun  4 09:37:07 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100 80G...  On   | 00000000:4F:00.0 Off |                    0 |\n| N/A   30C    P0    42W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A100 80G...  On   | 00000000:52:00.0 Off |                    0 |\n| N/A   32C    P0    43W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A100 80G...  On   | 00000000:53:00.0 Off |                    0 |\n| N/A   32C    P0    62W / 300W |  32271MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   3  NVIDIA A100 80G...  On   | 00000000:56:00.0 Off |                    0 |\n| N/A   29C    P0    41W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   4  NVIDIA A100 80G...  On   | 00000000:57:00.0 Off |                    0 |\n| N/A   30C    P0    44W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   5  NVIDIA A100 80G...  On   | 00000000:CE:00.0 Off |                    0 |\n| N/A   32C    P0    64W / 300W |    855MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   6  NVIDIA A100 80G...  On   | 00000000:D1:00.0 Off |                    0 |\n| N/A   31C    P0    42W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   7  NVIDIA A100 80G...  On   | 00000000:D2:00.0 Off |                    0 |\n| N/A   29C    P0    43W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   8  NVIDIA A100 80G...  On   | 00000000:D5:00.0 Off |                    0 |\n| N/A   31C    P0    42W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   9  NVIDIA A100 80G...  On   | 00000000:D6:00.0 Off |                    0 |\n| N/A   32C    P0    44W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    1   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    2   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    2   N/A  N/A   3646915      C   .../envs/spyglass/bin/python    32199MiB |\n|    3   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    4   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    5   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    5   N/A  N/A   3645824      C   .../envs/spyglass/bin/python      817MiB |\n|    6   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    7   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    8   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    9   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n+-----------------------------------------------------------------------------+\n</pre> <p>We can monitor GPU use with the terminal command <code>watch -n 0.1 nvidia-smi</code>, will update <code>nvidia-smi</code> every 100 ms. This won't work in a notebook, as it won't display the updates.</p> <p>Other ways to monitor GPU usage are:</p> <ul> <li>A jupyter widget by nvidia to monitor GPU usage in the notebook</li> <li>A terminal program like nvidia-smi with more information about  which GPUs are being utilized and by whom.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import os\nimport datajoint as dj\n\nimport cupy as cp\nimport numpy as np\n\nimport dask\nimport dask_cuda\n\nimport replay_trajectory_classification as rtc\nfrom replay_trajectory_classification import (\n    sorted_spikes_simulation as rtc_spike_sim,\n    environment as rtc_env,\n    continuous_state_transitions as rts,\n)\n\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport logging\n\n# Set up logging message formatting\nlogging.basicConfig(\n    level=\"INFO\", format=\"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\"\n)\n</pre> import os import datajoint as dj  import cupy as cp import numpy as np  import dask import dask_cuda  import replay_trajectory_classification as rtc from replay_trajectory_classification import (     sorted_spikes_simulation as rtc_spike_sim,     environment as rtc_env,     continuous_state_transitions as rts, )   # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import logging  # Set up logging message formatting logging.basicConfig(     level=\"INFO\", format=\"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\" ) <pre>[2023-08-01 11:16:06,882][INFO]: Connecting root@localhost:3306\n[2023-08-01 11:16:06,909][INFO]: Connected root@localhost:3306\n</pre> <p>First, we'll simulate some data.</p> In\u00a0[\u00a0]: Copied! <pre>(\n    time,\n    position,\n    sampling_frequency,\n    spikes,\n    place_fields,\n) = rtc_spike_sim.make_simulated_run_data()\n\nreplay_time, test_spikes = rtc_spike_sim.make_continuous_replay()\n</pre> (     time,     position,     sampling_frequency,     spikes,     place_fields, ) = rtc_spike_sim.make_simulated_run_data()  replay_time, test_spikes = rtc_spike_sim.make_continuous_replay() In\u00a0[\u00a0]: Copied! <pre>movement_var = rts.estimate_movement_var(position, sampling_frequency)\n\nenvironment = rtc_env.Environment(place_bin_size=np.sqrt(movement_var))\n\ncontinuous_transition_types = [\n    [\n        rts.RandomWalk(movement_var=movement_var * 120),\n        rts.Uniform(),\n        rts.Identity(),\n    ],\n    [rts.Uniform(), rts.Uniform(), rts.Uniform()],\n    [\n        rts.RandomWalk(movement_var=movement_var * 120),\n        rts.Uniform(),\n        rts.Identity(),\n    ],\n]\n\nclassifier = rtc.SortedSpikesClassifier(\n    environments=environment,\n    continuous_transition_types=continuous_transition_types,\n    # specify GPU enabled algorithm for the likelihood\n    sorted_spikes_algorithm=\"spiking_likelihood_kde_gpu\",\n    sorted_spikes_algorithm_params={\"position_std\": 3.0},\n)\nstate_names = [\"continuous\", \"fragmented\", \"stationary\"]\n</pre> movement_var = rts.estimate_movement_var(position, sampling_frequency)  environment = rtc_env.Environment(place_bin_size=np.sqrt(movement_var))  continuous_transition_types = [     [         rts.RandomWalk(movement_var=movement_var * 120),         rts.Uniform(),         rts.Identity(),     ],     [rts.Uniform(), rts.Uniform(), rts.Uniform()],     [         rts.RandomWalk(movement_var=movement_var * 120),         rts.Uniform(),         rts.Identity(),     ], ]  classifier = rtc.SortedSpikesClassifier(     environments=environment,     continuous_transition_types=continuous_transition_types,     # specify GPU enabled algorithm for the likelihood     sorted_spikes_algorithm=\"spiking_likelihood_kde_gpu\",     sorted_spikes_algorithm_params={\"position_std\": 3.0}, ) state_names = [\"continuous\", \"fragmented\", \"stationary\"] <p>We can use a context manager to specify which GPU (device)</p> In\u00a0[\u00a0]: Copied! <pre>GPU_ID = 5  # Use GPU #6\n\nwith cp.cuda.Device(GPU_ID):\n    # Fit the model place fields\n    classifier.fit(position, spikes)\n\n    # Run the model on the simulated replay\n    results = classifier.predict(\n        test_spikes,\n        time=replay_time,\n        state_names=state_names,\n        use_gpu=True,  # Use GPU for computation of causal/acausal posterior\n    )\n</pre> GPU_ID = 5  # Use GPU #6  with cp.cuda.Device(GPU_ID):     # Fit the model place fields     classifier.fit(position, spikes)      # Run the model on the simulated replay     results = classifier.predict(         test_spikes,         time=replay_time,         state_names=state_names,         use_gpu=True,  # Use GPU for computation of causal/acausal posterior     ) In\u00a0[3]: Copied! <pre>cluster = dask_cuda.LocalCUDACluster(CUDA_VISIBLE_DEVICES=[4, 5, 6])\nclient = dask.distributed.Client(cluster)\n\nclient\n</pre> cluster = dask_cuda.LocalCUDACluster(CUDA_VISIBLE_DEVICES=[4, 5, 6]) client = dask.distributed.Client(cluster)  client <pre>2022-05-18 13:50:10,288 - distributed.diskutils - INFO - Found stale lock file and directory '/stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-ly7bpyy1', purging\n2022-05-18 13:50:10,296 - distributed.diskutils - INFO - Found stale lock file and directory '/stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-n6nteep3', purging\n2022-05-18 13:50:10,302 - distributed.diskutils - INFO - Found stale lock file and directory '/stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-okcse855', purging\n2022-05-18 13:50:10,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n2022-05-18 13:50:10,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n2022-05-18 13:50:10,319 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n</pre> Out[3]: Client <p>Client-1c4b4a29-d6ec-11ec-9abe-3cecef615bf0</p> Connection method: Cluster object Cluster type: dask_cuda.LocalCUDACluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCUDACluster <p>096db37c</p> Dashboard: http://127.0.0.1:8787/status Workers: 3                  Total threads: 3                  Total memory: 3.94 TiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-9287d258-c1ff-4d5a-8e3c-2a66ca163173</p> Comm: tcp://127.0.0.1:37725                      Workers: 3                      Dashboard: http://127.0.0.1:8787/status Total threads: 3                      Started: Just now                      Total memory: 3.94 TiB                      Workers Worker: 4 Comm:  tcp://127.0.0.1:35731                          Total threads:  1                          Dashboard:  http://127.0.0.1:46067/status Memory:  1.31 TiB                          Nanny:  tcp://127.0.0.1:34151                          Local directory:  /stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-n17o941r                          GPU: NVIDIA A100 80GB PCIe                          GPU memory:  80.00 GiB                          Worker: 5 Comm:  tcp://127.0.0.1:39549                          Total threads:  1                          Dashboard:  http://127.0.0.1:42725/status Memory:  1.31 TiB                          Nanny:  tcp://127.0.0.1:41769                          Local directory:  /stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-ppy7ls9e                          GPU: NVIDIA A100 80GB PCIe                          GPU memory:  80.00 GiB                          Worker: 6 Comm:  tcp://127.0.0.1:39335                          Total threads:  1                          Dashboard:  http://127.0.0.1:34189/status Memory:  1.31 TiB                          Nanny:  tcp://127.0.0.1:46373                          Local directory:  /stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-m7thw3ne                          GPU: NVIDIA A100 80GB PCIe                          GPU memory:  80.00 GiB                          <p>To use this client, we declare a function we want to run on each GPU with the <code>dask.delayed</code> decorator.</p> <p>In the example below, we run <code>test_gpu</code> on each item of <code>data</code> where each item is processed on a different GPU.</p> In\u00a0[4]: Copied! <pre>def setup_logger(name_logfile, path_logfile):\n    \"\"\"Sets up a logger for each function that outputs\n    to the console and to a file\"\"\"\n    logger = logging.getLogger(name_logfile)\n    formatter = logging.Formatter(\n        \"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\"\n    )\n    fileHandler = logging.FileHandler(path_logfile, mode=\"w\")\n    fileHandler.setFormatter(formatter)\n    streamHandler = logging.StreamHandler()\n    streamHandler.setFormatter(formatter)\n\n    logger.setLevel(logging.INFO)\n    logger.addHandler(fileHandler)\n    logger.addHandler(streamHandler)\n\n    return logger\n\n\n# This uses the dask.delayed decorator on the test_gpu function\n@dask.delayed\ndef test_gpu(x, ind):\n    # Create a log file for this run of the function\n    logger = setup_logger(\n        name_logfile=f\"test_{ind}\", path_logfile=f\"test_{ind}.log\"\n    )\n\n    # Test to see if these go into different log files\n    logger.info(f\"This is a test of {ind}\")\n    logger.info(\"This should be in a unique file\")\n\n    # Run a GPU computation\n    return cp.asnumpy(cp.mean(x[:, None] @ x[:, None].T, axis=0))\n\n\n# Make up 10 fake datasets\nx = cp.random.normal(size=10_000, dtype=cp.float32)\ndata = [x + i for i in range(10)]\n\n# Append the result of the computation into a results list\nresults = [test_gpu(x, ind) for ind, x in enumerate(data)]\n\n# Run `dask.compute` on the results list for the code to run\ndask.compute(*results)\n</pre> def setup_logger(name_logfile, path_logfile):     \"\"\"Sets up a logger for each function that outputs     to the console and to a file\"\"\"     logger = logging.getLogger(name_logfile)     formatter = logging.Formatter(         \"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\"     )     fileHandler = logging.FileHandler(path_logfile, mode=\"w\")     fileHandler.setFormatter(formatter)     streamHandler = logging.StreamHandler()     streamHandler.setFormatter(formatter)      logger.setLevel(logging.INFO)     logger.addHandler(fileHandler)     logger.addHandler(streamHandler)      return logger   # This uses the dask.delayed decorator on the test_gpu function @dask.delayed def test_gpu(x, ind):     # Create a log file for this run of the function     logger = setup_logger(         name_logfile=f\"test_{ind}\", path_logfile=f\"test_{ind}.log\"     )      # Test to see if these go into different log files     logger.info(f\"This is a test of {ind}\")     logger.info(\"This should be in a unique file\")      # Run a GPU computation     return cp.asnumpy(cp.mean(x[:, None] @ x[:, None].T, axis=0))   # Make up 10 fake datasets x = cp.random.normal(size=10_000, dtype=cp.float32) data = [x + i for i in range(10)]  # Append the result of the computation into a results list results = [test_gpu(x, ind) for ind, x in enumerate(data)]  # Run `dask.compute` on the results list for the code to run dask.compute(*results) <pre>18-May-22 13:50:12 This is a test of 4\n18-May-22 13:50:12 This should be in a unique file\n18-May-22 13:50:12 This is a test of 3\n18-May-22 13:50:12 This should be in a unique file\n18-May-22 13:50:12 This is a test of 1\n18-May-22 13:50:12 This should be in a unique file\n18-May-22 13:50:13 This is a test of 9\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 6\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 5\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 0\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 2\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 7\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 8\n18-May-22 13:50:13 This should be in a unique file\n</pre> Out[4]: <pre>(array([ 0.00106875, -0.0032616 , -0.00759213, ..., -0.00612375,\n         0.0059738 , -0.01329288], dtype=float32),\n array([ 1.1191896 ,  0.6762629 ,  0.23331782, ...,  0.38350907,\n         1.6208985 , -0.34978032], dtype=float32),\n array([4.23731  , 3.3557875, 2.4742277, ..., 2.7731416, 5.235823 ,\n        1.3137323], dtype=float32),\n array([ 9.3554325,  8.035313 ,  6.7151375, ...,  7.1627736, 10.850748 ,\n         4.9772453], dtype=float32),\n array([16.47355 , 14.714837, 12.956048, ..., 13.552408, 18.465672,\n        10.640757], dtype=float32),\n array([25.591675, 23.39436 , 21.196953, ..., 21.942041, 28.0806  ,\n        18.304268], dtype=float32),\n array([36.7098  , 34.073883, 31.437868, ..., 32.331676, 39.695522,\n        27.967781], dtype=float32),\n array([49.827915, 46.753407, 43.67878 , ..., 44.721313, 53.31045 ,\n        39.631294], dtype=float32),\n array([64.946045, 61.432938, 57.9197  , ..., 59.11094 , 68.92538 ,\n        53.2948  ], dtype=float32),\n array([82.064156, 78.11245 , 74.16061 , ..., 75.50058 , 86.5403  ,\n        68.95831 ], dtype=float32))</pre> <p>This example also shows how to create a log file for each item in data with the <code>setup_logger</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/32_Decoding_with_GPUs/#gpu-use", "title": "GPU Use\u00b6", "text": ""}, {"location": "notebooks/32_Decoding_with_GPUs/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/32_Decoding_with_GPUs/#gpu-clusters", "title": "GPU Clusters\u00b6", "text": ""}, {"location": "notebooks/32_Decoding_with_GPUs/#connecting", "title": "Connecting\u00b6", "text": "<p>Members of the Frank Lab have access to two GPU cluster, <code>breeze</code> and <code>zephyr</code>. To access them, specify the cluster when you <code>ssh</code>, with the default port:</p> <p><code>ssh username@{breeze or zephyr}.cin.ucsf.edu</code></p> <p>There are currently 10 available GPUs, each with 80 GB RAM, each referred to by their IDs (0 - 9).</p>"}, {"location": "notebooks/32_Decoding_with_GPUs/#selecting-a-gpu", "title": "Selecting a GPU\u00b6", "text": "<p>For decoding, we first install <code>cupy</code>. By doing so with conda, we're sure to install the correct cuda-toolkit:</p> <pre>conda install cupy\n</pre> <p>Next, we'll select a single GPU for decoding, using <code>cp.cuda.Device(GPU_ID)</code> in a context manager (i.e., <code>with</code>). Below, we'll select GPU #6 (ID = 5).</p> <p>Warning: Omitting the context manager will cause cupy to default to using GPU 0.</p>"}, {"location": "notebooks/32_Decoding_with_GPUs/#which-gpu", "title": "Which GPU?\u00b6", "text": "<p>You can see which GPUs are occupied by running the command <code>nvidia-smi</code> in a terminal (or <code>!nvidia-smi</code> in a notebook). Pick a GPU with low memory usage.</p> <p>In the output below, GPUs 1, 4, 6, and 7 have low memory use and power draw (~42W), are probably not in use.</p>"}, {"location": "notebooks/32_Decoding_with_GPUs/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/32_Decoding_with_GPUs/#simulated-data", "title": "Simulated data\u00b6", "text": ""}, {"location": "notebooks/32_Decoding_with_GPUs/#set-up-classifier", "title": "Set up classifier\u00b6", "text": ""}, {"location": "notebooks/32_Decoding_with_GPUs/#multiple-gpus", "title": "Multiple GPUs\u00b6", "text": "<p>Using multiple GPUs requires the <code>dask_cuda</code>:</p> <pre>conda install -c rapidsai -c nvidia -c conda-forge dask-cuda\n</pre> <p>We will set up a client to select GPUs. By default, this is all available GPUs. Below, we select a subset using the <code>CUDA_VISIBLE_DEVICES</code>.</p>"}, {"location": "notebooks/33_Decoding_Clusterless/", "title": "Decoding Clusterless", "text": "<p>Developer Note: if you may make a PR in the future, be sure to copy this notebook, and use the <code>gitignore</code> prefix <code>temp</code> to avoid future conflicts.</p> <p>This is one notebook in a multi-part series on Spyglass.</p> <ul> <li>To set up your Spyglass environment and database, see the Setup notebook</li> <li>This tutorial assumes you've already extracted marks, as well as loaded position data. If 1D decoding, this data should also be linearized.</li> <li>This tutorial also assumes you're familiar with how to run processes on GPU, as presented in this notebook</li> </ul> <p>Clusterless decoding can be performed on either 1D or 2D data. A few steps in this notebook will refer to a <code>decode_1d</code> variable set in select data to include these steps.</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n</pre> %reload_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import os\nimport datajoint as dj\nimport matplotlib.pyplot as plt\nimport logging\nimport cupy as cp\n\nfrom pprint import pprint\n\n\nlogging.basicConfig(\n    level=\"INFO\", format=\"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\"\n)\n\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\ndj.config.load(\"dj_local_conf.json\")  # load config for database connection info\n\nimport spyglass.common as sgc\nimport spyglass.common.common_position as sgc_pos\nimport spyglass.common.interval as sgc_int\nimport spyglass.decoding.clusterless as sgd_clusterless\nimport spyglass.decoding.visualization as sgd_viz\nimport replay_trajectory_classification as rtc\nimport replay_trajectory_classification.environments as rtc_env\n\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import datajoint as dj import matplotlib.pyplot as plt import logging import cupy as cp  from pprint import pprint   logging.basicConfig(     level=\"INFO\", format=\"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\" )  # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") dj.config.load(\"dj_local_conf.json\")  # load config for database connection info  import spyglass.common as sgc import spyglass.common.common_position as sgc_pos import spyglass.common.interval as sgc_int import spyglass.decoding.clusterless as sgd_clusterless import spyglass.decoding.visualization as sgd_viz import replay_trajectory_classification as rtc import replay_trajectory_classification.environments as rtc_env   # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2023-08-01 11:16:06,882][INFO]: Connecting root@localhost:3306\n[2023-08-01 11:16:06,909][INFO]: Connected root@localhost:3306\n</pre> In\u00a0[4]: Copied! <pre>nwb_copy_file_name = \"chimi20200216_new_.nwb\"\ndecode_1d = True\n</pre> nwb_copy_file_name = \"chimi20200216_new_.nwb\" decode_1d = True <p>First, we'll fetch marks with <code>fetch_xarray</code>, which provides a labeled array of shape (n_time, n_mark_features, n_electrodes). Time is in 2 ms bins with either <code>NaN</code> if no spike occurred or the value of the spike features.</p> <p>If there is &gt;1 spike per time bin per tetrode, we take an an average of the marks. Ideally, we would use all the marks, this is a rare occurrence and decoding is generally robust to the averaging.</p> In\u00a0[10]: Copied! <pre>marks = (\n    sgd_clusterless.UnitMarksIndicator\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"sort_interval_name\": \"runs_noPrePostTrialTimes raw data valid times\",\n        \"filter_parameter_set_name\": \"franklab_default_hippocampus\",\n        \"unit_inclusion_param_name\": \"all2\",\n        \"mark_param_name\": \"default\",\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"sampling_rate\": 500,\n    }\n).fetch_xarray()\n\nmarks\n</pre> marks = (     sgd_clusterless.UnitMarksIndicator     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"sort_interval_name\": \"runs_noPrePostTrialTimes raw data valid times\",         \"filter_parameter_set_name\": \"franklab_default_hippocampus\",         \"unit_inclusion_param_name\": \"all2\",         \"mark_param_name\": \"default\",         \"interval_list_name\": \"pos 1 valid times\",         \"sampling_rate\": 500,     } ).fetch_xarray()  marks <pre>/stelmo/nwb/analysis/chimi20200216_new_7M0E8ERPE7.nwb\n/stelmo/nwb/analysis/chimi20200216_new_6WW86B509M.nwb\n/stelmo/nwb/analysis/chimi20200216_new_TLD0MCIC5H.nwb\n/stelmo/nwb/analysis/chimi20200216_new_7BEQDOTX3E.nwb\n/stelmo/nwb/analysis/chimi20200216_new_F8QVNUMVJS.nwb\n/stelmo/nwb/analysis/chimi20200216_new_BVZKYWREUE.nwb\n/stelmo/nwb/analysis/chimi20200216_new_3HMJON557D.nwb\n/stelmo/nwb/analysis/chimi20200216_new_QGMZ5ESFVA.nwb\n/stelmo/nwb/analysis/chimi20200216_new_1KRVBBCP2N.nwb\n/stelmo/nwb/analysis/chimi20200216_new_9E2Z0R6TLO.nwb\n/stelmo/nwb/analysis/chimi20200216_new_ALRF0STB1P.nwb\n/stelmo/nwb/analysis/chimi20200216_new_F2TDZW8LRY.nwb\n/stelmo/nwb/analysis/chimi20200216_new_LTEU71Z21T.nwb\n/stelmo/nwb/analysis/chimi20200216_new_KT4E4LIYAI.nwb\n/stelmo/nwb/analysis/chimi20200216_new_KOIRLX6R6X.nwb\n/stelmo/nwb/analysis/chimi20200216_new_4S01EA6NVN.nwb\n/stelmo/nwb/analysis/chimi20200216_new_ATQO860QOB.nwb\n/stelmo/nwb/analysis/chimi20200216_new_H3E2HYMEJA.nwb\n/stelmo/nwb/analysis/chimi20200216_new_4KJ4XVBKW3.nwb\n/stelmo/nwb/analysis/chimi20200216_new_0V98T6HQHX.nwb\n/stelmo/nwb/analysis/chimi20200216_new_A5FBXFDZMD.nwb\n/stelmo/nwb/analysis/chimi20200216_new_A5ELOH1L7Y.nwb\n</pre> Out[10]: <pre>&lt;xarray.DataArray (time: 655645, marks: 4, electrodes: 22)&gt;\narray([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       ...,\n\n       [[ -99.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-100.,   nan,   nan, ...,   nan,   nan,   nan],\n        [ -94.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-104.,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]])\nCoordinates:\n  * time        (time) float64 1.582e+09 1.582e+09 ... 1.582e+09 1.582e+09\n  * marks       (marks) &lt;U14 'amplitude_0000' ... 'amplitude_0003'\n  * electrodes  (electrodes) int64 0 1 2 3 5 6 7 8 9 ... 15 16 17 18 19 21 22 23</pre>xarray.DataArray<ul><li>time: 655645</li><li>marks: 4</li><li>electrodes: 22</li></ul><ul><li>nan nan nan nan nan -170.0 nan nan ... nan nan nan nan nan nan nan nan<pre>array([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       ...,\n\n       [[ -99.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-100.,   nan,   nan, ...,   nan,   nan,   nan],\n        [ -94.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-104.,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]])</pre></li><li>Coordinates: (3)<ul><li>time(time)float641.582e+09 1.582e+09 ... 1.582e+09<pre>array([1.581887e+09, 1.581887e+09, 1.581887e+09, ..., 1.581888e+09,\n       1.581888e+09, 1.581888e+09])</pre></li><li>marks(marks)&lt;U14'amplitude_0000' ... 'amplitude_...<pre>array(['amplitude_0000', 'amplitude_0001', 'amplitude_0002', 'amplitude_0003'],\n      dtype='&lt;U14')</pre></li><li>electrodes(electrodes)int640 1 2 3 5 6 7 ... 17 18 19 21 22 23<pre>array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19, 21, 22, 23])</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>We'll use <code>UnitMarksIndicator.plot_all_marks</code> to make sure our marks look right. This will plot each mark feature against the other for each electrode. We check for items that look overly correlated (strong diagonal on the off-diagonal plots) and extreme amplitudes.</p> <p>For tutorial purposes, we only look at the first 2 plots, but removing this argument will show all plots.</p> In\u00a0[11]: Copied! <pre>sgd_clusterless.UnitMarksIndicator.plot_all_marks(marks, plot_limit=2)\n</pre> sgd_clusterless.UnitMarksIndicator.plot_all_marks(marks, plot_limit=2) <p>Next, we'll grab the 2D position data from <code>IntervalPositionInfo</code> table.</p> <p>Note: Position will need to be upsampled to our decoding frequency (500 Hz). See this notebook for more information.</p> In\u00a0[12]: Copied! <pre>position_key = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": \"pos 1 valid times\",\n    \"position_info_param_name\": \"default_decoding\",\n}\n\nposition_info = (\n    sgc_pos.IntervalPositionInfo() &amp; position_key\n).fetch1_dataframe()\n\nposition_info\n</pre> position_key = {     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": \"pos 1 valid times\",     \"position_info_param_name\": \"default_decoding\", }  position_info = (     sgc_pos.IntervalPositionInfo() &amp; position_key ).fetch1_dataframe()  position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_6YC9LPAR7S.nwb\n</pre> Out[12]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.680048 1.741550 2.301478 2.886139 1.581887e+09 91.039455 211.144123 3.003241 1.827555 2.333931 2.964320 1.581887e+09 91.027260 211.161196 3.008398 1.915800 2.366668 3.044898 1.581887e+09 91.015065 211.178268 3.012802 2.006286 2.399705 3.127901 1.581887e+09 91.002871 211.195341 3.017242 2.099012 2.433059 3.213352 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.299625 -0.944304 0.057520 -0.356012 0.360629 1.581888e+09 182.158583 201.296373 -0.942329 0.053954 -0.356343 0.360404 1.581888e+09 182.158583 201.293121 -0.940357 0.050477 -0.356407 0.359964 1.581888e+09 182.158583 201.289869 -0.953059 0.047091 -0.356212 0.359312 1.581888e+09 182.158583 201.286617 -0.588081 0.043796 -0.355764 0.358450 <p>655645 rows \u00d7 6 columns</p> <p>It is important to visualize the 2D position and identify outliers.</p> In\u00a0[11]: Copied! <pre>plt.figure(figsize=(7, 6))\nplt.plot(position_info.head_position_x, position_info.head_position_y)\n</pre> plt.figure(figsize=(7, 6)) plt.plot(position_info.head_position_x, position_info.head_position_y) Out[11]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f6bfabcd280&gt;]</pre> <p>For 1D decoding, we load the linearized position tables.</p> In\u00a0[13]: Copied! <pre>if decode_1d:\n    linearization_key = {\n        \"position_info_param_name\": \"default_decoding\",\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"track_graph_name\": \"6 arm\",\n        \"linearization_param_name\": \"default\",\n    }\n\n    linear_position_df = (\n        sgc_pos.IntervalLinearizedPosition() &amp; linearization_key\n    ).fetch1_dataframe()\n\n    linear_position_df\nelse:\n    linear_position_df = position_info\n</pre> if decode_1d:     linearization_key = {         \"position_info_param_name\": \"default_decoding\",         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"track_graph_name\": \"6 arm\",         \"linearization_param_name\": \"default\",     }      linear_position_df = (         sgc_pos.IntervalLinearizedPosition() &amp; linearization_key     ).fetch1_dataframe()      linear_position_df else:     linear_position_df = position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_0YEJJLYTSM.nwb\n</pre> Out[13]: linear_position track_segment_id projected_x_position projected_y_position time 1.581887e+09 412.042773 0 90.802281 210.677533 1.581887e+09 412.061718 0 90.785714 210.686724 1.581887e+09 412.080664 0 90.769147 210.695914 1.581887e+09 412.099610 0 90.752579 210.705105 1.581887e+09 412.118556 0 90.736012 210.714296 ... ... ... ... ... 1.581888e+09 340.325042 1 175.434739 212.920160 1.581888e+09 340.323413 1 175.433329 212.919345 1.581888e+09 340.321785 1 175.431919 212.918529 1.581888e+09 340.320156 1 175.430509 212.917713 1.581888e+09 340.318527 1 175.429100 212.916898 <p>655645 rows \u00d7 4 columns</p> <p>We'll also sanity check linearized values by plotting the 2D position projected to its corresponding 1D segment.</p> In\u00a0[14]: Copied! <pre>if decode_1d:\n    plt.figure(figsize=(7, 6))\n    plt.scatter(\n        linear_position_df.projected_x_position,\n        linear_position_df.projected_y_position,\n        c=linear_position_df.track_segment_id,\n        cmap=\"tab20\",\n        s=1,\n    )\n</pre> if decode_1d:     plt.figure(figsize=(7, 6))     plt.scatter(         linear_position_df.projected_x_position,         linear_position_df.projected_y_position,         c=linear_position_df.track_segment_id,         cmap=\"tab20\",         s=1,     ) Out[14]: <pre>&lt;matplotlib.collections.PathCollection at 0x7fb80565b5e0&gt;</pre> <p>And then the linearized position itself:</p> In\u00a0[15]: Copied! <pre>if decode_1d:\n    plt.figure(figsize=(20, 10))\n    plt.scatter(\n        linear_position_df.index,\n        linear_position_df.linear_position,\n        s=1,\n        c=linear_position_df.track_segment_id,\n        cmap=\"tab20\",\n    )\n</pre> if decode_1d:     plt.figure(figsize=(20, 10))     plt.scatter(         linear_position_df.index,         linear_position_df.linear_position,         s=1,         c=linear_position_df.track_segment_id,         cmap=\"tab20\",     ) Out[15]: <pre>&lt;matplotlib.collections.PathCollection at 0x7fb8055de0a0&gt;</pre> <p>And then, we'll verify that all our data is the same size. It may not be due to the valid intervals of the neural and position data.</p> In\u00a0[17]: Copied! <pre>position_info.shape, marks.shape, linear_position_df.shape\n</pre> position_info.shape, marks.shape, linear_position_df.shape Out[17]: <pre>((655645, 6), (655645, 4, 22), (655645, 4))</pre> <p>We'll also validate the ephys and position data for decoding. If we had more than one time interval, we would decode on each separately.</p> In\u00a0[18]: Copied! <pre>key = {}\nkey[\"interval_list_name\"] = \"02_r1\"\nkey[\"nwb_file_name\"] = nwb_copy_file_name\n\ninterval = (\n    sgc.IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": key[\"interval_list_name\"],\n    }\n).fetch1(\"valid_times\")\n\nvalid_ephys_times = (\n    sgc.IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": \"raw data valid times\",\n    }\n).fetch1(\"valid_times\")\nposition_interval_names = (\n    sgc_pos.IntervalPositionInfo\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"position_info_param_name\": \"default_decoding\",\n    }\n).fetch(\"interval_list_name\")\nvalid_pos_times = [\n    (\n        sgc.IntervalList\n        &amp; {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": pos_interval_name,\n        }\n    ).fetch1(\"valid_times\")\n    for pos_interval_name in position_interval_names\n]\n\nintersect_interval = sgc_int.interval_list_intersect(\n    sgc_int.interval_list_intersect(interval, valid_ephys_times),\n    valid_pos_times[0],\n)\nvalid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1])\nvalid_time_slice\n</pre> key = {} key[\"interval_list_name\"] = \"02_r1\" key[\"nwb_file_name\"] = nwb_copy_file_name  interval = (     sgc.IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": key[\"interval_list_name\"],     } ).fetch1(\"valid_times\")  valid_ephys_times = (     sgc.IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": \"raw data valid times\",     } ).fetch1(\"valid_times\") position_interval_names = (     sgc_pos.IntervalPositionInfo     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"position_info_param_name\": \"default_decoding\",     } ).fetch(\"interval_list_name\") valid_pos_times = [     (         sgc.IntervalList         &amp; {             \"nwb_file_name\": key[\"nwb_file_name\"],             \"interval_list_name\": pos_interval_name,         }     ).fetch1(\"valid_times\")     for pos_interval_name in position_interval_names ]  intersect_interval = sgc_int.interval_list_intersect(     sgc_int.interval_list_intersect(interval, valid_ephys_times),     valid_pos_times[0], ) valid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1]) valid_time_slice Out[18]: <pre>slice(1581886916.3153033, 1581888227.5987928, None)</pre> In\u00a0[19]: Copied! <pre>linear_position_df = linear_position_df.loc[valid_time_slice]\nmarks = marks.sel(time=valid_time_slice)\nposition_info = position_info.loc[valid_time_slice]\n</pre> linear_position_df = linear_position_df.loc[valid_time_slice] marks = marks.sel(time=valid_time_slice) position_info = position_info.loc[valid_time_slice] In\u00a0[20]: Copied! <pre>position_info.shape, marks.shape, linear_position_df.shape\n</pre> position_info.shape, marks.shape, linear_position_df.shape Out[20]: <pre>((655643, 6), (655643, 4, 22), (655643, 4))</pre> <p>After sanity checks, we can finally get to decoding.</p> <p>Note: Portions of the code below have been integrated into <code>spyglass.decoding</code>, but are presented here in full.</p> <p>We'll fetch the default parameters and modify them. For 1D decoding, we'll also pass the track graph and parameters from linearization to handle the random walk properly. <code>position_std</code> and <code>mark_std</code> set the amount of smoothing in the position and mark dimensions. <code>block_size</code> controls how many samples get processed at a time so that we don't run out of GPU memory.</p> In\u00a0[21]: Copied! <pre>parameters = (\n    sgd_clusterless.ClusterlessClassifierParameters()\n    &amp; {\"classifier_param_name\": \"default_decoding_gpu\"}\n).fetch1()\n\nalgorithm_params = (\n    {\n        \"mark_std\": 24.0,\n        \"position_std\": 6.0,\n        \"block_size\": 2**12,  # in merging nbs, changed from 2**13 for 2d\n    },\n)\n\nif decode_1d:\n    track = sgc_pos.TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"}\n    track_graph = track.get_networkx_track_graph()\n    track_graph_params = track.fetch1()\n\n    parameters[\"classifier_params\"] = {\n        \"environments\": [\n            rtc_env.Environment(\n                track_graph=track_graph,\n                edge_order=track_graph_params[\"linear_edge_order\"],\n                edge_spacing=track_graph_params[\"linear_edge_spacing\"],\n            )\n        ],\n        \"clusterless_algorithm\": \"multiunit_likelihood_integer_gpu\",\n        \"clusterless_algorithm_params\": algorithm_params,\n    }\nelse:\n    parameters[\"classifier_params\"] = {\n        \"environments\": [rtc_env.Environment(place_bin_size=3.0)],\n        \"clusterless_algorithm_params\": {\n            **algorithm_params,\n            \"disable_progress_bar\": False,\n            \"use_diffusion\": False,\n        },\n    }\n\npprint(parameters)\n</pre> parameters = (     sgd_clusterless.ClusterlessClassifierParameters()     &amp; {\"classifier_param_name\": \"default_decoding_gpu\"} ).fetch1()  algorithm_params = (     {         \"mark_std\": 24.0,         \"position_std\": 6.0,         \"block_size\": 2**12,  # in merging nbs, changed from 2**13 for 2d     }, )  if decode_1d:     track = sgc_pos.TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"}     track_graph = track.get_networkx_track_graph()     track_graph_params = track.fetch1()      parameters[\"classifier_params\"] = {         \"environments\": [             rtc_env.Environment(                 track_graph=track_graph,                 edge_order=track_graph_params[\"linear_edge_order\"],                 edge_spacing=track_graph_params[\"linear_edge_spacing\"],             )         ],         \"clusterless_algorithm\": \"multiunit_likelihood_integer_gpu\",         \"clusterless_algorithm_params\": algorithm_params,     } else:     parameters[\"classifier_params\"] = {         \"environments\": [rtc_env.Environment(place_bin_size=3.0)],         \"clusterless_algorithm_params\": {             **algorithm_params,             \"disable_progress_bar\": False,             \"use_diffusion\": False,         },     }  pprint(parameters) <pre>{'classifier_param_name': 'default_decoding_gpu',\n 'classifier_params': {'clusterless_algorithm': 'multiunit_likelihood_integer_gpu',\n                       'clusterless_algorithm_params': {'block_size': 4096,\n                                                        'mark_std': 24.0,\n                                                        'position_std': 6.0},\n                       'continuous_transition_types': [[RandomWalk(environment_name='', movement_var=6.0, movement_mean=0.0, use_diffusion=False),\n                                                        Uniform(environment_name='', environment2_name=None)],\n                                                       [Uniform(environment_name='', environment2_name=None),\n                                                        Uniform(environment_name='', environment2_name=None)]],\n                       'discrete_transition_type': DiagonalDiscrete(diagonal_value=0.98),\n                       'environments': [Environment(environment_name='', place_bin_size=2.0, track_graph=&lt;networkx.classes.graph.Graph object at 0x7fb807560cd0&gt;, edge_order=[(3, 6), (6, 8), (6, 9), (3, 1), (1, 2), (1, 0), (3, 4), (4, 5), (4, 7)], edge_spacing=15, is_track_interior=None, position_range=None, infer_track_interior=True, fill_holes=False, dilate=False)],\n                       'infer_track_interior': True,\n                       'initial_conditions_type': UniformInitialConditions(),\n                       'observation_models': None},\n 'fit_params': {},\n 'predict_params': {'is_compute_acausal': True,\n                    'state_names': ['Continuous', 'Uniform'],\n                    'use_gpu': True}}\n</pre> <p>To run decoding on the first GPU device, we use <code>cp.dua.Device(0)</code>. For more information, see this notebook.</p> In\u00a0[22]: Copied! <pre>if decode_1d:\n    position = linear_position_df.linear_position.values\n    time = linear_position_df.index\nelse:\n    position = position_info[[\"head_position_x\", \"head_position_y\"]].values\n    time = position_info.index\n\nwith cp.cuda.Device(0):\n    classifier = rtc.ClusterlessClassifier(**parameters[\"classifier_params\"])\n    classifier.fit(\n        position=position,\n        multiunits=marks.values,\n        **parameters[\"fit_params\"],\n    )\n    results = classifier.predict(\n        multiunits=marks.values,\n        time=time,\n        **parameters[\"predict_params\"],\n    )\n    logging.info(\"Done!\")\n</pre> if decode_1d:     position = linear_position_df.linear_position.values     time = linear_position_df.index else:     position = position_info[[\"head_position_x\", \"head_position_y\"]].values     time = position_info.index  with cp.cuda.Device(0):     classifier = rtc.ClusterlessClassifier(**parameters[\"classifier_params\"])     classifier.fit(         position=position,         multiunits=marks.values,         **parameters[\"fit_params\"],     )     results = classifier.predict(         multiunits=marks.values,         time=time,         **parameters[\"predict_params\"],     )     logging.info(\"Done!\") <pre>12-Sep-22 12:15:15 Fitting initial conditions...\n12-Sep-22 12:15:15 Fitting continuous state transition...\n12-Sep-22 12:15:15 Fitting discrete state transition\n12-Sep-22 12:15:15 Fitting multiunits...\n12-Sep-22 12:15:18 Estimating likelihood...\n</pre> <pre>12-Sep-22 12:15:32 Estimating causal posterior...\n12-Sep-22 12:18:49 Estimating acausal posterior...\n12-Sep-22 12:26:34 Done!\n</pre> <p>Finally, we can sanity check plot the decodes in an interactive figure with <code>create_interactive_1D_decoding_figurl</code>, which will return a URL</p> <p>Note: For this figure, that you need to be running an interactive sorting view backend.</p> In\u00a0[23]: Copied! <pre>df = linear_position_df if decode_1d else position_info\n\nview = sgd_viz.create_interactive_1D_decoding_figurl(\n    position_info,\n    linear_position_df,\n    marks,\n    results,\n    position_name=\"linear_position\",\n    speed_name=\"head_speed\",\n    posterior_type=\"acausal_posterior\",\n    sampling_frequency=500,\n    view_height=800,\n)\n</pre> df = linear_position_df if decode_1d else position_info  view = sgd_viz.create_interactive_1D_decoding_figurl(     position_info,     linear_position_df,     marks,     results,     position_name=\"linear_position\",     speed_name=\"head_speed\",     posterior_type=\"acausal_posterior\",     sampling_frequency=500,     view_height=800, ) <pre>WARNING: create_position_plot is deprecated. Instead use vv.PositionPlot(...). See tests/test_position_plot.py\nComputing sha1 of /stelmo/nwb/.kachery-cloud/tmp_WUd2b9mq/file.npy\n/stelmo/nwb/.kachery-cloud/tmp_VGviN8q9\nCreating segment/1/0\nCreating segment/1/1\nCreating segment/1/2\nCreating segment/1/3\nCreating segment/1/4\nCreating segment/1/5\nCreating segment/1/6\nCreating segment/3/0\nCreating segment/3/1\nCreating segment/3/2\nCreating segment/9/0\nCreating segment/27/0\nCreating segment/81/0\nCreating segment/243/0\nCreating segment/729/0\nCreating segment/2187/0\nCreating segment/6561/0\nCreating segment/19683/0\nCreating segment/59049/0\nCreating segment/177147/0\nCreating segment/531441/0\nComputing sha1 of /stelmo/nwb/.kachery-cloud/tmp_VGviN8q9/live_position_pdf_plot.h5\n</pre> Out[23]: <pre>'https://figurl.org/f?v=gs://figurl/spikesortingview-9&amp;d=sha1://319b25199f81cc30d84f8eed471309b419c9b95d&amp;project=lqqrbobsev&amp;label=test'</pre> <p>To view the decode in the notebook, simply run <code>view</code> in a cell.</p> <p>To create shareable visualization in the cloud, call <code>url</code></p> In\u00a0[\u00a0]: Copied! <pre>dim = \"1D\" if decode_1d else \"2D\"\nview.url(label=f\"{dim} Decoding Example\")\n</pre> dim = \"1D\" if decode_1d else \"2D\" view.url(label=f\"{dim} Decoding Example\")"}, {"location": "notebooks/33_Decoding_Clusterless/#clusterless-decoding", "title": "Clusterless Decoding\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#overview", "title": "Overview\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#imports", "title": "Imports\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#select-data", "title": "Select data\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#unitmarksindicator", "title": "<code>UnitMarksIndicator</code>\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#position", "title": "Position\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#get-position", "title": "Get position\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#plot-position", "title": "Plot position\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#validate-data", "title": "Validate data\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#decoding", "title": "Decoding\u00b6", "text": ""}, {"location": "notebooks/33_Decoding_Clusterless/#visualization", "title": "Visualization\u00b6", "text": ""}]}